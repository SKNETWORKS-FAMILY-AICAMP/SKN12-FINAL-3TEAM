# RunPod ë°°í¬ ê°€ì´ë“œ ë° ê°œì„ ì‚¬í•­

## ğŸš¨ ì£¼ìš” ê°œì„  í•„ìš”ì‚¬í•­

### 1. í•˜ë“œì½”ë”©ëœ ê²½ë¡œ ì œê±°
í˜„ì¬ Windows ê²½ë¡œê°€ í•˜ë“œì½”ë”©ë˜ì–´ ìˆì–´ Linux í™˜ê²½ì—ì„œ ì‹¤í–‰ ë¶ˆê°€:
- `C:/Users/SH/Desktop/TtalKkac/...` â†’ í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ìƒëŒ€ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”

### 2. í™˜ê²½ ì„¤ì • íŒŒì¼ (config.py)
```python
import os
from pathlib import Path

class Config:
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    BASE_DIR = Path(__file__).parent.absolute()
    MODEL_DIR = os.getenv("MODEL_DIR", BASE_DIR / "models")
    DATA_DIR = os.getenv("DATA_DIR", BASE_DIR / "data")
    
    # ëª¨ë¸ ê²½ë¡œ
    BERT_MODEL_PATH = os.getenv("BERT_MODEL_PATH", MODEL_DIR / "Ttalkkac_model_v3.pt")
    BERT_CONFIG_PATH = os.getenv("BERT_CONFIG_PATH", MODEL_DIR / "bert_model_config.json")
    BERT_TOKENIZER_PATH = os.getenv("BERT_TOKENIZER_PATH", MODEL_DIR / "bert_tokenizer_config.json")
    
    # Qwen LoRA ê²½ë¡œ
    QWEN_LORA_1B = os.getenv("QWEN_LORA_1B", MODEL_DIR / "qwen3_lora_ttalkkac_1.7b")
    QWEN_LORA_4B = os.getenv("QWEN_LORA_4B", MODEL_DIR / "qwen3_lora_ttalkkac_4b")
    QWEN_LORA_8B = os.getenv("QWEN_LORA_8B", MODEL_DIR / "qwen3_lora_ttalkkac_8b")
    
    # API ì„¤ì •
    API_HOST = os.getenv("API_HOST", "0.0.0.0")
    API_PORT = int(os.getenv("API_PORT", 8000))
    
    # HuggingFace í† í°
    HF_TOKEN = os.getenv("HF_TOKEN", "")
    
    # GPU ì„¤ì •
    CUDA_DEVICE = os.getenv("CUDA_VISIBLE_DEVICES", "0")
    USE_VLLM = os.getenv("USE_VLLM", "true").lower() == "true"
    
    # ë¡œê¹…
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
    LOG_FILE = os.getenv("LOG_FILE", "ai_server.log")
```

### 3. RunPod ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ (setup_runpod.sh)
```bash
#!/bin/bash

echo "ğŸš€ TtalKkac AI Engine RunPod Setup"

# 1. ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸
apt-get update && apt-get install -y \
    ffmpeg \
    git \
    wget \
    curl \
    vim \
    htop \
    screen \
    && rm -rf /var/lib/apt/lists/*

# 2. Python í™˜ê²½ ì„¤ì •
pip install --upgrade pip setuptools wheel

# 3. í”„ë¡œì íŠ¸ í´ë¡ 
cd /workspace
git clone https://github.com/yourusername/TtalKkac.git
cd TtalKkac/ai-engine-dev

# 4. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
pip install -r requirements_api_server.txt

# 5. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
python download_models.py

# 6. í™˜ê²½ë³€ìˆ˜ ì„¤ì •
cat > .env << EOF
MODEL_DIR=/workspace/TtalKkac/ai-engine-dev/models
DATA_DIR=/workspace/TtalKkac/ai-engine-dev/data
HF_TOKEN=${HF_TOKEN}
USE_VLLM=true
CUDA_VISIBLE_DEVICES=0
API_PORT=8000
LOG_LEVEL=INFO
EOF

# 7. ngrok ì„¤ì¹˜
wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz
tar xvzf ngrok-v3-stable-linux-amd64.tgz
chmod +x ngrok
mv ngrok /usr/local/bin/

# 8. ngrok ì„¤ì •
ngrok config add-authtoken ${NGROK_AUTH_TOKEN}

echo "âœ… Setup complete!"
```

### 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (download_models.py)
```python
import os
import torch
from huggingface_hub import snapshot_download
from pathlib import Path

def download_models():
    """RunPodì—ì„œ í•„ìš”í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    
    model_dir = Path(os.getenv("MODEL_DIR", "./models"))
    model_dir.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ“¥ Downloading models...")
    
    # 1. BERT ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (HuggingFaceì— ì—…ë¡œë“œ í•„ìš”)
    if os.getenv("HF_TOKEN"):
        try:
            snapshot_download(
                repo_id="your-username/ttalkkac-bert-classifier",
                local_dir=model_dir / "bert",
                token=os.getenv("HF_TOKEN")
            )
            print("âœ… BERT model downloaded")
        except Exception as e:
            print(f"âš ï¸ BERT download failed: {e}")
    
    # 2. Qwen LoRA ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ
    lora_models = [
        "qwen3_lora_ttalkkac_1.7b",
        "qwen3_lora_ttalkkac_4b", 
        "qwen3_lora_ttalkkac_8b"
    ]
    
    for lora in lora_models:
        try:
            snapshot_download(
                repo_id=f"your-username/{lora}",
                local_dir=model_dir / lora,
                token=os.getenv("HF_TOKEN")
            )
            print(f"âœ… {lora} downloaded")
        except Exception as e:
            print(f"âš ï¸ {lora} download failed: {e}")
    
    print("âœ… All models downloaded")

if __name__ == "__main__":
    download_models()
```

### 5. ê°œì„ ëœ ì„œë²„ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ (start_server.sh)
```bash
#!/bin/bash

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
source .env

# GPU í™•ì¸
nvidia-smi

# ì„œë²„ì™€ ngrokì„ screen ì„¸ì…˜ìœ¼ë¡œ ì‹¤í–‰
screen -dmS ai_server bash -c "python ai_server_final_with_triplets.py"
screen -dmS ngrok bash -c "ngrok http 8000 --log=stdout"

# ë¡œê·¸ í™•ì¸
echo "ğŸ” Checking server status..."
sleep 5

# ngrok URL ê°€ì ¸ì˜¤ê¸°
NGROK_URL=$(curl -s localhost:4040/api/tunnels | python -c "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])")
echo "âœ… Ngrok URL: $NGROK_URL"

# backend .envì— URL ì—…ë°ì´íŠ¸
echo "AI_SERVER_URL=$NGROK_URL" >> /workspace/TtalKkac/backend/.env

echo "âœ… Server started!"
echo "ğŸ“ View logs:"
echo "  - AI Server: screen -r ai_server"
echo "  - Ngrok: screen -r ngrok"
```

### 6. ê°œì„ ëœ ai_server_final_with_triplets.py ìˆ˜ì •ì‚¬í•­

```python
# íŒŒì¼ ìƒë‹¨ì— ì¶”ê°€
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

# Config ì„í¬íŠ¸
from config import Config

# ëª¨ë“  í•˜ë“œì½”ë”©ëœ ê²½ë¡œë¥¼ Configë¡œ ëŒ€ì²´
# ì˜ˆì‹œ:
# ê¸°ì¡´: bert_model_path = "C:/Users/SH/Desktop/TtalKkac/ai-engine-dev/Bertëª¨ë¸/Ttalkkac_model_v3.pt"
# ë³€ê²½: bert_model_path = Config.BERT_MODEL_PATH

# BERT ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ìˆ˜ì •
def load_bert():
    global bert_classifier
    try:
        from bert_classifier import BertClassifier
        bert_classifier = BertClassifier(
            model_path=str(Config.BERT_MODEL_PATH),
            config_path=str(Config.BERT_CONFIG_PATH),
            tokenizer_config_path=str(Config.BERT_TOKENIZER_PATH)
        )
        logger.info("âœ… BERT classifier loaded")
    except Exception as e:
        logger.error(f"âŒ BERT loading failed: {e}")
        bert_classifier = None

# ì„œë²„ ì‹œì‘ ë¶€ë¶„ ìˆ˜ì •
if __name__ == "__main__":
    uvicorn.run(
        app,
        host=Config.API_HOST,
        port=Config.API_PORT,
        log_level=Config.LOG_LEVEL.lower()
    )
```

### 7. Docker ì»¨í…Œì´ë„ˆ ë°©ì‹ (ì„ íƒì‚¬í•­)
```dockerfile
FROM runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04

WORKDIR /workspace

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    ffmpeg git wget curl vim htop screen \
    && rm -rf /var/lib/apt/lists/*

# í”„ë¡œì íŠ¸ ë³µì‚¬
COPY . /workspace/TtalKkac/

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
WORKDIR /workspace/TtalKkac/ai-engine-dev
RUN pip install --no-cache-dir -r requirements.txt
RUN pip install --no-cache-dir -r requirements_api_server.txt

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
CMD ["bash", "start_server.sh"]
```

### 8. backend/.env ì—…ë°ì´íŠ¸
```env
# AI ì„œë²„ ì„¤ì • (RunPod)
AI_SERVER_URL=https://your-ngrok-url.ngrok-free.app
AI_SERVER_TIMEOUT=300

# ë°±ì—… AI ì„œë²„ (ë¡œì»¬)
AI_SERVER_BACKUP_URL=http://localhost:8000
```

### 9. Slack í†µí•© ê°œì„ ì‚¬í•­

#### backend/src/services/ai-service.ts
```typescript
import axios from 'axios';

class AIService {
  private aiServerUrl: string;
  private backupUrl: string;
  
  constructor() {
    this.aiServerUrl = process.env.AI_SERVER_URL || '';
    this.backupUrl = process.env.AI_SERVER_BACKUP_URL || '';
  }
  
  async processAudio(audioBuffer: Buffer): Promise<any> {
    const formData = new FormData();
    formData.append('file', new Blob([audioBuffer]), 'audio.mp3');
    
    try {
      // ë©”ì¸ ì„œë²„ ì‹œë„
      const response = await axios.post(
        `${this.aiServerUrl}/pipeline-final`,
        formData,
        {
          headers: { 'Content-Type': 'multipart/form-data' },
          timeout: 300000, // 5ë¶„
          maxContentLength: Infinity,
          maxBodyLength: Infinity
        }
      );
      return response.data;
    } catch (error) {
      console.error('Main AI server failed, trying backup...');
      
      // ë°±ì—… ì„œë²„ ì‹œë„
      const response = await axios.post(
        `${this.backupUrl}/pipeline-final`,
        formData,
        {
          headers: { 'Content-Type': 'multipart/form-data' },
          timeout: 300000
        }
      );
      return response.data;
    }
  }
  
  async healthCheck(): Promise<boolean> {
    try {
      const response = await axios.get(`${this.aiServerUrl}/health`, {
        timeout: 5000
      });
      return response.status === 200;
    } catch {
      return false;
    }
  }
}
```

## ğŸš€ RunPod ì‹¤í–‰ ìˆœì„œ

1. **RunPod GPU Pod ìƒì„±**
   - RTX 4090 ë˜ëŠ” A100 ì¶”ì²œ
   - ìµœì†Œ 24GB VRAM í•„ìš”
   - PyTorch 2.1.0 í…œí”Œë¦¿ ì„ íƒ

2. **ì´ˆê¸° ì„¤ì •**
   ```bash
   cd /workspace
   wget https://raw.githubusercontent.com/yourusername/TtalKkac/main/ai-engine-dev/setup_runpod.sh
   chmod +x setup_runpod.sh
   export HF_TOKEN="your-huggingface-token"
   export NGROK_AUTH_TOKEN="your-ngrok-token"
   ./setup_runpod.sh
   ```

3. **ì„œë²„ ì‹œì‘**
   ```bash
   cd /workspace/TtalKkac/ai-engine-dev
   ./start_server.sh
   ```

4. **Backend í™˜ê²½ë³€ìˆ˜ ì—…ë°ì´íŠ¸**
   ```bash
   # ngrok URL í™•ì¸ í›„ backend/.env ìˆ˜ì •
   vim /workspace/TtalKkac/backend/.env
   # AI_SERVER_URL=https://your-ngrok-url.ngrok-free.app
   ```

5. **í…ŒìŠ¤íŠ¸**
   ```bash
   # Health check
   curl http://localhost:8000/health
   
   # API í…ŒìŠ¤íŠ¸
   curl -X POST http://localhost:8000/transcribe-enhanced \
     -F "file=@test.MP3"
   ```

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **GPU ë©”ëª¨ë¦¬ ê´€ë¦¬**
   - VLLM ì‚¬ìš© ì‹œ ìµœì†Œ 24GB VRAM í•„ìš”
   - batch_size ì¡°ì •ìœ¼ë¡œ OOM ë°©ì§€

2. **ëª¨ë¸ ìºì‹±**
   - ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ ì†Œìš”
   - /workspace/modelsì— ìºì‹œ ì €ì¥

3. **ë³´ì•ˆ**
   - ngrok URLì„ ê³µê°œí•˜ì§€ ì•Šê¸°
   - API í‚¤ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬
   - Rate limiting ì ìš© ê¶Œì¥

4. **ëª¨ë‹ˆí„°ë§**
   - GPU ì‚¬ìš©ë¥ : `watch -n 1 nvidia-smi`
   - ì„œë²„ ë¡œê·¸: `screen -r ai_server`
   - ngrok ìƒíƒœ: `screen -r ngrok`

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

1. **VLLM í™œì„±í™”**
   ```env
   USE_VLLM=true
   ```

2. **ë°°ì¹˜ ì²˜ë¦¬**
   ```python
   BERT_BATCH_SIZE=64  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
   ```

3. **ëª¨ë¸ ì–‘ìí™”**
   - AWQ ë˜ëŠ” GPTQ ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½

4. **ìºì‹±**
   - Redis ì¶”ê°€ë¡œ ë°˜ë³µ ìš”ì²­ ìµœì í™”