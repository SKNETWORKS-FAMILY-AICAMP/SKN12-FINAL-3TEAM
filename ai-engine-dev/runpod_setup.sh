#!/bin/bash

# RunPod GPU í™˜ê²½ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
echo "=========================================="
echo "RunPod Environment Setup for TtalKkak"
echo "=========================================="
echo ""

# 1. ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
echo "[1/6] Updating system packages..."
apt-get update && apt-get install -y \
    ffmpeg \
    git \
    vim \
    htop \
    wget \
    curl

# 2. Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
echo ""
echo "[2/6] Installing Python packages..."
pip install --upgrade pip

# ê¸°ë³¸ íŒ¨í‚¤ì§€
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers accelerate datasets
pip install fastapi uvicorn pydantic

# WhisperX ì„¤ì¹˜
echo ""
echo "[3/6] Installing WhisperX..."
pip install git+https://github.com/m-bain/whisperx.git

# VLLM ì„¤ì¹˜ (GPU í™˜ê²½)
echo ""
echo "[4/6] Installing VLLM..."
pip install vllm

# ì¶”ê°€ íŒ¨í‚¤ì§€
pip install sentencepiece protobuf
pip install tiktoken
pip install numpy scipy pandas

# 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
echo ""
echo "[5/6] Downloading models..."

# Hugging Face ìºì‹œ ì„¤ì •
export HF_HOME=/workspace/.cache/huggingface
mkdir -p $HF_HOME

# Qwen ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œ)
python3 -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
print('Downloading Qwen2.5-32B-Instruct-AWQ...')
try:
    tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-32B-Instruct-AWQ', trust_remote_code=True)
    print('âœ… Tokenizer downloaded')
    # ëª¨ë¸ì€ VLLMì´ ìžë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
except Exception as e:
    print(f'âš ï¸ Download failed: {e}')
"

# WhisperX ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
python3 -c "
import whisperx
import torch
print('Downloading WhisperX large-v3...')
try:
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = whisperx.load_model('large-v3', device, compute_type='float16' if device=='cuda' else 'int8')
    print('âœ… WhisperX model downloaded')
except Exception as e:
    print(f'âš ï¸ Download failed: {e}')
"

# 4. ìž‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
echo ""
echo "[6/6] Setting up working directory..."
cd /workspace

# í”„ë¡œì íŠ¸ íŒŒì¼ ë³µì‚¬ (ì´ë¯¸ ì—…ë¡œë“œëœ ê²½ìš°)
if [ -f "process_file_standalone.py" ]; then
    echo "âœ… Project files found"
else
    echo "âš ï¸ Please upload project files:"
    echo "  - process_file_standalone.py"
    echo "  - task_schemas.py"
    echo "  - meeting_analysis_prompts.py"
    echo "  - prd_generation_prompts.py"
    echo "  - (optional) triplet_processor.py"
    echo "  - (optional) bert_classifier.py"
fi

# 5. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo ""
echo "Setting environment variables..."
cat > /workspace/.env << EOF
# RunPod í™˜ê²½ ë³€ìˆ˜
USE_VLLM=true
CUDA_VISIBLE_DEVICES=0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# ëª¨ë¸ ìºì‹œ
HF_HOME=/workspace/.cache/huggingface
TORCH_HOME=/workspace/.cache/torch

# VLLM ì„¤ì •
VLLM_USE_MODELSCOPE=false
TENSOR_PARALLEL_SIZE=1
GPU_MEMORY_UTILIZATION=0.8
MAX_MODEL_LEN=16384
EOF

source /workspace/.env

# 6. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
echo ""
echo "Creating test script..."
cat > /workspace/test_gpu.py << 'EOF'
import torch
import os

print("="*50)
print("GPU Environment Test")
print("="*50)

# GPU í™•ì¸
if torch.cuda.is_available():
    print(f"âœ… GPU Available: {torch.cuda.get_device_name()}")
    print(f"   CUDA Version: {torch.version.cuda}")
    print(f"   Device Count: {torch.cuda.device_count()}")
    print(f"   Current Device: {torch.cuda.current_device()}")
    
    # ë©”ëª¨ë¦¬ ì •ë³´
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    allocated = torch.cuda.memory_allocated() / 1024**3
    reserved = torch.cuda.memory_reserved() / 1024**3
    
    print(f"\nðŸ’¾ Memory Info:")
    print(f"   Total VRAM: {total_memory:.1f} GB")
    print(f"   Allocated: {allocated:.1f} GB")
    print(f"   Reserved: {reserved:.1f} GB")
    print(f"   Available: {total_memory - reserved:.1f} GB")
else:
    print("âŒ No GPU detected")

# í™˜ê²½ ë³€ìˆ˜
print(f"\nðŸ”§ Environment:")
print(f"   USE_VLLM: {os.getenv('USE_VLLM', 'false')}")
print(f"   HF_HOME: {os.getenv('HF_HOME', 'not set')}")

print("="*50)
EOF

# 7. ì‹¤í–‰ ê¶Œí•œ ì„¤ì •
chmod +x run_standalone.sh

# 8. ì™„ë£Œ ë©”ì‹œì§€
echo ""
echo "=========================================="
echo "âœ… Setup Complete!"
echo "=========================================="
echo ""
echo "ðŸ“‹ Quick Start:"
echo "  1. Upload your audio/text file"
echo "  2. Run: ./run_standalone.sh your_file.mp3"
echo ""
echo "ðŸ”§ Test GPU:"
echo "  python test_gpu.py"
echo ""
echo "ðŸ“ File Structure:"
echo "  /workspace/"
echo "  â”œâ”€â”€ process_file_standalone.py  (main script)"
echo "  â”œâ”€â”€ run_standalone.sh           (runner)"
echo "  â”œâ”€â”€ test.MP3                    (your audio)"
echo "  â””â”€â”€ pipeline_results/           (output)"
echo ""
echo "ðŸ’¡ Tips:"
echo "  - Use --use-vllm flag for faster inference"
echo "  - Results saved to pipeline_results/session_*/"
echo "  - Check GPU usage with: nvidia-smi"
echo "=========================================="