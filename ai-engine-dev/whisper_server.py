#!/usr/bin/env python3
"""
WhisperX ì „ìš© ì„œë²„ (í¬íŠ¸ 8001)
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
import whisperx
import torch
import numpy as np
import io
import soundfile as sf
import logging
import gc

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="WhisperX Server", version="1.0.0")

# WhisperX ëª¨ë¸ ì „ì—­ ë³€ìˆ˜
whisper_model = None
device = "cuda" if torch.cuda.is_available() else "cpu"
compute_type = "float16" if device == "cuda" else "int8"

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œìž‘ ì‹œ WhisperX ëª¨ë¸ ë¡œë“œ"""
    global whisper_model
    try:
        logger.info(f"ðŸŽ¤ Loading WhisperX on {device}...")
        from faster_whisper import WhisperModel
        whisper_model = WhisperModel("base", device=device, compute_type=compute_type)
        logger.info("âœ… WhisperX loaded successfully!")
    except Exception as e:
        logger.error(f"âŒ WhisperX loading failed: {e}")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "service": "whisper",
        "model_loaded": whisper_model is not None,
        "device": device
    }

@app.post("/transcribe")
async def transcribe_audio(file: UploadFile = File(...)):
    """ìŒì„± íŒŒì¼ ì „ì‚¬"""
    if not whisper_model:
        raise HTTPException(status_code=503, detail="WhisperX model not loaded")
    
    try:
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°
        audio_bytes = await file.read()
        audio_data, sample_rate = sf.read(io.BytesIO(audio_bytes))
        
        # ëª¨ë…¸ë¡œ ë³€í™˜
        if len(audio_data.shape) > 1:
            audio_data = np.mean(audio_data, axis=1)
        
        # ì „ì‚¬
        segments, info = whisper_model.transcribe(
            audio_data,
            language="ko",
            task="transcribe"
        )
        
        # ê²°ê³¼ ì •ë¦¬
        text = " ".join([segment.text for segment in segments])
        
        return {
            "success": True,
            "text": text,
            "language": info.language,
            "duration": info.duration
        }
        
    except Exception as e:
        logger.error(f"Transcription error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
    finally:
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)