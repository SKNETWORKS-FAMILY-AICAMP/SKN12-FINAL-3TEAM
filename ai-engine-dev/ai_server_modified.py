"""
Modified load_qwen3 function to use local qwen3_lora_ttalkkac_4b model
This file contains only the modified function to replace in ai_server_final_with_triplets.py
"""

import os
import torch
import logging
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

logger = logging.getLogger(__name__)

def load_qwen3():
    """Qwen3 4B LoRA ëª¨ë¸ ë¡œë”© (ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©)"""
    global qwen_model, qwen_tokenizer
    
    if qwen_model is None or qwen_tokenizer is None:
        logger.info("ğŸš€ Loading Qwen3 4B LoRA model...")
        
        try:
            # Config ì„í¬íŠ¸ (ìˆëŠ” ê²½ìš°)
            try:
                from config import Config
                lora_path = Config.QWEN_LORA_4B
                base_model_name = Config.QWEN_BASE_MODEL
                use_vllm = Config.QWEN_USE_VLLM
            except ImportError:
                # Config ì—†ìœ¼ë©´ ì§ì ‘ ê²½ë¡œ ì„¤ì •
                # RunPodì—ì„œëŠ” /workspace ê²½ë¡œ ì‚¬ìš©
                if os.path.exists("/workspace"):
                    lora_path = "/workspace/TtalKkac/ai-engine-dev/models/qwen_lora/qwen3_lora_ttalkkac_4b"
                else:
                    # ë¡œì»¬ Windows ê²½ë¡œ
                    lora_path = "C:/Users/SH/Desktop/TtalKkac/ai-engine-dev/qwen3_lora_ttalkkac_4b"
                
                # ë² ì´ìŠ¤ ëª¨ë¸ (LoRAê°€ í•™ìŠµëœ ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ì¼ì¹˜í•´ì•¼ í•¨)
                # 4B ëª¨ë¸ì€ ë³´í†µ Qwen2.5-7B-Instruct ë² ì´ìŠ¤ ì‚¬ìš©
                base_model_name = os.getenv("QWEN_BASE_MODEL", "Qwen/Qwen2.5-7B-Instruct")
                use_vllm = os.getenv("USE_VLLM", "false").lower() == "true"
            
            # ê²½ë¡œ í™•ì¸
            lora_path = Path(lora_path)
            if not lora_path.exists():
                raise FileNotFoundError(f"LoRA model not found at {lora_path}")
            
            logger.info(f"ğŸ“ LoRA path: {lora_path}")
            logger.info(f"ğŸ¤– Base model: {base_model_name}")
            
            # VLLM ì‚¬ìš© ì‹œë„
            if use_vllm:
                try:
                    logger.info("âš¡ Attempting to use VLLM for inference...")
                    from vllm import LLM
                    from vllm.lora.request import LoRARequest
                    
                    # VLLM with LoRA support
                    qwen_model = LLM(
                        model=base_model_name,
                        enable_lora=True,
                        max_lora_rank=64,
                        tensor_parallel_size=1,
                        gpu_memory_utilization=0.8,
                        trust_remote_code=True,
                        max_model_len=8192,  # 4B ëª¨ë¸ìš© ì¶•ì†Œ
                        dtype="float16"
                    )
                    
                    # LoRA ì–´ëŒ‘í„° ë“±ë¡
                    lora_request = LoRARequest(
                        "ttalkkac_4b",
                        1,
                        str(lora_path)
                    )
                    
                    # í† í¬ë‚˜ì´ì € ë¡œë“œ
                    qwen_tokenizer = AutoTokenizer.from_pretrained(
                        str(lora_path),  # LoRA í´ë”ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ
                        trust_remote_code=True
                    )
                    
                    # í† í¬ë‚˜ì´ì €ê°€ ì—†ìœ¼ë©´ ë² ì´ìŠ¤ ëª¨ë¸ì—ì„œ ë¡œë“œ
                    if qwen_tokenizer is None or not hasattr(qwen_tokenizer, 'apply_chat_template'):
                        qwen_tokenizer = AutoTokenizer.from_pretrained(
                            base_model_name,
                            trust_remote_code=True
                        )
                    
                    logger.info("âœ… VLLM with LoRA loaded successfully")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ VLLM loading failed: {e}")
                    logger.info("ğŸ”„ Falling back to Transformers + PEFT...")
                    use_vllm = False
            
            # Transformers + PEFT ë°©ì‹ (ê¸°ë³¸)
            if not use_vllm:
                logger.info("ğŸ“š Using Transformers + PEFT for LoRA model")
                
                # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ (LoRA í´ë”ì—ì„œ ë¨¼ì € ì‹œë„)
                try:
                    qwen_tokenizer = AutoTokenizer.from_pretrained(
                        str(lora_path),
                        trust_remote_code=True
                    )
                    logger.info(f"âœ… Tokenizer loaded from LoRA path")
                except Exception as e:
                    logger.info(f"âš ï¸ Loading tokenizer from base model instead: {e}")
                    qwen_tokenizer = AutoTokenizer.from_pretrained(
                        base_model_name,
                        trust_remote_code=True
                    )
                
                # íŒ¨ë”© í† í° ì„¤ì •
                if qwen_tokenizer.pad_token is None:
                    qwen_tokenizer.pad_token = qwen_tokenizer.eos_token
                
                # 2. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
                logger.info(f"Loading base model: {base_model_name}")
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë¡œë”©
                qwen_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float16,  # FP16 ì‚¬ìš©
                    device_map="auto",  # ìë™ ë””ë°”ì´ìŠ¤ ë§¤í•‘
                    trust_remote_code=True,
                    use_cache=True,  # KV ìºì‹œ ì‚¬ìš©
                    low_cpu_mem_usage=True  # CPU ë©”ëª¨ë¦¬ ì ˆì•½
                )
                
                # 3. LoRA ì–´ëŒ‘í„° ë¡œë“œ
                logger.info(f"Loading LoRA adapter from {lora_path}")
                qwen_model = PeftModel.from_pretrained(
                    qwen_model,
                    str(lora_path),
                    torch_dtype=torch.float16
                )
                
                # 4. ì¶”ë¡  ëª¨ë“œ ì„¤ì • (ì„ íƒì  ë³‘í•©)
                merge_adapter = os.getenv("MERGE_LORA_ADAPTER", "false").lower() == "true"
                if merge_adapter:
                    logger.info("ğŸ”€ Merging LoRA adapter with base model...")
                    qwen_model = qwen_model.merge_and_unload()
                    logger.info("âœ… LoRA adapter merged")
                else:
                    # ë³‘í•©í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
                    qwen_model.eval()
                
                # GPUë¡œ ì´ë™ (device_map="auto"ê°€ ì²˜ë¦¬ ëª»í•œ ê²½ìš°)
                if torch.cuda.is_available() and not hasattr(qwen_model, 'device'):
                    qwen_model = qwen_model.cuda()
                    logger.info("ğŸ® Model moved to GPU")
                
                logger.info("âœ… Qwen3 4B LoRA model loaded successfully")
                
                # ëª¨ë¸ ì •ë³´ ì¶œë ¥
                total_params = sum(p.numel() for p in qwen_model.parameters())
                trainable_params = sum(p.numel() for p in qwen_model.parameters() if p.requires_grad)
                logger.info(f"ğŸ“Š Model info:")
                logger.info(f"   - Total parameters: {total_params:,}")
                logger.info(f"   - Trainable parameters: {trainable_params:,}")
                logger.info(f"   - Device: {next(qwen_model.parameters()).device}")
                
        except FileNotFoundError as e:
            logger.error(f"âŒ LoRA model file not found: {e}")
            logger.error(f"Please ensure the model exists at: {lora_path}")
            raise e
            
        except Exception as e:
            logger.error(f"âŒ Qwen3 4B LoRA loading failed: {e}")
            
            # ë””ë²„ê¹… ì •ë³´
            logger.error("Debug info:")
            logger.error(f"  - LoRA path exists: {lora_path.exists() if isinstance(lora_path, Path) else os.path.exists(lora_path)}")
            logger.error(f"  - Current directory: {os.getcwd()}")
            logger.error(f"  - Python path: {sys.path}")
            
            # ëŒ€ì²´ ë°©ì•ˆ ì œì‹œ
            logger.info("ğŸ’¡ Alternatives:")
            logger.info("1. Check if the LoRA model path is correct")
            logger.info("2. Ensure PEFT is installed: pip install peft")
            logger.info("3. Try using the base model without LoRA")
            
            raise e
    
    return qwen_model, qwen_tokenizer

# ì¶”ê°€ í—¬í¼ í•¨ìˆ˜: ëª¨ë¸ ì •ë³´ í™•ì¸
def check_model_info():
    """ë¡œë“œëœ ëª¨ë¸ ì •ë³´ í™•ì¸"""
    global qwen_model, qwen_tokenizer
    
    if qwen_model is None:
        return {"status": "not_loaded", "message": "Model not loaded"}
    
    info = {
        "status": "loaded",
        "model_type": type(qwen_model).__name__,
        "has_lora": hasattr(qwen_model, 'peft_config'),
    }
    
    if hasattr(qwen_model, 'peft_config'):
        info["lora_config"] = {
            "r": qwen_model.peft_config.r if hasattr(qwen_model.peft_config, 'r') else None,
            "lora_alpha": qwen_model.peft_config.lora_alpha if hasattr(qwen_model.peft_config, 'lora_alpha') else None,
        }
    
    if qwen_tokenizer:
        info["tokenizer"] = {
            "vocab_size": qwen_tokenizer.vocab_size,
            "model_max_length": qwen_tokenizer.model_max_length,
        }
    
    # GPU ë©”ëª¨ë¦¬ ìƒíƒœ
    if torch.cuda.is_available():
        info["gpu_memory"] = {
            "allocated": f"{torch.cuda.memory_allocated() / 1024**3:.2f} GB",
            "reserved": f"{torch.cuda.memory_reserved() / 1024**3:.2f} GB",
        }
    
    return info