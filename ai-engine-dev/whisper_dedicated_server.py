#!/usr/bin/env python3
"""
WhisperX ì „ìš© ì„œë²„ (í¬íŠ¸ 8001)
ai_server_final_with_triplets.pyì˜ WhisperX ë¶€ë¶„ë§Œ ë¶„ë¦¬
"""

import os
import tempfile
import logging
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager
import time

import torch
import numpy as np
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ê¸€ë¡œë²Œ ëª¨ë¸ ë³€ìˆ˜
whisper_model = None

# ì‘ë‹µ ëª¨ë¸ (ì›ë³¸ê³¼ ë™ì¼)
class TranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    gpu_available: bool
    gpu_count: int
    model_loaded: bool
    memory_info: Optional[Dict[str, float]] = None

def load_whisperx():
    """WhisperX ëª¨ë¸ ë¡œë”© (ì›ë³¸ê³¼ ë™ì¼í•œ ë¡œì§)"""
    global whisper_model
    
    if whisper_model is None:
        logger.info("ğŸ¤ Loading WhisperX...")
        try:
            # faster-whisperë¥¼ ì§ì ‘ ì‚¬ìš©
            from faster_whisper import WhisperModel
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            compute_type = "float16" if device == "cuda" else "int8"
            
            # faster-whisper ëª¨ë¸ ë¡œë“œ
            try:
                whisper_model = WhisperModel("base", device=device, compute_type=compute_type)
                logger.info(f"âœ… Faster-Whisper base model loaded (device: {device})")
                return whisper_model
            except Exception as e:
                logger.warning(f"GPU ë¡œë“œ ì‹¤íŒ¨: {e}, CPU ì‹œë„...")
                whisper_model = WhisperModel("base", device="cpu", compute_type="int8")
                logger.info("âœ… Faster-Whisper base model loaded on CPU")
                return whisper_model
            
        except ImportError:
            # whisperx fallback
            try:
                import whisperx
                whisper_model = whisperx.load_model("base")
                logger.info("âœ… WhisperX base model loaded (fallback)")
                return whisper_model
            except Exception as e:
                logger.error(f"âŒ Both faster-whisper and whisperx failed: {e}")
                whisper_model = None
        except Exception as e:
            logger.error(f"âŒ WhisperX loading failed: {e}")
            whisper_model = None
    
    return whisper_model

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    logger.info("ğŸš€ Starting WhisperX Dedicated Server...")
    logger.info("ğŸ”§ Model preloading: Enabled")
    
    # WhisperX ì‚¬ì „ ë¡œë”©
    start_time = time.time()
    try:
        load_whisperx()
        load_time = time.time() - start_time
        logger.info(f"âœ… WhisperX loaded in {load_time:.2f} seconds")
    except Exception as e:
        logger.error(f"âŒ WhisperX preloading failed: {e}")
    
    yield
    
    # ì¢…ë£Œ ì‹œ
    logger.info("ğŸ‘‹ Shutting down WhisperX server...")
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="WhisperX Dedicated Server",
    description="Speech-to-Text service for TtalKkak",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "WhisperX Dedicated Server",
        "version": "1.0.0",
        "port": 8001,
        "features": [
            "WhisperX/Faster-Whisper Speech-to-Text",
            "Korean language support",
            "Segment-based transcription"
        ],
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    gpu_available = torch.cuda.is_available()
    gpu_count = torch.cuda.device_count() if gpu_available else 0
    
    memory_info = None
    if gpu_available:
        try:
            memory_info = {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3,
                "total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
        except:
            pass
    
    return HealthResponse(
        status="healthy",
        gpu_available=gpu_available,
        gpu_count=gpu_count,
        model_loaded=whisper_model is not None,
        memory_info=memory_info
    )

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(audio: UploadFile = File(...)):
    """ìŒì„± íŒŒì¼ ì „ì‚¬ (WhisperX) - ì›ë³¸ê³¼ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤"""
    try:
        logger.info(f"ğŸ¤ Transcribing audio: {audio.filename}")
        
        # ëª¨ë¸ ë¡œë”©
        model = load_whisperx()
        if model is None:
            raise HTTPException(status_code=503, detail="WhisperX model not loaded")
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì„ì‹œ ì €ì¥
        audio_content = await audio.read()
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_file.write(audio_content)
            temp_path = temp_file.name
        
        try:
            # faster-whisper ë°©ì‹
            if hasattr(model, 'transcribe') and 'WhisperModel' in str(type(model)):
                # faster-whisper transcribe
                segments_generator, info = model.transcribe(
                    temp_path,
                    language="ko",
                    task="transcribe",
                    beam_size=5,
                    best_of=5,
                    patience=1,
                    length_penalty=1,
                    temperature=0.0,
                    compression_ratio_threshold=2.4,
                    log_prob_threshold=-1.0,
                    no_speech_threshold=0.6
                )
                
                # ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                segments = []
                for segment in segments_generator:
                    segments.append({
                        "id": len(segments),
                        "start": segment.start,
                        "end": segment.end,
                        "text": segment.text.strip(),
                        "tokens": segment.tokens if hasattr(segment, 'tokens') else None,
                        "temperature": segment.temperature if hasattr(segment, 'temperature') else 0.0,
                        "avg_logprob": segment.avg_logprob if hasattr(segment, 'avg_logprob') else 0.0,
                        "compression_ratio": segment.compression_ratio if hasattr(segment, 'compression_ratio') else 1.0,
                        "no_speech_prob": segment.no_speech_prob if hasattr(segment, 'no_speech_prob') else 0.0
                    })
                
                full_text = " ".join([seg["text"] for seg in segments])
                
                result = {
                    "segments": segments,
                    "full_text": full_text,
                    "language": info.language if hasattr(info, 'language') else "ko",
                    "duration": info.duration if hasattr(info, 'duration') else sum([seg["end"] - seg["start"] for seg in segments])
                }
            else:
                # whisperx ë°©ì‹ (fallback)
                result_raw = model.transcribe(temp_path, batch_size=16)
                segments = result_raw.get("segments", [])
                full_text = " ".join([seg.get("text", "") for seg in segments])
                
                result = {
                    "segments": segments,
                    "full_text": full_text,
                    "language": result_raw.get("language", "ko"),
                    "duration": sum([seg.get("end", 0) - seg.get("start", 0) for seg in segments])
                }
            
            logger.info(f"âœ… Transcription completed: {len(full_text)} characters")
            
            return TranscriptionResponse(
                success=True,
                transcription=result
            )
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.unlink(temp_path)
            except:
                pass
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Transcription error: {e}")
        return TranscriptionResponse(
            success=False,
            error=str(e)
        )

@app.post("/transcribe-segments")
async def transcribe_segments(audio: UploadFile = File(...)):
    """ì„¸ê·¸ë¨¼íŠ¸ë³„ ìƒì„¸ ì „ì‚¬ (ì¶”ê°€ ê¸°ëŠ¥)"""
    try:
        result = await transcribe_audio(audio)
        
        if result.success and result.transcription:
            # ì„¸ê·¸ë¨¼íŠ¸ë³„ ìƒì„¸ ì •ë³´ ì¶”ê°€
            segments_detail = []
            for seg in result.transcription.get("segments", []):
                segments_detail.append({
                    "id": seg.get("id"),
                    "start": seg.get("start"),
                    "end": seg.get("end"),
                    "duration": seg.get("end", 0) - seg.get("start", 0),
                    "text": seg.get("text", ""),
                    "confidence": seg.get("avg_logprob", 0) if "avg_logprob" in seg else None
                })
            
            return {
                "success": True,
                "segments": segments_detail,
                "total_duration": result.transcription.get("duration"),
                "language": result.transcription.get("language"),
                "full_text": result.transcription.get("full_text")
            }
        else:
            return result
            
    except Exception as e:
        logger.error(f"âŒ Segment transcription error: {e}")
        return {"success": False, "error": str(e)}

@app.get("/models/status")
async def model_status():
    """ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    return {
        "whisperx": {
            "loaded": whisper_model is not None,
            "type": str(type(whisper_model)) if whisper_model else None,
            "device": "cuda" if torch.cuda.is_available() else "cpu"
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)