#!/usr/bin/env python3
"""
WhisperX ì „ìš© ì„œë²„ (í¬íŠ¸ 8001)
ìŒì„± ì „ì‚¬ë§Œ ë‹´ë‹¹ - íŒ¨í‚¤ì§€ ì¶©ëŒ ë°©ì§€
"""

import os
import tempfile
import logging
from typing import Optional, Dict, Any
from contextlib import asynccontextmanager
import time

import torch
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ê¸€ë¡œë²Œ ëª¨ë¸ ë³€ìˆ˜
whisper_model = None

class TranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

def load_whisperx():
    """WhisperX ëª¨ë¸ ë¡œë”©"""
    global whisper_model
    
    if whisper_model is None:
        logger.info("ğŸ¤ Loading WhisperX...")
        try:
            from faster_whisper import WhisperModel
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            compute_type = "float16" if device == "cuda" else "int8"
            
            whisper_model = WhisperModel("base", device=device, compute_type=compute_type)
            logger.info(f"âœ… Faster-Whisper base model loaded (device: {device})")
            return whisper_model
            
        except Exception as e:
            logger.error(f"âŒ WhisperX loading failed: {e}")
            whisper_model = None
    
    return whisper_model

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œ
    logger.info("ğŸš€ Starting WhisperX Server...")
    start_time = time.time()
    load_whisperx()
    logger.info(f"âœ… WhisperX loaded in {time.time() - start_time:.2f} seconds")
    yield
    # ì¢…ë£Œ ì‹œ
    logger.info("ğŸ‘‹ Shutting down WhisperX server...")

app = FastAPI(
    title="WhisperX Server",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "whisperx_loaded": whisper_model is not None,
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    }

@app.post("/transcribe")
async def transcribe_audio(audio: UploadFile = File(...)):
    """ìŒì„± íŒŒì¼ ì „ì‚¬"""
    try:
        logger.info(f"ğŸ¤ Transcribing: {audio.filename}")
        
        model = load_whisperx()
        if model is None:
            raise HTTPException(status_code=503, detail="WhisperX model not loaded")
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì„ì‹œ ì €ì¥
        audio_content = await audio.read()
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_file.write(audio_content)
            temp_path = temp_file.name
        
        try:
            # faster-whisper transcribe
            segments_generator, info = model.transcribe(
                temp_path,
                language="ko",
                task="transcribe",
                beam_size=5
            )
            
            # ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            segments = []
            for segment in segments_generator:
                segments.append({
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment.text.strip()
                })
            
            full_text = " ".join([seg["text"] for seg in segments])
            
            logger.info(f"âœ… Transcription completed: {len(full_text)} chars")
            
            return TranscriptionResponse(
                success=True,
                transcription={
                    "segments": segments,
                    "full_text": full_text,
                    "language": "ko",
                    "duration": info.duration if hasattr(info, 'duration') else 0
                }
            )
            
        finally:
            os.unlink(temp_path)
                
    except Exception as e:
        logger.error(f"âŒ Transcription error: {e}")
        return TranscriptionResponse(success=False, error=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)