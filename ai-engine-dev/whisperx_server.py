#!/usr/bin/env python3
"""
WhisperX Ï†ÑÏö© ÏÑúÎ≤Ñ (Ìè¨Ìä∏ 8001)
ÏùåÏÑ± Ï†ÑÏÇ¨Îßå Îã¥Îãπ - Ìå®ÌÇ§ÏßÄ Ï∂©Îèå Î∞©ÏßÄ
"""

import os
import tempfile
import logging
from typing import Optional, Dict, Any
from contextlib import asynccontextmanager
import time

import torch
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Í∏ÄÎ°úÎ≤å Î™®Îç∏ Î≥ÄÏàò
whisper_model = None

class TranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

def load_whisperx():
    """WhisperX Î™®Îç∏ Î°úÎî©"""
    global whisper_model
    
    if whisper_model is None:
        logger.info("üé§ Loading WhisperX...")
        try:
            from faster_whisper import WhisperModel
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            compute_type = "float16" if device == "cuda" else "int8"  # GPUÎäî float16Ïù¥ Îçî ÏïàÏ†ïÏ†Å
            
            # GPU Î©îÎ™®Î¶¨ ÌôïÏù∏
            if device == "cuda":
                logger.info(f"üéÆ GPU Available: {torch.cuda.get_device_name(0)}")
                logger.info(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
            
            whisper_model = WhisperModel(
                "base", 
                device=device, 
                compute_type=compute_type,
                cpu_threads=8 if device == "cpu" else 0,  # CPUÏùº ÎïåÎßå Ïä§Î†àÎìú ÏÑ§Ï†ï
                num_workers=1  # Îã®Ïùº ÏõåÏª§Î°ú ÏïàÏ†ïÌôî
            )
            logger.info(f"‚úÖ Faster-Whisper base model loaded (device: {device}, compute_type: {compute_type})")
            return whisper_model
            
        except Exception as e:
            logger.error(f"‚ùå WhisperX loading failed: {e}")
            whisper_model = None
    
    return whisper_model

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ÏãúÏûë Ïãú
    logger.info("üöÄ Starting WhisperX Server...")
    start_time = time.time()
    load_whisperx()
    logger.info(f"‚úÖ WhisperX loaded in {time.time() - start_time:.2f} seconds")
    yield
    # Ï¢ÖÎ£å Ïãú
    logger.info("üëã Shutting down WhisperX server...")

app = FastAPI(
    title="WhisperX Server",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "whisperx_loaded": whisper_model is not None,
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    }

@app.post("/transcribe")
async def transcribe_audio(audio: UploadFile = File(...)):
    """ÏùåÏÑ± ÌååÏùº Ï†ÑÏÇ¨"""
    try:
        logger.info(f"üé§ Transcribing: {audio.filename}")
        
        model = load_whisperx()
        if model is None:
            raise HTTPException(status_code=503, detail="WhisperX model not loaded")
        
        # Ïò§ÎîîÏò§ ÌååÏùº ÏûÑÏãú Ï†ÄÏû•
        audio_content = await audio.read()
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_file.write(audio_content)
            temp_path = temp_file.name
        
        try:
            # faster-whisper transcribe
            # GPU ÏÇ¨Ïö© ÌôïÏù∏ Î°úÍπÖ
            logger.info(f"üñ•Ô∏è Using device: {model.device}")
            logger.info(f"üìä Model compute type: {model.model.compute_type if hasattr(model.model, 'compute_type') else 'unknown'}")
            
            segments_generator, info = model.transcribe(
                temp_path,
                language="ko",
                task="transcribe",
                beam_size=1  # Îπ†Î•∏ Ï≤òÎ¶¨Î•º ÏúÑÌï¥ beam_size Ï∂ïÏÜå
            )
            
            # ÏÑ∏Í∑∏Î®ºÌä∏ Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôò (ÏßÑÌñâ ÏÉÅÌô© Î°úÍπÖ)
            segments = []
            segment_count = 0
            last_log_time = time.time()
            
            for segment in segments_generator:
                segments.append({
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment.text.strip()
                })
                segment_count += 1
                
                # 5Ï¥àÎßàÎã§ ÏßÑÌñâ ÏÉÅÌô© Î°úÍπÖ
                if time.time() - last_log_time > 5:
                    logger.info(f"üìù Processing... {segment_count} segments, current time: {segment.end:.1f}s")
                    last_log_time = time.time()
            
            full_text = " ".join([seg["text"] for seg in segments])
            
            logger.info(f"‚úÖ Transcription completed: {len(full_text)} chars")
            
            return TranscriptionResponse(
                success=True,
                transcription={
                    "segments": segments,
                    "full_text": full_text,
                    "language": "ko",
                    "duration": info.duration if hasattr(info, 'duration') else 0
                }
            )
            
        finally:
            os.unlink(temp_path)
                
    except Exception as e:
        logger.error(f"‚ùå Transcription error: {e}")
        return TranscriptionResponse(success=False, error=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)