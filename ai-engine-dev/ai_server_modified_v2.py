#!/usr/bin/env python3
"""
TtalKkak Î©îÏù∏ AI ÏÑúÎ≤Ñ - WhisperX Î∂ÑÎ¶¨ Î≤ÑÏ†Ñ
WhisperXÎäî ÏõêÍ≤© ÏÑúÎ≤Ñ(8001)Î°ú Ìò∏Ï∂ú, ÎÇòÎ®∏ÏßÄÎäî Î°úÏª¨ Ï≤òÎ¶¨
ÏõêÎ≥∏ ai_server_final_with_triplets.pyÏóêÏÑú WhisperX Î∂ÄÎ∂ÑÎßå ÏàòÏ†ï
"""

import os
import io
import json
import tempfile
import logging
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager
import time
import httpx
import asyncio

import torch
import numpy as np
from fastapi import FastAPI, File, UploadFile, HTTPException, Body, Form, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# Î°úÏª¨ Î™®Îìà ÏûÑÌè¨Ìä∏ (ÏõêÎ≥∏Í≥º ÎèôÏùº)
from task_schemas import (
    TaskItem, SubTask, MeetingAnalysisResult, ComplexityAnalysis, 
    TaskExpansionRequest, TaskExpansionResult, PipelineRequest, PipelineResult,
    calculate_task_complexity_advanced, validate_task_dependencies, 
    generate_task_id_sequence, TASK_SCHEMA_EXAMPLE
)
from meeting_analysis_prompts import (
    generate_meeting_analysis_system_prompt,
    generate_meeting_analysis_user_prompt,
    generate_task_expansion_system_prompt,
    generate_complexity_analysis_prompt,
    validate_meeting_analysis
)
from prd_generation_prompts import (
    generate_notion_project_prompt,
    generate_task_master_prd_prompt,
    format_notion_project,
    format_task_master_prd,
    validate_notion_project,
    validate_task_master_prd,
    NOTION_PROJECT_SCHEMA,
    TASK_MASTER_PRD_SCHEMA
)

import uvicorn

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Triplet + BERT Î™®Îìà ÏûÑÌè¨Ìä∏
try:
    from triplet_processor import get_triplet_processor
    from bert_classifier import get_bert_classifier
    TRIPLET_AVAILABLE = True
    print("‚úÖ Triplet + BERT Î™®Îìà Î°úÎìú ÏÑ±Í≥µ")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Triplet + BERT Î™®Îìà Î°úÎìú Ïã§Ìå®: {e}")
    TRIPLET_AVAILABLE = False

# WhisperX ÏÑúÎ≤Ñ URL
WHISPERX_SERVER = "http://localhost:8001"

# Í∏ÄÎ°úÎ≤å Î™®Îç∏ Î≥ÄÏàò (WhisperX Ï†úÏô∏)
qwen_model = None
qwen_tokenizer = None
triplet_processor = None
bert_classifier = None

# === ÏõêÎ≥∏ ÌååÏùºÏùò Î™®Îì† Ìï®ÏàòÎì§ (WhisperX Í¥ÄÎ†® Ï†úÏô∏) ===

async def generate_tasks_from_prd(prd_data: dict, num_tasks: int = 5) -> List[TaskItem]:
    """PRDÏóêÏÑú Task Master Ïä§ÌÉÄÏùºÎ°ú ÌÉúÏä§ÌÅ¨ ÏÉùÏÑ± (ÏõêÎ≥∏Í≥º ÎèôÏùº)"""
    try:
        logger.info(f"üéØ Generating {num_tasks} tasks from PRD using Task Master approach...")
        
        from prd_generation_prompts import (
            generate_prd_to_tasks_system_prompt,
            generate_prd_to_tasks_user_prompt
        )
        
        system_prompt = generate_prd_to_tasks_system_prompt(num_tasks)
        user_prompt = generate_prd_to_tasks_user_prompt(prd_data, num_tasks)
        
        messages = [{"role": "user", "content": user_prompt}]
        
        if qwen_model and qwen_tokenizer:
            text = qwen_tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            inputs = qwen_tokenizer([text], return_tensors="pt").to(qwen_model.device)
            
            with torch.no_grad():
                outputs = qwen_model.generate(
                    **inputs,
                    max_new_tokens=2048,
                    temperature=0.3,
                    do_sample=True,
                    pad_token_id=qwen_tokenizer.eos_token_id
                )
            
            response = qwen_tokenizer.decode(
                outputs[0][len(inputs["input_ids"][0]):], 
                skip_special_tokens=True
            )
            
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                if json_end == -1:
                    json_content = response[json_start:].strip()
                else:
                    json_content = response[json_start:json_end].strip()
            else:
                json_content = response.strip()
            
            task_data = json.loads(json_content)
            
            task_items = []
            for task_info in task_data.get("tasks", []):
                task_item = TaskItem(
                    id=task_info.get("id", len(task_items) + 1),
                    title=task_info.get("title", f"Task {len(task_items) + 1}"),
                    description=task_info.get("description", ""),
                    details=task_info.get("details", ""),
                    priority=task_info.get("priority", "medium"),
                    status=task_info.get("status", "pending"),
                    dependencies=task_info.get("dependencies", []),
                    test_strategy=task_info.get("testStrategy", ""),
                    subtasks=[]
                )
                task_items.append(task_item)
            
            logger.info(f"‚úÖ Generated {len(task_items)} tasks from PRD")
            return task_items
        else:
            logger.error("‚ùå Qwen model not available for task generation")
            return []
            
    except Exception as e:
        logger.error(f"‚ùå Error generating tasks from PRD: {e}")
        return []

async def analyze_task_complexity(task_items: List[TaskItem]) -> dict:
    """Task Master Ïä§ÌÉÄÏùº Î≥µÏû°ÎèÑ Î∂ÑÏÑù (ÏõêÎ≥∏Í≥º ÎèôÏùº)"""
    try:
        logger.info("üîç Analyzing task complexity using Task Master approach...")
        
        from prd_generation_prompts import (
            generate_complexity_analysis_system_prompt,
            generate_complexity_analysis_prompt
        )
        
        tasks_data = {
            "tasks": [
                {
                    "id": task.id,
                    "title": task.title,
                    "description": task.description,
                    "details": task.details,
                    "priority": task.priority
                }
                for task in task_items
            ]
        }
        
        system_prompt = generate_complexity_analysis_system_prompt()
        user_prompt = generate_complexity_analysis_prompt(tasks_data)
        
        messages = [{"role": "user", "content": user_prompt}]
        
        if qwen_model and qwen_tokenizer:
            text = qwen_tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            inputs = qwen_tokenizer([text], return_tensors="pt").to(qwen_model.device)
            
            with torch.no_grad():
                outputs = qwen_model.generate(
                    **inputs,
                    max_new_tokens=2048,
                    temperature=0.3,
                    do_sample=True,
                    pad_token_id=qwen_tokenizer.eos_token_id
                )
            
            response = qwen_tokenizer.decode(
                outputs[0][len(inputs["input_ids"][0]):], 
                skip_special_tokens=True
            )
            
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                if json_end == -1:
                    json_content = response[json_start:].strip()
                else:
                    json_content = response[json_start:json_end].strip()
            else:
                first_bracket = response.find('[')
                last_bracket = response.rfind(']')
                if first_bracket != -1 and last_bracket > first_bracket:
                    json_content = response[first_bracket:last_bracket + 1]
                else:
                    json_content = response.strip()
            
            complexity_analysis = json.loads(json_content)
            
            complexity_map = {}
            for analysis in complexity_analysis:
                task_id = analysis.get("taskId")
                if task_id:
                    complexity_map[task_id] = analysis
            
            logger.info(f"‚úÖ Complexity analysis completed for {len(complexity_map)} tasks")
            return complexity_map
        else:
            logger.error("‚ùå Qwen model not available for complexity analysis")
            return {}
            
    except Exception as e:
        logger.error(f"‚ùå Error analyzing task complexity: {e}")
        return {}

def create_complexity_based_default_subtasks(task: TaskItem, task_analysis: dict) -> List[SubTask]:
    """Task Master Ïä§ÌÉÄÏùº: Î≥µÏû°ÎèÑ Í∏∞Î∞ò Í∏∞Î≥∏ ÏÑúÎ∏åÌÉúÏä§ÌÅ¨ ÏÉùÏÑ± (ÏõêÎ≥∏Í≥º ÎèôÏùº)"""
    default_subtasks = []
    
    complexity_score = task_analysis.get('complexityScore', 5)
    recommended_subtasks = task_analysis.get('recommendedSubtasks', 3)
    
    if complexity_score >= 8:
        templates = [
            {
                "title": "Architecture Design & Planning",
                "desc": f"Design system architecture and create detailed technical specifications for {task.title}",
                "details": "Create technical design documents, API specifications, database schemas, and implementation roadmap",
                "hours": 12,
                "priority": "high"
            },
            {
                "title": "Core Implementation",
                "desc": f"Implement the main functionality and core features of {task.title}",
                "details": "Develop core business logic, implement primary user flows, and establish data processing pipeline",
                "hours": 20,
                "priority": "high"
            },
            {
                "title": "Integration & Testing",
                "desc": f"Integrate components and conduct comprehensive testing for {task.title}",
                "details": "Perform unit testing, integration testing, and end-to-end testing with comprehensive test coverage",
                "hours": 10,
                "priority": "medium"
            }
        ]
    elif complexity_score >= 5:
        templates = [
            {
                "title": "Requirements Analysis & Design",
                "desc": f"Analyze requirements and create implementation plan for {task.title}",
                "details": "Define functional requirements, create wireframes, and establish development approach",
                "hours": 6,
                "priority": "high"
            },
            {
                "title": "Implementation & Development",
                "desc": f"Develop and implement the main features of {task.title}",
                "details": "Code implementation following best practices, implement business logic, and create user interfaces",
                "hours": 12,
                "priority": "high"
            }
        ]
    else:
        templates = [
            {
                "title": "Setup & Preparation",
                "desc": f"Set up environment and prepare resources for {task.title}",
                "details": "Configure development environment, gather required resources, and create initial project structure",
                "hours": 3,
                "priority": "medium"
            },
            {
                "title": "Implementation",
                "desc": f"Implement the required functionality for {task.title}",
                "details": "Code implementation with focus on clean, maintainable code following established patterns",
                "hours": 6,
                "priority": "high"
            }
        ]
    
    for i in range(min(recommended_subtasks, len(templates))):
        template = templates[i]
        subtask = SubTask(
            id=i + 1,
            title=template["title"],
            description=template["desc"],
            priority=template["priority"],
            estimated_hours=template["hours"],
            status="pending"
        )
        default_subtasks.append(subtask)
    
    return default_subtasks

# ÏùëÎãµ Î™®Îç∏Îì§ (ÏõêÎ≥∏Í≥º ÎèôÏùº)
class TranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class EnhancedTranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    triplet_data: Optional[Dict[str, Any]] = None
    filtered_transcript: Optional[str] = None
    processing_stats: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class AnalysisRequest(BaseModel):
    transcript: str
    num_tasks: int = 5
    additional_context: Optional[str] = None

class NotionProjectResponse(BaseModel):
    success: bool
    notion_project: Optional[Dict[str, Any]] = None
    formatted_notion: Optional[str] = None
    error: Optional[str] = None

class TaskMasterPRDResponse(BaseModel):
    success: bool
    prd_data: Optional[Dict[str, Any]] = None
    formatted_prd: Optional[str] = None
    error: Optional[str] = None

class TwoStageAnalysisRequest(BaseModel):
    transcript: str
    generate_notion: bool = True
    generate_tasks: bool = True
    num_tasks: int = 5
    additional_context: Optional[str] = None
    auto_expand_tasks: bool = True

class TwoStageAnalysisResponse(BaseModel):
    success: bool
    stage1_notion: Optional[Dict[str, Any]] = None
    stage2_prd: Optional[Dict[str, Any]] = None
    stage3_tasks: Optional[MeetingAnalysisResult] = None
    formatted_notion: Optional[str] = None
    formatted_prd: Optional[str] = None
    processing_time: Optional[float] = None
    error: Optional[str] = None

class EnhancedTwoStageResult(BaseModel):
    success: bool
    triplet_stats: Optional[Dict[str, Any]] = None
    classification_stats: Optional[Dict[str, Any]] = None
    stage1_notion: Optional[Dict[str, Any]] = None
    stage2_prd: Optional[Dict[str, Any]] = None
    stage3_tasks: Optional[MeetingAnalysisResult] = None
    formatted_notion: Optional[str] = None
    formatted_prd: Optional[str] = None
    original_transcript_length: Optional[int] = None
    filtered_transcript_length: Optional[int] = None
    noise_reduction_ratio: Optional[float] = None
    processing_time: Optional[float] = None
    error: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    gpu_available: bool
    gpu_count: int
    models_loaded: Dict[str, bool]
    memory_info: Optional[Dict[str, float]] = None

def load_qwen3():
    """Qwen3-4B LoRA Î™®Îç∏ Î°úÎî© (ÏõêÎ≥∏Í≥º ÎèôÏùº)"""
    global qwen_model, qwen_tokenizer
    
    if qwen_model is None or qwen_tokenizer is None:
        logger.info("üöÄ Loading Qwen3-4B LoRA...")
        try:
            logger.info("üìö Using Transformers")
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            if os.path.exists("/workspace"):
                model_name = "/workspace/SKN12-FINAL-3TEAM/ai-engine-dev/qwen3_lora_ttalkkac_4b"
            else:
                model_name = "C:/Users/SH/Desktop/TtalKkac/ai-engine-dev/qwen3_lora_ttalkkac_4b"
            
            # ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú
            try:
                qwen_tokenizer = AutoTokenizer.from_pretrained(
                    model_name, 
                    trust_remote_code=True,
                    use_fast=True
                )
                logger.info("‚úÖ Î°úÏª¨ Qwen3 ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú ÏÑ±Í≥µ")
            except Exception as e:
                logger.warning(f"Î°úÏª¨ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ïã§Ìå®: {e}")
                qwen_tokenizer = AutoTokenizer.from_pretrained(
                    "Qwen/Qwen2-1.5B-Instruct", 
                    trust_remote_code=True,
                    use_fast=True
                )
                logger.info("‚úÖ Qwen2 ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú ÏÑ±Í≥µ (fallback)")
            
            # Qwen Î™®Îç∏ Î°úÎìú
            try:
                # config.jsonÏù¥ qwen2Î°ú ÏàòÏ†ïÎêú ÏÉÅÌÉú Í∞ÄÏ†ï
                qwen_model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    trust_remote_code=True,
                    device_map="auto" if torch.cuda.is_available() else None
                )
                logger.info("‚úÖ Î°úÏª¨ Qwen3 Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ")
            except Exception as e:
                logger.warning(f"Î°úÏª¨ Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
                # Fallback to base model
                base_model_name = "Qwen/Qwen2-1.5B-Instruct"
                qwen_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float16,
                    trust_remote_code=True
                )
                if torch.cuda.is_available():
                    qwen_model = qwen_model.cuda()
                logger.info("‚úÖ Qwen2 base Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ (fallback)")
            
            logger.info("‚úÖ Qwen Î™®Îç∏ Î°úÎìú ÏôÑÎ£å")
            
        except Exception as e:
            logger.error(f"‚ùå Qwen3-4B LoRA loading failed: {e}")
            raise e
    
    return qwen_model, qwen_tokenizer

def generate_structured_response(
    system_prompt: str, 
    user_prompt: str, 
    response_schema: Dict[str, Any],
    temperature: float = 0.3,
    max_input_tokens: int = 28000,
    enable_chunking: bool = True
) -> Dict[str, Any]:
    """Íµ¨Ï°∞ÌôîÎêú ÏùëÎãµ ÏÉùÏÑ± (ÏõêÎ≥∏Í≥º ÎèôÏùº)"""
    
    try:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        text = qwen_tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        inputs = qwen_tokenizer([text], return_tensors="pt")
        if torch.cuda.is_available():
            inputs = inputs.to("cuda")
        
        with torch.no_grad():
            outputs = qwen_model.generate(
                **inputs,
                max_new_tokens=4096,
                temperature=temperature,
                do_sample=True,
                top_p=0.9,
                pad_token_id=qwen_tokenizer.eos_token_id
            )
        
        response = qwen_tokenizer.decode(
            outputs[0][len(inputs["input_ids"][0]):], 
            skip_special_tokens=True
        )
        
        if "```json" in response:
            json_start = response.find("```json") + 7
            json_end = response.find("```", json_start)
            if json_end == -1:
                json_content = response[json_start:].strip()
            else:
                json_content = response[json_start:json_end].strip()
        else:
            json_content = response.strip()
        
        result = json.loads(json_content)
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Response generation error: {e}")
        return {"error": str(e)}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ÏÑúÎ≤Ñ ÏÉùÎ™ÖÏ£ºÍ∏∞ Í¥ÄÎ¶¨"""
    logger.info("üöÄ Starting TtalKkak AI Server (WhisperX Remote)...")
    logger.info("üîß Model preloading: Enabled")
    
    global triplet_processor, bert_classifier
    
    # WhisperX ÏÑúÎ≤Ñ Ï≤¥ÌÅ¨
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{WHISPERX_SERVER}/health")
            if response.status_code == 200:
                logger.info("‚úÖ WhisperX server connected")
            else:
                logger.warning("‚ö†Ô∏è WhisperX server not responding")
    except:
        logger.warning("‚ö†Ô∏è WhisperX server not available at startup")
    
    # Qwen3 Î™®Îç∏ Î°úÎî©
    start_time = time.time()
    try:
        load_qwen3()
        logger.info(f"‚úÖ Qwen3-4B LoRA loaded in {time.time() - start_time:.2f} seconds")
    except Exception as e:
        logger.error(f"‚ùå Qwen3 loading failed: {e}")
    
    # BERT/Triplet Î°úÎî©
    if TRIPLET_AVAILABLE:
        try:
            triplet_processor = get_triplet_processor()
            bert_classifier = get_bert_classifier()
            logger.info("‚úÖ BERT + Triplet loaded")
        except Exception as e:
            logger.error(f"‚ùå BERT/Triplet loading failed: {e}")
    
    logger.info(f"‚è±Ô∏è Total loading time: {time.time() - start_time:.2f} seconds")
    
    yield
    
    logger.info("üëã Shutting down server...")
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# FastAPI Ïï± ÏÉùÏÑ± (ÏõêÎ≥∏Í≥º ÎèôÏùº)
app = FastAPI(
    title="TtalKkak Final AI Server with Triplets",
    description="WhisperX (Remote) + Triplet + BERT + Qwen3-4B + 2-Stage PRD Process",
    version="3.2.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    """Î£®Ìä∏ ÏóîÎìúÌè¨Ïù∏Ìä∏"""
    return {
        "message": "TtalKkak Final AI Server with Triplets",
        "version": "3.2.0",
        "features": [
            "WhisperX Speech-to-Text (Remote)",
            "Triplet Context Analysis", 
            "BERT Noise Filtering",
            "2-Stage PRD Process",
            "Notion Project Generation", 
            "Task Master PRD Format",
            "Advanced Task Generation"
        ],
        "workflow": "ÌöåÏùòÎ°ù ‚Üí Triplet ÌïÑÌÑ∞ÎßÅ ‚Üí Í∏∞ÌöçÏïà ‚Üí Task Master PRD ‚Üí ÏóÖÎ¨¥ÏÉùÏÑ±",
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Ìó¨Ïä§ Ï≤¥ÌÅ¨"""
    gpu_available = torch.cuda.is_available()
    gpu_count = torch.cuda.device_count() if gpu_available else 0
    
    memory_info = None
    if gpu_available:
        try:
            memory_info = {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3,
                "total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
        except:
            pass
    
    # WhisperX ÏÑúÎ≤Ñ ÏÉÅÌÉú Ï≤¥ÌÅ¨
    whisperx_loaded = False
    try:
        async with httpx.AsyncClient(timeout=2.0) as client:
            response = await client.get(f"{WHISPERX_SERVER}/health")
            if response.status_code == 200:
                whisperx_loaded = response.json().get("whisperx_loaded", False)
    except:
        pass
    
    return HealthResponse(
        status="healthy",
        gpu_available=gpu_available,
        gpu_count=gpu_count,
        models_loaded={
            "whisperx": whisperx_loaded,
            "qwen3": qwen_model is not None,
            "triplet_bert": TRIPLET_AVAILABLE
        },
        memory_info=memory_info
    )

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(audio: UploadFile = File(...)):
    """ÏùåÏÑ± ÌååÏùº Ï†ÑÏÇ¨ (WhisperX ÏõêÍ≤© Ìò∏Ï∂ú)"""
    try:
        logger.info(f"üé§ Transcribing audio via remote: {audio.filename}")
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            files = {"audio": (audio.filename, await audio.read(), audio.content_type)}
            response = await client.post(f"{WHISPERX_SERVER}/transcribe", files=files)
            
            if response.status_code == 200:
                return TranscriptionResponse(**response.json())
            else:
                return TranscriptionResponse(
                    success=False,
                    error=f"WhisperX server error: {response.status_code}"
                )
                
    except httpx.TimeoutException:
        return TranscriptionResponse(
            success=False,
            error="WhisperX server timeout"
        )
    except Exception as e:
        logger.error(f"‚ùå Transcription error: {e}")
        return TranscriptionResponse(
            success=False,
            error=str(e)
        )

@app.post("/transcribe-enhanced", response_model=EnhancedTranscriptionResponse)
async def transcribe_audio_enhanced(
    audio: UploadFile = File(...),
    enable_bert_filtering: bool = True,
    save_noise_log: bool = True
):
    """Ìñ•ÏÉÅÎêú ÏùåÏÑ± ÌååÏùº Ï†ÑÏÇ¨ (Triplet + BERT ÌïÑÌÑ∞ÎßÅ)"""
    try:
        logger.info(f"üé§ Enhanced transcribing: {audio.filename}")
        
        # 1. WhisperX ÏõêÍ≤© Ï†ÑÏÇ¨
        basic_result = await transcribe_audio(audio)
        if not basic_result.success:
            return EnhancedTranscriptionResponse(
                success=False,
                error=basic_result.error
            )
        
        # 2. Triplet ÌîÑÎ°úÏÑ∏ÏÑúÎ°ú Ï≤òÎ¶¨ (Î°úÏª¨)
        if TRIPLET_AVAILABLE and enable_bert_filtering:
            try:
                enhanced_result = triplet_processor.process_whisperx_result(
                    whisperx_result=basic_result.transcription,
                    enable_bert_filtering=enable_bert_filtering,
                    save_noise_log=save_noise_log
                )
                
                if enhanced_result["success"]:
                    logger.info("‚úÖ Enhanced transcription with Triplets completed")
                    
                    return EnhancedTranscriptionResponse(
                        success=True,
                        transcription=basic_result.transcription,
                        triplet_data=enhanced_result.get("triplet_data"),
                        filtered_transcript=enhanced_result.get("filtered_transcript"),
                        processing_stats=enhanced_result.get("processing_stats")
                    )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Triplet processing error: {e}")
        
        # 3. Triplet ÏÇ¨Ïö© Î∂àÍ∞ÄÎä• Ïãú Í∏∞Î≥∏ Í≤∞Í≥º Î∞òÌôò
        return EnhancedTranscriptionResponse(
            success=True,
            transcription=basic_result.transcription,
            filtered_transcript=basic_result.transcription["full_text"],
            processing_stats={"triplet_available": TRIPLET_AVAILABLE}
        )
        
    except Exception as e:
        logger.error(f"‚ùå Enhanced transcription error: {e}")
        return EnhancedTranscriptionResponse(
            success=False,
            error=str(e)
        )

# === ÎÇòÎ®∏ÏßÄ ÏóîÎìúÌè¨Ïù∏Ìä∏Îì§ÏùÄ ÏõêÎ≥∏Í≥º ÎèôÏùº (Î°úÏª¨ Ï≤òÎ¶¨) ===

@app.post("/generate-notion-project", response_model=NotionProjectResponse)
async def generate_notion_project(request: AnalysisRequest):
    """1Îã®Í≥Ñ: ÌöåÏùòÎ°ù ‚Üí ÎÖ∏ÏÖò Í∏∞ÌöçÏïà ÏÉùÏÑ±"""
    try:
        logger.info("üìù Stage 1: Generating Notion project document...")
        
        system_prompt = generate_meeting_analysis_system_prompt()
        user_prompt = generate_meeting_analysis_user_prompt(request.transcript)
        
        result = generate_structured_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=NOTION_PROJECT_SCHEMA,
            temperature=0.3
        )
        
        if "error" in result:
            return NotionProjectResponse(
                success=False,
                error=result["error"]
            )
        
        validated_result = validate_notion_project(result)
        formatted_notion = format_notion_project(validated_result)
        
        logger.info("‚úÖ Notion project document generated successfully")
        
        return NotionProjectResponse(
            success=True,
            notion_project=validated_result,
            formatted_notion=formatted_notion
        )
        
    except Exception as e:
        logger.error(f"‚ùå Notion generation error: {e}")
        return NotionProjectResponse(
            success=False,
            error=str(e)
        )

@app.post("/generate-task-master-prd", response_model=TaskMasterPRDResponse)
async def generate_task_master_prd(notion_data: Dict[str, Any] = Body(...)):
    """2Îã®Í≥Ñ: ÎÖ∏ÏÖò ‚Üí Task Master PRD ÏÉùÏÑ±"""
    try:
        logger.info("üìã Stage 2: Generating Task Master PRD...")
        
        system_prompt = generate_task_master_prd_prompt()
        user_prompt = generate_notion_project_prompt(notion_data)
        
        result = generate_structured_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=TASK_MASTER_PRD_SCHEMA,
            temperature=0.3
        )
        
        if "error" in result:
            return TaskMasterPRDResponse(
                success=False,
                error=result["error"]
            )
        
        validated_result = validate_task_master_prd(result)
        formatted_prd = format_task_master_prd(validated_result)
        
        logger.info("‚úÖ Task Master PRD generated successfully")
        
        return TaskMasterPRDResponse(
            success=True,
            prd_data=validated_result,
            formatted_prd=formatted_prd
        )
        
    except Exception as e:
        logger.error(f"‚ùå PRD generation error: {e}")
        return TaskMasterPRDResponse(
            success=False,
            error=str(e)
        )

@app.post("/two-stage-analysis", response_model=TwoStageAnalysisResponse)
async def two_stage_analysis(request: TwoStageAnalysisRequest):
    """2Îã®Í≥Ñ Î∂ÑÏÑù ÌååÏù¥ÌîÑÎùºÏù∏"""
    try:
        start_time = time.time()
        
        # Stage 1: ÎÖ∏ÏÖò ÌîÑÎ°úÏ†ùÌä∏ ÏÉùÏÑ±
        if request.generate_notion:
            logger.info("üìù Stage 1: Generating Notion project...")
            
            notion_request = AnalysisRequest(
                transcript=request.transcript,
                num_tasks=request.num_tasks,
                additional_context=request.additional_context
            )
            
            notion_result = await generate_notion_project(notion_request)
            
            if not notion_result.success:
                return TwoStageAnalysisResponse(
                    success=False,
                    error=notion_result.error
                )
            
            # Stage 2: Task Master PRD ÏÉùÏÑ±
            logger.info("üìã Stage 2: Generating Task Master PRD...")
            
            prd_result = await generate_task_master_prd(notion_result.notion_project)
            
            if not prd_result.success:
                return TwoStageAnalysisResponse(
                    success=False,
                    stage1_notion=notion_result.notion_project,
                    formatted_notion=notion_result.formatted_notion,
                    error=prd_result.error
                )
            
            # Stage 3: ÌÉúÏä§ÌÅ¨ ÏÉùÏÑ± (ÏÑ†ÌÉùÏ†Å)
            tasks_result = None
            if request.generate_tasks:
                logger.info("üéØ Stage 3: Generating tasks...")
                
                task_items = await generate_tasks_from_prd(
                    prd_result.prd_data,
                    request.num_tasks
                )
                
                if request.auto_expand_tasks and task_items:
                    complexity_map = await analyze_task_complexity(task_items)
                    
                    for task in task_items:
                        task_analysis = complexity_map.get(task.id, {})
                        task.subtasks = create_complexity_based_default_subtasks(task, task_analysis)
                
                tasks_result = MeetingAnalysisResult(
                    tasks=task_items,
                    metadata={
                        "total_tasks": len(task_items),
                        "auto_expanded": request.auto_expand_tasks
                    }
                )
            
            processing_time = time.time() - start_time
            
            return TwoStageAnalysisResponse(
                success=True,
                stage1_notion=notion_result.notion_project,
                stage2_prd=prd_result.prd_data,
                stage3_tasks=tasks_result,
                formatted_notion=notion_result.formatted_notion,
                formatted_prd=prd_result.formatted_prd,
                processing_time=processing_time
            )
        
        return TwoStageAnalysisResponse(
            success=False,
            error="No analysis requested"
        )
        
    except Exception as e:
        logger.error(f"‚ùå Two-stage analysis error: {e}")
        return TwoStageAnalysisResponse(
            success=False,
            error=str(e)
        )

@app.post("/two-stage-pipeline", response_model=EnhancedTwoStageResult)
async def two_stage_pipeline(
    audio: UploadFile = File(...),
    num_tasks: int = 5,
    enable_bert_filtering: bool = True,
    save_noise_log: bool = True,
    auto_expand_tasks: bool = True
):
    """ÏùåÏÑ± ÌååÏùº Í∏∞Î∞ò Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏"""
    try:
        start_time = time.time()
        
        # 1. Ìñ•ÏÉÅÎêú Ï†ÑÏÇ¨ (WhisperX ÏõêÍ≤© + Triplet Î°úÏª¨)
        enhanced_result = await transcribe_audio_enhanced(
            audio=audio,
            enable_bert_filtering=enable_bert_filtering,
            save_noise_log=save_noise_log
        )
        
        if not enhanced_result.success:
            return EnhancedTwoStageResult(
                success=False,
                error=enhanced_result.error
            )
        
        # 2. ÌïÑÌÑ∞ÎßÅÎêú ÌÖçÏä§Ìä∏Î°ú 2Îã®Í≥Ñ Î∂ÑÏÑù
        filtered_text = enhanced_result.filtered_transcript or enhanced_result.transcription["full_text"]
        
        analysis_request = TwoStageAnalysisRequest(
            transcript=filtered_text,
            num_tasks=num_tasks,
            auto_expand_tasks=auto_expand_tasks
        )
        
        analysis_result = await two_stage_analysis(analysis_request)
        
        processing_time = time.time() - start_time
        
        # ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
        original_length = len(enhanced_result.transcription["full_text"])
        filtered_length = len(filtered_text)
        noise_ratio = 1 - (filtered_length / original_length) if original_length > 0 else 0
        
        return EnhancedTwoStageResult(
            success=analysis_result.success,
            triplet_stats=enhanced_result.triplet_data,
            classification_stats=enhanced_result.processing_stats,
            stage1_notion=analysis_result.stage1_notion,
            stage2_prd=analysis_result.stage2_prd,
            stage3_tasks=analysis_result.stage3_tasks,
            formatted_notion=analysis_result.formatted_notion,
            formatted_prd=analysis_result.formatted_prd,
            original_transcript_length=original_length,
            filtered_transcript_length=filtered_length,
            noise_reduction_ratio=noise_ratio,
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"‚ùå Full pipeline error: {e}")
        return EnhancedTwoStageResult(
            success=False,
            error=str(e)
        )

@app.get("/models/status")
async def model_status():
    """Î™®Îç∏ ÏÉÅÌÉú ÌôïÏù∏"""
    whisperx_status = {"loaded": False, "server": "offline"}
    
    try:
        async with httpx.AsyncClient(timeout=2.0) as client:
            response = await client.get(f"{WHISPERX_SERVER}/health")
            if response.status_code == 200:
                whisperx_status = {
                    "loaded": response.json().get("whisperx_loaded", False),
                    "server": "online"
                }
    except:
        pass
    
    return {
        "whisperx": whisperx_status,
        "qwen3": {
            "loaded": qwen_model is not None,
            "tokenizer_loaded": qwen_tokenizer is not None
        },
        "bert": {
            "loaded": TRIPLET_AVAILABLE and bert_classifier is not None
        },
        "triplet": {
            "loaded": TRIPLET_AVAILABLE and triplet_processor is not None
        }
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)