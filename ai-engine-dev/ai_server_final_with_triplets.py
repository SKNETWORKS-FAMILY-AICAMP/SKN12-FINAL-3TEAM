"""
TtalKkak ìµœì¢… AI ì„œë²„ - Triplet + BERT í†µí•©
íšŒì˜ë¡ â†’ Triplet í•„í„°ë§ â†’ ê¸°íšì•ˆ â†’ Task Master PRD â†’ ì—…ë¬´ìƒì„±
"""

import os
import io
import json
import tempfile
import logging
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager
import time
import httpx  # WhisperX ì›ê²© ì„œë²„ í˜¸ì¶œìš©

import torch
import numpy as np
from fastapi import FastAPI, File, UploadFile, HTTPException, Body, Form, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from task_schemas import (
    TaskItem, SubTask, MeetingAnalysisResult, ComplexityAnalysis, 
    TaskExpansionRequest, TaskExpansionResult, PipelineRequest, PipelineResult,
    calculate_task_complexity_advanced, validate_task_dependencies, 
    generate_task_id_sequence, TASK_SCHEMA_EXAMPLE
)
from meeting_analysis_prompts import (
    generate_meeting_analysis_system_prompt,
    generate_meeting_analysis_user_prompt,
    generate_task_expansion_system_prompt,
    generate_complexity_analysis_prompt,
    validate_meeting_analysis
)
from prd_generation_prompts import (
    generate_notion_project_prompt,
    generate_task_master_prd_prompt,
    format_notion_project,
    format_task_master_prd,
    validate_notion_project,
    validate_task_master_prd,
    NOTION_PROJECT_SCHEMA,
    TASK_MASTER_PRD_SCHEMA
)

import uvicorn

# ë¡œê¹… ì„¤ì • (ëª¨ë“ˆ ì„í¬íŠ¸ ì „ì— ì„¤ì •)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Triplet + BERT ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from triplet_processor import get_triplet_processor
    from bert_classifier import get_bert_classifier
    TRIPLET_AVAILABLE = True
    print("âœ… Triplet + BERT ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logger.warning(f"âš ï¸ Triplet + BERT ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    TRIPLET_AVAILABLE = False

# WhisperX ì›ê²© ì„œë²„ URL
WHISPERX_SERVER = "http://localhost:8002"

# ê¸€ë¡œë²Œ ëª¨ë¸ ë³€ìˆ˜
whisper_model = None  # WhisperXëŠ” ì›ê²© ì„œë²„ì—ì„œ ì²˜ë¦¬
qwen_model = None
qwen_tokenizer = None

# Task Master ìŠ¤íƒ€ì¼ ë³µì¡ë„ ê¸°ë°˜ í”„ë¡œì„¸ìŠ¤ í•¨ìˆ˜ë“¤
async def generate_tasks_from_prd(prd_data: dict, num_tasks: int = 5) -> List[TaskItem]:
    """PRDì—ì„œ Task Master ìŠ¤íƒ€ì¼ë¡œ íƒœìŠ¤í¬ ìƒì„±"""
    try:
        logger.info(f"ğŸ¯ Generating {num_tasks} tasks from PRD using Task Master approach...")
        
        # Task Master PRD â†’ Task ìƒì„± í”„ë¡¬í”„íŠ¸
        from prd_generation_prompts import (
            generate_prd_to_tasks_system_prompt,
            generate_prd_to_tasks_user_prompt
        )
        
        system_prompt = generate_prd_to_tasks_system_prompt(num_tasks)
        user_prompt = generate_prd_to_tasks_user_prompt(prd_data, num_tasks)
        
        # JSON ì‘ë‹µ ê°•ì œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        json_enforced_prompt = f"""
{system_prompt}

**CRITICAL INSTRUCTIONS:**
- DO NOT use <think> or </think> tags
- DO NOT provide any explanations or thinking process
- START your response directly with a JSON object
- ONLY output valid JSON, nothing else
- ALL task titles, descriptions, and details MUST be in Korean (í•œêµ­ì–´)
- Technical terms can remain in English where appropriate

{user_prompt}

RESPOND WITH JSON ONLY (í•œêµ­ì–´ë¡œ ì‘ì„±):"""
        
        # Qwen ëª¨ë¸ë¡œ íƒœìŠ¤í¬ ìƒì„±
        messages = [{"role": "user", "content": json_enforced_prompt}]
        
        if qwen_model and qwen_tokenizer:
            text = qwen_tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # VLLM ì‚¬ìš© (qwen_modelì´ ì‹¤ì œë¡œëŠ” VLLM LLM ê°ì²´)
            from vllm import SamplingParams
            sampling_params = SamplingParams(
                temperature=0.3,
                max_tokens=2048,
                top_p=0.95
            )
            
            outputs = qwen_model.generate([text], sampling_params)
            response = outputs[0].outputs[0].text
            
            # ë””ë²„ê¹…: ì‘ë‹µ ë‚´ìš© í™•ì¸
            logger.info(f"ğŸ“ Raw response length: {len(response)} chars")
            logger.info(f"ğŸ“ Response preview: {response[:500]}...")
            
            # <think> íƒœê·¸ ì œê±°
            if "<think>" in response:
                think_end = response.find("</think>")
                if think_end != -1:
                    response = response[think_end + 8:].strip()
                else:
                    # </think> íƒœê·¸ê°€ ì—†ìœ¼ë©´ <think> ì´í›„ ì „ì²´ ì œê±°
                    think_start = response.find("<think>")
                    response = response[:think_start].strip()
            
            # JSON íŒŒì‹±
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                if json_end == -1:
                    json_content = response[json_start:].strip()
                else:
                    json_content = response[json_start:json_end].strip()
            else:
                # JSON ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ ì‘ë‹µì„ JSONìœ¼ë¡œ ê°„ì£¼
                json_content = response.strip()
                # í•˜ì§€ë§Œ ë¨¼ì € JSON ì‹œì‘ ë¶€ë¶„ ì°¾ê¸°
                if response.strip().startswith('{'):
                    json_content = response.strip()
                else:
                    # JSONì´ ì¤‘ê°„ì— ì‹œì‘í•˜ëŠ” ê²½ìš°
                    json_start_idx = response.find('{')
                    if json_start_idx != -1:
                        json_content = response[json_start_idx:].strip()
                    else:
                        logger.error(f"âŒ No JSON found in response")
                        return []
            
            # JSON ì •ë¦¬ ë° ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ìˆ˜ì •
            import re
            
            # 1. ë’¤ë”°ë¥´ëŠ” ì‰¼í‘œ ì œê±° (ë°°ì—´ì´ë‚˜ ê°ì²´ ë)
            json_content = re.sub(r',\s*}', '}', json_content)
            json_content = re.sub(r',\s*]', ']', json_content)
            
            # 2. ì‘ì€ë”°ì˜´í‘œë¥¼ í°ë”°ì˜´í‘œë¡œ ë³€ê²½ (JSONì€ í°ë”°ì˜´í‘œë§Œ í—ˆìš©)
            # ë‹¨, ê°’ ë‚´ë¶€ì˜ ì‘ì€ë”°ì˜´í‘œëŠ” ìœ ì§€
            json_content = re.sub(r"'([^']*)'(?=\s*:)", r'"\1"', json_content)
            
            # 3. í‚¤ì— ë”°ì˜´í‘œê°€ ì—†ëŠ” ê²½ìš° ì¶”ê°€ (ì˜ˆ: {key: "value"} -> {"key": "value"})
            json_content = re.sub(r'([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', json_content)
            
            # 4. undefined, null ì²˜ë¦¬
            json_content = json_content.replace('undefined', 'null')
            
            # 5. ë¶ˆì™„ì „í•œ JSON ë ì²˜ë¦¬
            open_braces = json_content.count('{') - json_content.count('}')
            open_brackets = json_content.count('[') - json_content.count(']')
            
            if open_braces > 0:
                json_content += '}' * open_braces
            if open_brackets > 0:
                json_content += ']' * open_brackets
            
            try:
                task_data = json.loads(json_content)
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSON parse error after cleaning: {e}")
                logger.error(f"ğŸ“ Cleaned JSON content: {json_content[:500]}...")
                raise
            
            # TaskItem ê°ì²´ë¡œ ë³€í™˜
            task_items = []
            for task_info in task_data.get("tasks", []):
                task_item = TaskItem(
                    id=task_info.get("id", len(task_items) + 1),
                    title=task_info.get("title", f"Task {len(task_items) + 1}"),
                    description=task_info.get("description", ""),
                    details=task_info.get("details", ""),
                    priority=task_info.get("priority", "medium"),
                    status=task_info.get("status", "pending"),
                    assignee=task_info.get("assignee", None),
                    start_date=task_info.get("start_date", None),
                    deadline=task_info.get("deadline", None),
                    due_date=task_info.get("due_date", None),
                    estimated_hours=task_info.get("estimated_hours", 8),
                    complexity=task_info.get("complexity", 5),
                    dependencies=task_info.get("dependencies", []),
                    test_strategy=task_info.get("testStrategy", ""),
                    subtasks=[]  # ì„œë¸ŒíƒœìŠ¤í¬ëŠ” ë‚˜ì¤‘ì— ë³µì¡ë„ ë¶„ì„ í›„ ìƒì„±
                )
                task_items.append(task_item)
            
            logger.info(f"âœ… Generated {len(task_items)} tasks from PRD")
            return task_items
        else:
            logger.error("âŒ Qwen model not available for task generation")
            return []
            
    except Exception as e:
        logger.error(f"âŒ Error generating tasks from PRD: {e}")
        
        # Fallback: ë”ë¯¸ íƒœìŠ¤í¬ ìƒì„±
        logger.info("âš ï¸ Using fallback dummy tasks")
        dummy_tasks = []
        for i in range(num_tasks):
            dummy_tasks.append(TaskItem(
                id=i + 1,
                title=f"Task {i + 1}: ì‹œìŠ¤í…œ êµ¬í˜„",
                description=f"í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ê¸°ëŠ¥ {i + 1} êµ¬í˜„",
                details="ìƒì„¸ êµ¬í˜„ ë‚´ìš©",
                priority="high" if i == 0 else "medium",
                status="pending",
                assignee=None,
                start_date=None,
                deadline=None,
                due_date=None,
                estimated_hours=(i + 1) * 8,  # íƒœìŠ¤í¬ë³„ë¡œ ë‹¤ë¥¸ ì‹œê°„
                complexity=7 + i if i < 3 else 5,  # ë³µì¡ë„ë„ ë‹¤ì–‘í•˜ê²Œ
                dependencies=[],
                test_strategy="ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë° í†µí•© í…ŒìŠ¤íŠ¸",
                subtasks=[]
            ))
        return dummy_tasks

async def analyze_task_complexity(task_items: List[TaskItem]) -> dict:
    """Task Master ìŠ¤íƒ€ì¼ ë³µì¡ë„ ë¶„ì„"""
    try:
        logger.info("ğŸ” Analyzing task complexity using Task Master approach...")
        
        from prd_generation_prompts import (
            generate_complexity_analysis_system_prompt,
            generate_complexity_analysis_prompt
        )
        
        # íƒœìŠ¤í¬ ë°ì´í„° ì¤€ë¹„
        tasks_data = {
            "tasks": [
                {
                    "id": task.id,
                    "title": task.title,
                    "description": task.description,
                    "details": task.details,
                    "priority": task.priority
                }
                for task in task_items
            ]
        }
        
        system_prompt = generate_complexity_analysis_system_prompt()
        user_prompt = generate_complexity_analysis_prompt(tasks_data)
        
        # Qwen ëª¨ë¸ë¡œ ë³µì¡ë„ ë¶„ì„
        messages = [{"role": "user", "content": user_prompt}]
        
        if qwen_model and qwen_tokenizer:
            text = qwen_tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # VLLM ì‚¬ìš© (qwen_modelì´ ì‹¤ì œë¡œëŠ” VLLM LLM ê°ì²´)
            from vllm import SamplingParams
            sampling_params = SamplingParams(
                temperature=0.3,
                max_tokens=2048,
                top_p=0.95
            )
            
            outputs = qwen_model.generate([text], sampling_params)
            response = outputs[0].outputs[0].text
            
            # JSON íŒŒì‹±
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                if json_end == -1:
                    json_content = response[json_start:].strip()
                else:
                    json_content = response[json_start:json_end].strip()
            else:
                # ë°°ì—´ ì°¾ê¸°
                first_bracket = response.find('[')
                last_bracket = response.rfind(']')
                if first_bracket != -1 and last_bracket > first_bracket:
                    json_content = response[first_bracket:last_bracket + 1]
                else:
                    json_content = response.strip()
            
            complexity_analysis = json.loads(json_content)
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (taskIdë¥¼ í‚¤ë¡œ)
            complexity_map = {}
            for analysis in complexity_analysis:
                task_id = analysis.get("taskId")
                if task_id:
                    complexity_map[task_id] = analysis
            
            logger.info(f"âœ… Complexity analysis completed for {len(complexity_map)} tasks")
            return complexity_map
        else:
            logger.error("âŒ Qwen model not available for complexity analysis")
            return {}
            
    except Exception as e:
        logger.error(f"âŒ Error analyzing task complexity: {e}")
        return {}

# Task Master ìŠ¤íƒ€ì¼ ë³µì¡ë„ ê¸°ë°˜ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±
async def generate_subtasks_for_all_tasks(task_items: List[TaskItem], complexity_analysis: dict = None) -> List[TaskItem]:
    """Task Master ìŠ¤íƒ€ì¼: ë³µì¡ë„ ë¶„ì„ ê¸°ë°˜ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±"""
    try:
        logger.info("ğŸ”§ Generating subtasks using Task Master complexity-based approach...")
        
        from prd_generation_prompts import (
            generate_complexity_based_subtask_prompt,
            generate_complexity_based_subtask_system_prompt
        )
        
        for task in task_items:
            try:
                # ë³µì¡ë„ ë¶„ì„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                task_analysis = complexity_analysis.get(task.id, {}) if complexity_analysis else {}
                
                # ê¸°ë³¸ê°’ ì„¤ì •
                complexity_score = task_analysis.get('complexityScore', 5)
                recommended_subtasks = task_analysis.get('recommendedSubtasks', 3)
                
                # íƒœìŠ¤í¬ì— ë³µì¡ë„ ì •ë³´ ì—…ë°ì´íŠ¸
                task.complexity = complexity_score
                task.estimated_hours = task_analysis.get('estimatedHours', complexity_score * 8)
                
                # ë‚ ì§œ ì„¤ì • (ì‹œì‘ì¼ê³¼ ë§ˆê°ì¼)
                from datetime import datetime, timedelta
                today = datetime.now()
                task.start_date = today.strftime('%Y-%m-%d')
                task.due_date = (today + timedelta(days=7 * complexity_score)).strftime('%Y-%m-%d')
                task.deadline = task.due_date
                
                # ì„œë¸ŒíƒœìŠ¤í¬ ID ì‹œì‘ì 
                next_subtask_id = 1
                
                # Task Master ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„±
                system_prompt = generate_complexity_based_subtask_system_prompt(
                    recommended_subtasks, next_subtask_id
                )
                user_prompt = generate_complexity_based_subtask_prompt(
                    {
                        'id': task.id,
                        'title': task.title,
                        'description': task.description,
                        'details': task.details
                    },
                    task_analysis,
                    next_subtask_id
                )
                
                # Qwen ëª¨ë¸ë¡œ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±
                # JSON ì‘ë‹µ ê°•ì œ í”„ë¡¬í”„íŠ¸
                json_prompt = f"""
{system_prompt}

**CRITICAL: RESPOND ONLY WITH JSON. NO THINKING, NO EXPLANATIONS.**
- DO NOT use <think> tags
- START directly with JSON
- Output valid JSON ONLY

{user_prompt}

RESPOND WITH JSON ONLY:"""
                
                messages = [{"role": "user", "content": json_prompt}]
                
                if qwen_model and qwen_tokenizer:
                    text = qwen_tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    
                    # VLLM ì‚¬ìš©
                    from vllm import SamplingParams
                    sampling_params = SamplingParams(
                        temperature=0.3,
                        max_tokens=2048,  # ì¦ê°€: ì„œë¸ŒíƒœìŠ¤í¬ëŠ” ë” ê¸´ ì‘ë‹µ í•„ìš”
                        top_p=0.95
                    )
                    
                    outputs = qwen_model.generate([text], sampling_params)
                    response = outputs[0].outputs[0].text
                    
                    # <think> íƒœê·¸ ì œê±°
                    if "<think>" in response:
                        think_end = response.find("</think>")
                        if think_end != -1:
                            response = response[think_end + 8:].strip()
                        else:
                            json_start_idx = response.find('{')
                            if json_start_idx != -1:
                                response = response[json_start_idx:].strip()
                    
                    # JSON íŒŒì‹±
                    if "```json" in response:
                        json_start = response.find("```json") + 7
                        json_end = response.find("```", json_start)
                        if json_end == -1:
                            json_content = response[json_start:].strip()
                        else:
                            json_content = response[json_start:json_end].strip()
                    else:
                        json_content = response.strip()
                    
                    subtask_data = json.loads(json_content)
                    
                    # SubTask ê°ì²´ë¡œ ë³€í™˜ (Task Master í•„ë“œ í¬í•¨)
                    subtasks = []
                    for i, subtask_info in enumerate(subtask_data.get("subtasks", [])):
                        subtask = SubTask(
                            id=subtask_info.get("id", i + 1),
                            title=subtask_info.get("title", f"ì„œë¸ŒíƒœìŠ¤í¬ {i+1}"),
                            description=subtask_info.get("description", ""),
                            priority=subtask_info.get("priority", "medium"),
                            estimated_hours=subtask_info.get("estimated_hours", 4),
                            status=subtask_info.get("status", "pending")
                        )
                        subtasks.append(subtask)
                    
                    task.subtasks = subtasks
                    logger.info(f"âœ… '{task.title}' ë³µì¡ë„ ê¸°ë°˜ ì„œë¸ŒíƒœìŠ¤í¬ {len(subtasks)}ê°œ ìƒì„± (ë³µì¡ë„: {complexity_score}/10)")
                    
                else:
                    logger.warning("âš ï¸ Qwen ëª¨ë¸ ì—†ìŒ, ë³µì¡ë„ ê¸°ë°˜ ê¸°ë³¸ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±")
                    task.subtasks = create_complexity_based_default_subtasks(task, task_analysis)
                    
            except Exception as e:
                logger.warning(f"âš ï¸ '{task.title}' ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
                if 'response' in locals():
                    logger.error(f"   Response length: {len(response)}")
                    logger.error(f"   Response preview: {response[:200]}...")
                # ë³µì¡ë„ ê¸°ë°˜ ê¸°ë³¸ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±
                task_analysis = complexity_analysis.get(task.id, {}) if complexity_analysis else {}
                task.subtasks = create_complexity_based_default_subtasks(task, task_analysis)
        
        total_subtasks = sum(len(task.subtasks) for task in task_items)
        logger.info(f"âœ… Task Master ìŠ¤íƒ€ì¼ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„± ì™„ë£Œ: ì´ {total_subtasks}ê°œ")
        return task_items
        
    except Exception as e:
        logger.error(f"âŒ Task Master ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return task_items

def create_complexity_based_default_subtasks(task: TaskItem, task_analysis: dict) -> List[SubTask]:
    """Task Master ìŠ¤íƒ€ì¼: ë³µì¡ë„ ê¸°ë°˜ ê¸°ë³¸ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±"""
    default_subtasks = []
    
    # ë³µì¡ë„ ë¶„ì„ ì •ë³´
    complexity_score = task_analysis.get('complexityScore', 5)
    recommended_subtasks = task_analysis.get('recommendedSubtasks', 3)
    
    # ë³µì¡ë„ì— ë”°ë¥¸ Task Master ìŠ¤íƒ€ì¼ í…œí”Œë¦¿
    if complexity_score >= 8:  # ê³ ë³µì¡ë„ (8-10)
        templates = [
            {
                "title": "Architecture Design & Planning",
                "desc": f"Design system architecture and create detailed technical specifications for {task.title}",
                "details": "Create technical design documents, API specifications, database schemas, and implementation roadmap",
                "hours": 12,
                "priority": "high"
            },
            {
                "title": "Core Implementation",
                "desc": f"Implement the main functionality and core features of {task.title}",
                "details": "Develop core business logic, implement primary user flows, and establish data processing pipeline",
                "hours": 20,
                "priority": "high"
            },
            {
                "title": "Integration & Testing",
                "desc": f"Integrate components and conduct comprehensive testing for {task.title}",
                "details": "Perform unit testing, integration testing, and end-to-end testing with comprehensive test coverage",
                "hours": 10,
                "priority": "medium"
            },
            {
                "title": "Performance Optimization",
                "desc": f"Optimize performance and conduct security review for {task.title}",
                "details": "Profile application performance, implement caching strategies, and conduct security audit",
                "hours": 8,
                "priority": "medium"
            }
        ]
    elif complexity_score >= 5:  # ì¤‘ë³µì¡ë„ (5-7)
        templates = [
            {
                "title": "Requirements Analysis & Design",
                "desc": f"Analyze requirements and create implementation plan for {task.title}",
                "details": "Define functional requirements, create wireframes, and establish development approach",
                "hours": 6,
                "priority": "high"
            },
            {
                "title": "Implementation & Development",
                "desc": f"Develop and implement the main features of {task.title}",
                "details": "Code implementation following best practices, implement business logic, and create user interfaces",
                "hours": 12,
                "priority": "high"
            },
            {
                "title": "Testing & Documentation",
                "desc": f"Test functionality and create documentation for {task.title}",
                "details": "Write unit tests, conduct manual testing, and create user and technical documentation",
                "hours": 6,
                "priority": "medium"
            }
        ]
    else:  # ì €ë³µì¡ë„ (1-4)
        templates = [
            {
                "title": "Setup & Preparation",
                "desc": f"Set up environment and prepare resources for {task.title}",
                "details": "Configure development environment, gather required resources, and create initial project structure",
                "hours": 3,
                "priority": "medium"
            },
            {
                "title": "Implementation",
                "desc": f"Implement the required functionality for {task.title}",
                "details": "Code implementation with focus on clean, maintainable code following established patterns",
                "hours": 6,
                "priority": "high"
            },
            {
                "title": "Validation & Cleanup",
                "desc": f"Validate implementation and finalize {task.title}",
                "details": "Test implementation, review code quality, and ensure all requirements are met",
                "hours": 3,
                "priority": "medium"
            }
        ]
    
    # ì¶”ì²œëœ ì„œë¸ŒíƒœìŠ¤í¬ ìˆ˜ë§Œí¼ ìƒì„±
    for i in range(min(recommended_subtasks, len(templates))):
        template = templates[i]
        subtask = SubTask(
            id=i + 1,
            title=template["title"],
            description=template["desc"],
            priority=template["priority"],
            estimated_hours=template["hours"],
            status="pending"
        )
        default_subtasks.append(subtask)
    
    return default_subtasks

# ê¸°ì¡´ í•¨ìˆ˜ë„ ìœ ì§€ (í˜¸í™˜ì„±ì„ ìœ„í•´)
def create_default_subtasks(task: TaskItem, num_subtasks: int = 3) -> List[SubTask]:
    """ê¸°ë³¸ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„±)"""
    # Task Master ë°©ì‹ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    task_analysis = {
        'complexityScore': getattr(task, 'complexity', 5),
        'recommendedSubtasks': num_subtasks
    }
    return create_complexity_based_default_subtasks(task, task_analysis)

# ìƒˆë¡œìš´ ì‘ë‹µ ëª¨ë¸ë“¤
class NotionProjectResponse(BaseModel):
    success: bool
    notion_project: Optional[Dict[str, Any]] = None
    formatted_notion: Optional[str] = None
    error: Optional[str] = None

class TaskMasterPRDResponse(BaseModel):
    success: bool
    prd_data: Optional[Dict[str, Any]] = None
    formatted_prd: Optional[str] = None
    error: Optional[str] = None

class TwoStageAnalysisRequest(BaseModel):
    transcript: str
    generate_notion: bool = True
    generate_tasks: bool = True
    generate_subtasks: bool = True  # Stage 4 ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„± ì—¬ë¶€
    num_tasks: int = 5
    additional_context: Optional[str] = None
    auto_expand_tasks: bool = True  # ğŸš€ ìë™ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„± ê¸°ë³¸ê°’ì„ Trueë¡œ ì„¤ì •

class TwoStageAnalysisResponse(BaseModel):
    success: bool
    stage1_notion: Optional[Dict[str, Any]] = None
    stage2_prd: Optional[Dict[str, Any]] = None
    stage3_tasks: Optional[MeetingAnalysisResult] = None
    stage4_subtasks: Optional[List[Dict[str, Any]]] = None  # Stage 4 ì„œë¸ŒíƒœìŠ¤í¬ ê²°ê³¼
    formatted_notion: Optional[str] = None
    formatted_prd: Optional[str] = None
    processing_time: Optional[float] = None
    error: Optional[str] = None

# Triplet ê´€ë ¨ ìƒˆë¡œìš´ ì‘ë‹µ ëª¨ë¸ë“¤
class EnhancedTranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    triplet_data: Optional[Dict[str, Any]] = None
    filtered_transcript: Optional[str] = None
    processing_stats: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class EnhancedTwoStageResult(BaseModel):
    success: bool
    triplet_stats: Optional[Dict[str, Any]] = None
    classification_stats: Optional[Dict[str, Any]] = None
    stage1_notion: Optional[Dict[str, Any]] = None
    stage2_prd: Optional[Dict[str, Any]] = None
    stage3_tasks: Optional[MeetingAnalysisResult] = None
    stage4_subtasks: Optional[List[Dict[str, Any]]] = None  # Stage 4 ì„œë¸ŒíƒœìŠ¤í¬ ê²°ê³¼
    formatted_notion: Optional[str] = None
    formatted_prd: Optional[str] = None
    original_transcript_length: Optional[int] = None
    filtered_transcript_length: Optional[int] = None
    noise_reduction_ratio: Optional[float] = None
    processing_time: Optional[float] = None
    error: Optional[str] = None

# ê¸°ì¡´ ëª¨ë¸ë“¤
class TranscriptionResponse(BaseModel):
    success: bool
    transcription: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class AnalysisRequest(BaseModel):
    transcript: str
    num_tasks: int = 5
    additional_context: Optional[str] = None

class AnalysisResponse(BaseModel):
    success: bool
    analysis: Optional[MeetingAnalysisResult] = None
    error: Optional[str] = None

class HealthResponse(BaseModel):
    status: str
    gpu_available: bool
    gpu_count: int
    models_loaded: Dict[str, bool]
    memory_info: Optional[Dict[str, float]] = None

def load_whisperx():
    """WhisperX ëª¨ë¸ ë¡œë”© - ì›ê²© ì„œë²„ ì‚¬ìš©"""
    # WhisperXëŠ” ì´ì œ ì›ê²© ì„œë²„(í¬íŠ¸ 8001)ì—ì„œ ì²˜ë¦¬
    # ì´ í•¨ìˆ˜ëŠ” í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ë˜ ì‹¤ì œ ë¡œë”©ì€ í•˜ì§€ ì•ŠìŒ
    logger.info("ğŸ¤ WhisperX will be handled by remote server (port 8001)")
    return None

def load_qwen3():
    """Qwen3-4B LoRA ëª¨ë¸ ë¡œë”© (VLLM ìµœì í™”)"""
    global qwen_model, qwen_tokenizer
    
    if qwen_model is None or qwen_tokenizer is None:
        logger.info("ğŸš€ Loading Qwen3-4B LoRA with VLLM...")
        try:
            # VLLM ì‚¬ìš© ì—¬ë¶€ ì²´í¬
            use_vllm = os.getenv("USE_VLLM", "true").lower() == "true"
            
            if use_vllm:
                try:
                    logger.info("âš¡ Using VLLM for ultra-fast inference")
                    from vllm import LLM
                    from transformers import AutoTokenizer
                except ImportError as e:
                    logger.warning(f"âš ï¸ VLLM import failed: {e}")
                    logger.info("ğŸ”„ Falling back to Transformers...")
                    use_vllm = False
                
            if use_vllm:
                # LoRA ì–´ëŒ‘í„° ê²½ë¡œ
                if os.path.exists("/workspace"):
                    lora_path = "/workspace/SKN12-FINAL-3TEAM/ai-engine-dev/qwen3_lora_ttalkkac_4b"
                else:
                    lora_path = "C:/Users/SH/Desktop/TtalKkac/ai-engine-dev/qwen3_lora_ttalkkac_4b"
                
                # ë³‘í•©ëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
                merged_model_path = "/workspace/SKN12-FINAL-3TEAM/ai-engine-dev/qwen3-4b-merged"
                
                # ë³‘í•©ëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì¦‰ì‹œ ë³‘í•© ìˆ˜í–‰
                if not os.path.exists(merged_model_path):
                    logger.info("ğŸ”„ LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•© ì¤‘...")
                    try:
                        from transformers import AutoModelForCausalLM, AutoTokenizer
                        from peft import PeftModel
                        
                        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
                        base_model = AutoModelForCausalLM.from_pretrained(
                            "Qwen/Qwen3-4B",
                            torch_dtype=torch.float16,
                            trust_remote_code=True,
                            device_map="auto"
                        )
                        
                        # LoRA ì–´ëŒ‘í„° ì ìš©
                        if os.path.exists(lora_path) and os.path.exists(f"{lora_path}/adapter_config.json"):
                            logger.info(f"ğŸ“ LoRA ì–´ëŒ‘í„° ì ìš©: {lora_path}")
                            model_with_lora = PeftModel.from_pretrained(base_model, lora_path)
                            
                            # ë³‘í•© ë° ì €ì¥
                            merged_model = model_with_lora.merge_and_unload()
                            merged_model.save_pretrained(merged_model_path)
                            
                            # í† í¬ë‚˜ì´ì €ë„ ì €ì¥
                            tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-4B", trust_remote_code=True)
                            tokenizer.save_pretrained(merged_model_path)
                            
                            logger.info(f"âœ… ë³‘í•© ì™„ë£Œ: {merged_model_path}")
                            del base_model, model_with_lora, merged_model  # ë©”ëª¨ë¦¬ ì •ë¦¬
                        else:
                            logger.warning("âš ï¸ LoRA ì–´ëŒ‘í„° íŒŒì¼ ì—†ìŒ, ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©")
                            merged_model_path = "Qwen/Qwen3-4B"
                    except Exception as e:
                        logger.error(f"âŒ ë³‘í•© ì‹¤íŒ¨: {e}")
                        merged_model_path = "Qwen/Qwen3-4B"
                
                try:
                    # VLLMìœ¼ë¡œ ë³‘í•©ëœ ëª¨ë¸ ë¡œë”©
                    logger.info(f"ğŸš€ VLLMìœ¼ë¡œ ëª¨ë¸ ë¡œë”©: {merged_model_path}")
                    qwen_model = LLM(
                        model=merged_model_path,
                        tensor_parallel_size=1,
                        gpu_memory_utilization=0.7,  # GPU ë©”ëª¨ë¦¬ 70%
                        trust_remote_code=True,
                        max_model_len=16384,  # í† í° ê¸¸ì´
                        enforce_eager=True,  # CUDA ê·¸ë˜í”„ ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
                        swap_space=4,  # 4GB swap space
                        max_num_seqs=64  # ë™ì‹œ ì‹œí€€ìŠ¤ ìˆ˜
                    )
                    
                    # í† í¬ë‚˜ì´ì €ëŠ” ë³„ë„ ë¡œë”© (í…œí”Œë¦¿ ì ìš©ìš©)
                    qwen_tokenizer = AutoTokenizer.from_pretrained(
                        merged_model_path if os.path.exists(merged_model_path) else "Qwen/Qwen3-4B", 
                        trust_remote_code=True
                    )
                    
                    logger.info("ğŸ‰ VLLM Qwen3-4B LoRA loaded successfully")
                except Exception as e:
                    logger.error(f"âŒ VLLM model loading failed: {e}")
                    logger.info("ğŸ”„ Falling back to Transformers...")
                    use_vllm = False
                
            if not use_vllm:
                # ê¸°ì¡´ Transformers ë°©ì‹ (ë°±ì—…ìš©)
                try:
                    logger.info("ğŸ“š Using Transformers (fallback mode)")
                    from transformers import AutoTokenizer, AutoModelForCausalLM
                except ImportError as e:
                    logger.error(f"âŒ Transformers import failed: {e}")
                    raise RuntimeError("Both VLLM and Transformers unavailable!")
                
                # RunPodì—ì„œ ì‚¬ìš©í•  ê²½ë¡œ ì„¤ì •
                if os.path.exists("/workspace"):
                    model_name = "/workspace/SKN12-FINAL-3TEAM/ai-engine-dev/qwen3_lora_ttalkkac_4b"
                else:
                    model_name = "C:/Users/SH/Desktop/TtalKkac/ai-engine-dev/qwen3_lora_ttalkkac_4b"
                
                # Qwen3-4B í† í¬ë‚˜ì´ì € ë¡œë“œ
                try:
                    # ë¨¼ì € ë¡œì»¬ ëª¨ë¸ì—ì„œ í† í¬ë‚˜ì´ì € ì‹œë„
                    qwen_tokenizer = AutoTokenizer.from_pretrained(
                        model_name, 
                        trust_remote_code=True,
                        use_fast=True
                    )
                    logger.info("âœ… ë¡œì»¬ Qwen3-4B í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
                except Exception as e:
                    logger.warning(f"ë¡œì»¬ í† í¬ë‚˜ì´ì € ì‹¤íŒ¨: {e}")
                    # Qwen3-4B í† í¬ë‚˜ì´ì € ì‚¬ìš©
                    try:
                        qwen_tokenizer = AutoTokenizer.from_pretrained(
                            "Qwen/Qwen3-4B", 
                            trust_remote_code=True,
                            use_fast=True
                        )
                        logger.info("âœ… Qwen3-4B í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
                    except Exception as e2:
                        logger.error(f"Qwen3-4B í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e2}")
                        raise
                
                # Qwen3-4B ëª¨ë¸ ë¡œë“œ
                model_loaded = False
                
                # ë¨¼ì € ë¡œì»¬ ëª¨ë¸ ì‹œë„
                if os.path.exists(model_name):
                    logger.info("ğŸ”„ ë¡œì»¬ ëª¨ë¸ ì§ì ‘ ë¡œë“œ ì‹œë„...")
                    config_path = os.path.join(model_name, "config.json")
                    if os.path.exists(config_path):
                        try:
                            # ë¡œì»¬ Qwen3-4B ëª¨ë¸ ë¡œë“œ
                            qwen_model = AutoModelForCausalLM.from_pretrained(
                                model_name,
                                torch_dtype=torch.float16,
                                trust_remote_code=True,
                                local_files_only=True
                            )
                            if torch.cuda.is_available():
                                qwen_model = qwen_model.cuda()
                            
                            model_loaded = True
                            logger.info("âœ… ë¡œì»¬ Qwen3 ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                            
                        except Exception as e:
                            logger.warning(f"ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                
                # ë¡œì»¬ ëª¨ë¸ ì‹¤íŒ¨ì‹œ HuggingFaceì—ì„œ Qwen3-4B ë‹¤ìš´ë¡œë“œ
                if not model_loaded:
                    try:
                        logger.info("ğŸ¯ Qwen3-4B ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
                        base_model_name = "Qwen/Qwen3-4B"
                        
                        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
                        base_model = AutoModelForCausalLM.from_pretrained(
                            base_model_name,
                            torch_dtype=torch.float16,
                            trust_remote_code=True
                        )
                        if torch.cuda.is_available():
                            base_model = base_model.cuda()
                        logger.info("âœ… Qwen3-4B ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                        
                        # LoRA ì–´ëŒ‘í„° ì ìš© (ìˆëŠ” ê²½ìš°)
                        if os.path.exists(model_name) and os.path.exists(os.path.join(model_name, "adapter_config.json")):
                            try:
                                from peft import PeftModel
                                qwen_model = PeftModel.from_pretrained(base_model, model_name)
                                logger.info("âœ… Qwen3 LoRA ì–´ëŒ‘í„° ì ìš© ì„±ê³µ")
                            except Exception as e:
                                logger.warning(f"LoRA ì ìš© ì‹¤íŒ¨: {e}, ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©")
                                qwen_model = base_model
                        else:
                            qwen_model = base_model
                            logger.info("ğŸ”” LoRA ì–´ëŒ‘í„° ì—†ìŒ, Qwen3-4B ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©")
                        
                        model_loaded = True
                    
                    except Exception as e:
                        logger.error(f"Qwen3-4B ë¡œë“œ ì‹¤íŒ¨: {e}")
                    
                    # Fallback: ë¡œì»¬ ëª¨ë¸ ì‹œë„ (config.json ìˆ˜ì • í•„ìš”)
                    if os.path.exists(model_name):
                        logger.info("ğŸ”„ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹œë„ (config ìˆ˜ì •)...")
                        config_path = os.path.join(model_name, "config.json")
                        if os.path.exists(config_path):
                            try:
                                # Qwen3-4B LoRA ì–´ëŒ‘í„° ë¡œë“œ ì‹œë„
                                qwen_model = AutoModelForCausalLM.from_pretrained(
                                    model_name,
                                    torch_dtype=torch.float16,
                                    trust_remote_code=True,
                                    ignore_mismatched_sizes=True
                                )
                                if torch.cuda.is_available():
                                    qwen_model = qwen_model.cuda()
                                model_loaded = True
                                logger.info("âœ… ë¡œì»¬ Qwen3 ëª¨ë¸ ë¡œë“œ ì„±ê³µ (config ìˆ˜ì •)")
                                
                                # ì›ë˜ëŒ€ë¡œ ë³µêµ¬
                                config['model_type'] = original_type
                                with open(config_path, 'w') as f:
                                    json.dump(config, f, indent=2)
                                    
                            except Exception as e2:
                                logger.error(f"ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e2}")
                
                logger.info("âœ… Transformers Qwen3-4B LoRA loaded successfully")
            
        except Exception as e:
            logger.error(f"âŒ Qwen3-4B LoRA loading failed: {e}")
            # VLLM ì‹¤íŒ¨ ì‹œ Transformersë¡œ ëŒ€ì²´
            if 'vllm' in str(e).lower():
                logger.warning("ğŸ”„ VLLM failed, falling back to Transformers...")
                os.environ["USE_VLLM"] = "false"
                return load_qwen3()  # ì¬ê·€ í˜¸ì¶œë¡œ Transformers ë¡œë”©
            raise e
    
    return qwen_model, qwen_tokenizer

def generate_structured_response(
    system_prompt: str, 
    user_prompt: str, 
    response_schema: Dict[str, Any],
    temperature: float = 0.3,
    max_input_tokens: int = 28000,  # Qwen3 ì•ˆì „ ë§ˆì§„ ì ìš©
    enable_chunking: bool = True
) -> Dict[str, Any]:
    """êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„± (ì²­í‚¹ ì§€ì›)"""
    
    # ì²­í‚¹ í•„ìš” ì—¬ë¶€ í™•ì¸       
    if enable_chunking:
        try:
            from chunking_processor import get_chunking_processor
            chunking_processor = get_chunking_processor(max_context_tokens=32768)
            
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ ì¶”ì •
            total_prompt = f"{system_prompt}\n{user_prompt}"
            estimated_tokens = chunking_processor.estimate_tokens(total_prompt)
            
            if estimated_tokens > max_input_tokens:
                logger.info(f"ğŸ”„ ì²­í‚¹ í•„ìš” ê°ì§€ (í† í°: {estimated_tokens} > {max_input_tokens})")
                return generate_chunked_response(
                    system_prompt, user_prompt, response_schema, 
                    temperature, chunking_processor
                )
            else:
                logger.info(f"ğŸ“ ë‹¨ì¼ ì²˜ë¦¬ (í† í°: {estimated_tokens})")
        except ImportError:
            logger.warning("âš ï¸ ì²­í‚¹ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # ëª¨ë¸ ë¡œë”©
    qwen_model, qwen_tokenizer = load_qwen3()
    
    # ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ í¬í•¨ í”„ë¡¬í”„íŠ¸
    schema_prompt = f"""
{system_prompt}

**CRITICAL: RESPOND ONLY WITH JSON. NO THINKING, NO EXPLANATIONS, JUST JSON.**

**Response Schema:**
You must respond with a JSON object following this exact structure:
```json
{json.dumps(response_schema, indent=2, ensure_ascii=False)}
```

**Important Rules:**
1. DO NOT use <think> tags or any other markup
2. START your response directly with ```json
3. Always return valid JSON format ONLY
4. Use Korean for all text content unless technical terms require English
5. Follow the exact schema structure
6. Include all required fields
7. NO explanations before or after the JSON

{user_prompt}

**Response (JSON ONLY):**
```json
"""
    
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [{"role": "user", "content": schema_prompt}]
    
    # ì¶”ë¡  ì‹¤í–‰ (VLLM vs Transformers)
    use_vllm = os.getenv("USE_VLLM", "true").lower() == "true"
    
    if use_vllm and hasattr(qwen_model, 'generate'):
        # VLLM ì¶”ë¡  (ì´ˆê³ ì†!)
        logger.info("âš¡ VLLM ì¶”ë¡  ì‹¤í–‰...")
        start_time = time.time()
        
        from vllm import SamplingParams
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
        sampling_params = SamplingParams(
            max_tokens=4096,  # ì¦ê°€
            temperature=temperature,
            top_p=0.9,
            repetition_penalty=1.1,
            stop=None
        )
        
        # ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        text = qwen_tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # VLLM ì¶”ë¡  ì‹¤í–‰
        outputs = qwen_model.generate([text], sampling_params)
        response = outputs[0].outputs[0].text
        
        inference_time = time.time() - start_time
        logger.info(f"ğŸ‰ VLLM ì¶”ë¡  ì™„ë£Œ: {inference_time:.3f}ì´ˆ")
        
    else:
        # Transformers ì¶”ë¡  (ê¸°ì¡´ ë°©ì‹)
        logger.info("ğŸ“š Transformers ì¶”ë¡  ì‹¤í–‰...")
        start_time = time.time()
        
        # í† í°í™”
        text = qwen_tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # VLLM ì‚¬ìš©
        from vllm import SamplingParams
        sampling_params = SamplingParams(
            temperature=temperature,
            max_tokens=4096,  # ì¦ê°€
            top_p=0.9,
            repetition_penalty=1.1
        )
        
        outputs = qwen_model.generate([text], sampling_params)
        response = outputs[0].outputs[0].text
        
        inference_time = time.time() - start_time
        logger.info(f"âœ… Transformers ì¶”ë¡  ì™„ë£Œ: {inference_time:.3f}ì´ˆ")
    
    # <think> íƒœê·¸ ì œê±°
    if "<think>" in response:
        think_end = response.find("</think>")
        if think_end != -1:
            response = response[think_end + 8:].strip()
        else:
            # </think> íƒœê·¸ê°€ ì—†ìœ¼ë©´ JSON ì‹œì‘ ì°¾ê¸°
            json_start_idx = response.find('{')
            if json_start_idx != -1:
                response = response[json_start_idx:].strip()
    
    # JSON ì¶”ì¶œ ë° íŒŒì‹±
    try:
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "```json" in response:
            json_start = response.find("```json") + 7
            json_end = response.find("```", json_start)
            if json_end == -1:
                json_content = response[json_start:].strip()
            else:
                json_content = response[json_start:json_end].strip()
        else:
            # JSON ë§ˆì»¤ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì‘ë‹µì—ì„œ JSON ì°¾ê¸°
            json_content = response.strip()
        
        # JSON íŒŒì‹± ì „ ì •ë¦¬
        # ëì— ìˆëŠ” ``` ì œê±°
        if json_content.endswith('```'):
            json_content = json_content[:-3].strip()
        
        # JSON íŒŒì‹±
        parsed_result = json.loads(json_content)
        return parsed_result
        
    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        logger.error(f"Raw response length: {len(response)}")
        logger.error(f"Raw response: {response}")
        
        # Fallback: ë¶€ë¶„ì ìœ¼ë¡œ íŒŒì‹± ê°€ëŠ¥í•œ ë¶€ë¶„ ì°¾ê¸°
        try:
            # JSON ë ë¶€ë¶„ ì •ë¦¬
            # ë§ˆì§€ë§‰ }ë¥¼ ì°¾ì•„ì„œ ê·¸ ì´í›„ ì œê±°
            last_brace = json_content.rfind('}')
            if last_brace != -1:
                json_content_cleaned = json_content[:last_brace + 1]
                logger.info(f"âš ï¸ Attempting to fix JSON by trimming after last }}")
                parsed_result = json.loads(json_content_cleaned)
                return parsed_result
                
            # ë¶ˆì™„ì „í•œ JSON ë³µêµ¬ ì‹œë„
            if json_content.count('{') > json_content.count('}'):
                # ë‹«ëŠ” ì¤‘ê´„í˜¸ ì¶”ê°€
                json_content += '}' * (json_content.count('{') - json_content.count('}'))
                logger.info("âš ï¸ Attempting to fix incomplete JSON by adding closing braces")
                parsed_result = json.loads(json_content)
                return parsed_result
        except:
            pass
            
        return {
            "error": f"JSON parsing failed: {str(e)}",
            "raw_response": response[:1000]
        }
    except Exception as e:
        logger.error(f"âŒ ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "error": f"Response processing failed: {str(e)}",
            "raw_response": response[:1000] if 'response' in locals() else "No response"
        }

def generate_chunked_response(
    system_prompt: str,
    user_prompt: str, 
    response_schema: Dict[str, Any],
    temperature: float,
    chunking_processor
) -> Dict[str, Any]:
    """ì²­í‚¹ëœ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬"""
    try:
        logger.info("ğŸš€ ì²­í‚¹ ê¸°ë°˜ ì²˜ë¦¬ ì‹œì‘...")
        start_time = time.time()
        
        # 1. user_promptë¥¼ ì²­í‚¹
        chunks = chunking_processor.create_chunks_with_overlap(user_prompt)
        logger.info(f"ğŸ“Š ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # 2. ê° ì²­í¬ë³„ë¡œ ì²˜ë¦¬
        chunk_results = []
        for i, chunk in enumerate(chunks):
            logger.info(f"ğŸ”„ ì²­í¬ {i+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘... (í† í°: {chunk['estimated_tokens']})")
            
            # ì²­í¬ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
            chunk_system_prompt = f"""{system_prompt}

**ì²­í‚¹ ì²˜ë¦¬ ì •ë³´:**
- í˜„ì¬ ì²­í¬: {i+1}/{len(chunks)}
- ì´ ì²­í¬ëŠ” ì „ì²´ íšŒì˜ì˜ ì¼ë¶€ì…ë‹ˆë‹¤
- ì´ ì²­í¬ì—ì„œ ë°œê²¬ë˜ëŠ” ë‚´ìš©ë§Œ ë¶„ì„í•˜ì„¸ìš”
- ë‹¤ë¥¸ ì²­í¬ì˜ ë‚´ìš©ì€ ë‚˜ì¤‘ì— í†µí•©ë©ë‹ˆë‹¤"""

            chunk_user_prompt = f"""ë‹¤ìŒì€ ì „ì²´ íšŒì˜ë¡ì˜ ì¼ë¶€ì…ë‹ˆë‹¤:

{chunk['text']}

ìœ„ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì´ ë¶€ë¶„ì—ì„œ ë°œê²¬ë˜ëŠ” ì•¡ì…˜ ì•„ì´í…œ, ê²°ì •ì‚¬í•­, í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”."""
            
            # ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬
            chunk_result = generate_structured_response(
                system_prompt=chunk_system_prompt,
                user_prompt=chunk_user_prompt,
                response_schema=response_schema,
                temperature=temperature,
                enable_chunking=False  # ì¬ê·€ ë°©ì§€
            )
            
            chunk_results.append(chunk_result)
            logger.info(f"âœ… ì²­í¬ {i+1} ì²˜ë¦¬ ì™„ë£Œ")
        
        # 3. ê²°ê³¼ í†µí•©
        logger.info("ğŸ”„ ì²­í¬ ê²°ê³¼ í†µí•© ì¤‘...")
        merged_result = chunking_processor.merge_chunk_results(chunk_results)
        
        processing_time = time.time() - start_time
        logger.info(f"âœ… ì²­í‚¹ ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        merged_result["metadata"] = merged_result.get("metadata", {})
        merged_result["metadata"].update({
            "chunking_applied": True,
            "total_chunks": len(chunks),
            "processing_time": processing_time,
            "original_tokens": chunking_processor.estimate_tokens(user_prompt),
            "chunks_info": [
                {
                    "chunk_id": chunk["chunk_id"],
                    "tokens": chunk["estimated_tokens"],
                    "has_overlap": chunk["has_overlap"]
                }
                for chunk in chunks
            ]
        })
        
        return merged_result
        
    except Exception as e:
        logger.error(f"âŒ ì²­í‚¹ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "error": f"Chunking processing failed: {str(e)}",
            "fallback_message": "ì²­í‚¹ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì²˜ë¦¬ë¥¼ ì‹œë„í•˜ì„¸ìš”."
        }

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ëª¨ë¸ ë¡œë”©/ì •ë¦¬"""
    logger.info("ğŸš€ Starting TtalKkak Final AI Server with Triplets...")
    
    # ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë¡œë”© (ê¸°ë³¸ í™œì„±í™”ë¡œ ë³€ê²½)
    preload_enabled = os.getenv("PRELOAD_MODELS", "true").lower() == "true"
    logger.info(f"ğŸ”§ Model preloading: {'Enabled' if preload_enabled else 'Disabled'}")
    
    if preload_enabled:
        try:
            logger.info("ğŸ“¦ Starting parallel model preloading...")
            import asyncio
            
            # ë³‘ë ¬ ë¡œë”©ì„ ìœ„í•œ ë¹„ë™ê¸° ë˜í¼ í•¨ìˆ˜ë“¤ (ì‹œê°„ ì¸¡ì • í¬í•¨)
            async def load_whisperx_async():
                # WhisperX ì›ê²© ì„œë²„ ì²´í¬
                start_time = time.time()
                logger.info("ğŸ¤ Checking WhisperX remote server...")
                try:
                    async with httpx.AsyncClient(timeout=httpx.Timeout(10.0)) as client:
                        response = await client.get(f"{WHISPERX_SERVER}/health")
                        if response.status_code == 200:
                            logger.info("âœ… WhisperX server connected")
                        else:
                            logger.warning("âš ï¸ WhisperX server not responding")
                except:
                    logger.warning("âš ï¸ WhisperX server not available")
                elapsed = time.time() - start_time
                return elapsed
            
            async def load_qwen3_async():
                start_time = time.time()
                logger.info("ğŸ§  Loading Qwen3-4B LoRA...")
                load_qwen3()
                elapsed = time.time() - start_time
                logger.info(f"âœ… Qwen3-4B LoRA loaded in {elapsed:.2f} seconds")
                return elapsed
            
            async def load_bert_async():
                if TRIPLET_AVAILABLE:
                    start_time = time.time()
                    logger.info("ğŸ” Loading BERT classifier...")
                    get_bert_classifier()
                    elapsed = time.time() - start_time
                    logger.info(f"âœ… BERT classifier loaded in {elapsed:.2f} seconds")
                    return elapsed
                return 0
            
            # ë³‘ë ¬ ë¡œë”© ì‹¤í–‰ (ì´ ì‹œê°„ ì¸¡ì •)
            total_start_time = time.time()
            
            results = await asyncio.gather(
                load_whisperx_async(),
                load_qwen3_async(), 
                load_bert_async(),
                return_exceptions=True
            )
            
            total_elapsed = time.time() - total_start_time
            
            # ê²°ê³¼ ì •ë¦¬
            whisperx_time, qwen3_time, bert_time = results
            if isinstance(whisperx_time, Exception):
                whisperx_time = 0
                logger.error(f"âŒ WhisperX loading failed: {whisperx_time}")
            if isinstance(qwen3_time, Exception):
                qwen3_time = 0
                logger.error(f"âŒ Qwen3 loading failed: {qwen3_time}")
            if isinstance(bert_time, Exception):
                bert_time = 0
                logger.error(f"âŒ BERT loading failed: {bert_time}")
            
            logger.info("ğŸ‰ All models preloaded successfully!")
            logger.info("â±ï¸  Loading Time Summary:")
            logger.info(f"   - WhisperX: {whisperx_time:.2f}s")
            logger.info(f"   - Qwen3-4B: {qwen3_time:.2f}s") 
            logger.info(f"   - BERT: {bert_time:.2f}s")
            logger.info(f"   - Total (parallel): {total_elapsed:.2f}s")
            logger.info(f"   - Sequential would take: {whisperx_time + qwen3_time + bert_time:.2f}s")
            logger.info(f"   - Time saved: {(whisperx_time + qwen3_time + bert_time) - total_elapsed:.2f}s")
            
        except Exception as e:
            logger.error(f"âŒ Model preloading failed: {e}")
            logger.info("âš ï¸ Server will continue with lazy loading")
    else:
        logger.info("ğŸ“ Using lazy loading (models load on first request)")
    
    yield
    
    logger.info("ğŸ›‘ Shutting down TtalKkak Final AI Server...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="TtalKkak Final AI Server with Triplets",
    description="WhisperX (Remote) + Triplet + BERT + Qwen3-4B + 2-Stage PRD Process",
    version="3.2.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "TtalKkak Final AI Server with Triplets",
        "version": "3.1.0",
        "features": [
            "WhisperX Speech-to-Text",
            "Triplet Context Analysis", 
            "BERT Noise Filtering",
            "2-Stage PRD Process",
            "Notion Project Generation", 
            "Task Master PRD Format",
            "Advanced Task Generation"
        ],
        "workflow": "íšŒì˜ë¡ â†’ Triplet í•„í„°ë§ â†’ ê¸°íšì•ˆ â†’ Task Master PRD â†’ ì—…ë¬´ìƒì„±",
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    gpu_available = torch.cuda.is_available()
    gpu_count = torch.cuda.device_count() if gpu_available else 0
    
    memory_info = None
    if gpu_available:
        try:
            memory_info = {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3,
                "total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
        except:
            pass
    
    # WhisperX ì›ê²© ì„œë²„ ìƒíƒœ ì²´í¬
    whisperx_loaded = False
    try:
        import httpx
        with httpx.Client(timeout=httpx.Timeout(5.0)) as client:
            response = client.get(f"{WHISPERX_SERVER}/health")
            if response.status_code == 200:
                whisperx_loaded = response.json().get("whisperx_loaded", False)
    except:
        pass
    
    return HealthResponse(
        status="healthy",
        gpu_available=gpu_available,
        gpu_count=gpu_count,
        models_loaded={
            "whisperx": whisperx_loaded,  # ì›ê²© ì„œë²„ ìƒíƒœ
            "qwen3": qwen_model is not None,
            "triplet_bert": TRIPLET_AVAILABLE
        },
        memory_info=memory_info
    )

@app.post("/transcribe", response_model=TranscriptionResponse)
async def transcribe_audio(audio: UploadFile = File(...)):
    """ìŒì„± íŒŒì¼ ì „ì‚¬ (WhisperX ì›ê²© ì„œë²„)"""
    try:
        logger.info(f"ğŸ¤ Transcribing audio via remote server: {audio.filename}")
        
        # WhisperX ì›ê²© ì„œë²„ë¡œ ì „ì†¡
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •: connect=30ì´ˆ, read=20ë¶„, write=60ì´ˆ, pool=30ì´ˆ
        timeout = httpx.Timeout(
            connect=30.0,
            read=1200.0,  # 20ë¶„
            write=60.0,
            pool=30.0
        )
        async with httpx.AsyncClient(timeout=timeout) as client:
            # ë¨¼ì € ì„œë²„ ìƒíƒœ í™•ì¸
            try:
                health_response = await client.get(f"{WHISPERX_SERVER}/health")
                if health_response.status_code != 200:
                    logger.error(f"âŒ WhisperX server not healthy: {health_response.status_code}")
                    return TranscriptionResponse(
                        success=False,
                        error="WhisperX server not available"
                    )
            except Exception as e:
                logger.error(f"âŒ Cannot connect to WhisperX server: {e}")
                return TranscriptionResponse(
                    success=False,
                    error=f"Cannot connect to WhisperX server: {str(e)}"
                )
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì†¡
            files = {"audio": (audio.filename, await audio.read(), audio.content_type)}
            response = await client.post(f"{WHISPERX_SERVER}/transcribe", files=files)
            
            if response.status_code == 200:
                result = response.json()
                logger.info(f"âœ… Remote transcription completed")
                return TranscriptionResponse(**result)
            else:
                logger.error(f"âŒ WhisperX server error: {response.status_code}")
                return TranscriptionResponse(
                    success=False,
                    error=f"WhisperX server error: {response.status_code}"
                )
                
    except httpx.TimeoutException:
        logger.error("âŒ WhisperX server timeout")
        return TranscriptionResponse(
            success=False,
            error="WhisperX server timeout"
        )
    except Exception as e:
        logger.error(f"âŒ Transcription error: {e}")
        return TranscriptionResponse(
            success=False,
            error=str(e)
        )

@app.post("/transcribe-enhanced", response_model=EnhancedTranscriptionResponse)
async def transcribe_audio_enhanced(
    audio: UploadFile = File(...),
    enable_bert_filtering: bool = True,
    save_noise_log: bool = True
):
    """í–¥ìƒëœ ìŒì„± íŒŒì¼ ì „ì‚¬ (Triplet + BERT í•„í„°ë§)"""
    try:
        logger.info(f"ğŸ¤ Enhanced transcribing: {audio.filename}")
        
        # 1. ê¸°ë³¸ WhisperX ì „ì‚¬
        basic_result = await transcribe_audio(audio)
        if not basic_result.success:
            return EnhancedTranscriptionResponse(
                success=False,
                error=basic_result.error
            )
        
        # 2. Triplet í”„ë¡œì„¸ì„œë¡œ ì²˜ë¦¬ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if TRIPLET_AVAILABLE and enable_bert_filtering:
            try:
                triplet_processor = get_triplet_processor()
                
                enhanced_result = triplet_processor.process_whisperx_result(
                    whisperx_result=basic_result.transcription,
                    enable_bert_filtering=enable_bert_filtering,
                    save_noise_log=save_noise_log
                )
                
                if enhanced_result["success"]:
                    logger.info("âœ… Enhanced transcription with Triplets completed")
                    
                    return EnhancedTranscriptionResponse(
                        success=True,
                        transcription=basic_result.transcription,
                        triplet_data=enhanced_result.get("triplet_data"),
                        filtered_transcript=enhanced_result.get("filtered_transcript"),
                        processing_stats=enhanced_result.get("processing_stats")
                    )
                else:
                    logger.warning("âš ï¸ Triplet processing failed, using basic transcription")
            except Exception as e:
                logger.warning(f"âš ï¸ Triplet processing error: {e}")
        
        # 3. Triplet ì‚¬ìš© ë¶ˆê°€ëŠ¥ ì‹œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
        return EnhancedTranscriptionResponse(
            success=True,
            transcription=basic_result.transcription,
            filtered_transcript=basic_result.transcription["full_text"],
            processing_stats={"triplet_available": TRIPLET_AVAILABLE}
        )
        
    except Exception as e:
        logger.error(f"âŒ Enhanced transcription error: {e}")
        return EnhancedTranscriptionResponse(
            success=False,
            error=str(e)
        )

@app.post("/generate-notion-project", response_model=NotionProjectResponse)
async def generate_notion_project(request: AnalysisRequest):
    """1ë‹¨ê³„: íšŒì˜ë¡ â†’ ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„±"""
    try:
        logger.info("ğŸ“ Stage 1: Generating Notion project document...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± - generate_meeting_analysis_prompts ì‚¬ìš©
        from meeting_analysis_prompts import (
            generate_meeting_analysis_system_prompt,
            generate_meeting_analysis_user_prompt
        )
        
        # íšŒì˜ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = generate_meeting_analysis_system_prompt()
        user_prompt = generate_meeting_analysis_user_prompt(
            request.transcript, 
            request.additional_context or ""
        )
        
        # êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±
        result = generate_structured_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=NOTION_PROJECT_SCHEMA,
            temperature=0.3
        )
        
        if "error" in result:
            return NotionProjectResponse(
                success=False,
                error=result["error"]
            )
        
        # ë°ì´í„° ê²€ì¦
        validated_result = validate_notion_project(result)
        
        # ë…¸ì…˜ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        formatted_notion = format_notion_project(validated_result)
        
        logger.info("âœ… Stage 1 completed: Notion project generated")
        
        return NotionProjectResponse(
            success=True,
            notion_project=validated_result,
            formatted_notion=formatted_notion
        )
        
    except Exception as e:
        logger.error(f"âŒ Notion project generation error: {e}")
        return NotionProjectResponse(
            success=False,
            error=str(e)
        )

@app.post("/generate-task-master-prd", response_model=TaskMasterPRDResponse)
async def generate_task_master_prd(notion_project: Dict[str, Any]):
    """2ë‹¨ê³„: ë…¸ì…˜ ê¸°íšì•ˆ â†’ Task Master PRD ë³€í™˜"""
    try:
        logger.info("ğŸ”„ Stage 2: Converting to Task Master PRD format...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = "ë‹¹ì‹ ì€ ê¸°íšì•ˆì„ Task Master PRD í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
        user_prompt = generate_task_master_prd_prompt(notion_project)
        
        # êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„±
        result = generate_structured_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_schema=TASK_MASTER_PRD_SCHEMA,
            temperature=0.3
        )
        
        if "error" in result:
            return TaskMasterPRDResponse(
                success=False,
                error=result["error"]
            )
        
        # ë°ì´í„° ê²€ì¦
        validated_result = validate_task_master_prd(result)
        
        # Task Master PRD í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        formatted_prd = format_task_master_prd(validated_result)
        
        logger.info("âœ… Stage 2 completed: Task Master PRD generated")
        
        return TaskMasterPRDResponse(
            success=True,
            prd_data=validated_result,
            formatted_prd=formatted_prd
        )
        
    except Exception as e:
        logger.error(f"âŒ Task Master PRD generation error: {e}")
        return TaskMasterPRDResponse(
            success=False,
            error=str(e)
        )

@app.post("/two-stage-analysis", response_model=TwoStageAnalysisResponse)
async def two_stage_analysis(request: TwoStageAnalysisRequest):
    """2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ í†µí•© ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting 2-stage analysis process...")
        
        start_time = time.time()
        
        # 1ë‹¨ê³„: ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„±
        stage1_result = None
        if request.generate_notion:
            logger.info("ğŸ“ Stage 1: Generating Notion project...")
            notion_request = AnalysisRequest(
                transcript=request.transcript,
                additional_context=request.additional_context
            )
            stage1_response = await generate_notion_project(notion_request)
            
            if not stage1_response.success:
                return TwoStageAnalysisResponse(
                    success=False,
                    error=f"Stage 1 failed: {stage1_response.error}"
                )
            
            stage1_result = stage1_response.notion_project
        
        # 2ë‹¨ê³„: Task Master PRD ë³€í™˜
        stage2_result = None
        if request.generate_tasks and stage1_result:
            logger.info("ğŸ”„ Stage 2: Converting to Task Master PRD...")
            stage2_response = await generate_task_master_prd(stage1_result)
            
            if not stage2_response.success:
                return TwoStageAnalysisResponse(
                    success=False,
                    error=f"Stage 2 failed: {stage2_response.error}"
                )
            
            stage2_result = stage2_response.prd_data
        
        # 3ë‹¨ê³„: Task Master ì›Œí¬í”Œë¡œìš° (PRD â†’ Task â†’ ë³µì¡ë„ ë¶„ì„ â†’ ì„œë¸ŒíƒœìŠ¤í¬)
        stage3_result = None
        if request.generate_tasks and stage2_result:
            logger.info("ğŸ¯ Stage 3: Task Master workflow - PRD to Tasks...")
            
            try:
                # Step 3-1: PRDì—ì„œ íƒœìŠ¤í¬ ìƒì„± (Task Master ë°©ì‹)
                logger.info("   Step 3-1: Generating tasks from PRD...")
                task_items = await generate_tasks_from_prd(stage2_result, request.num_tasks)
                
                if not task_items:
                    logger.error("âŒ No tasks generated from PRD")
                    stage3_result = {
                        "success": False,
                        "summary": "Failed to generate tasks from PRD",
                        "error": "Failed to generate tasks from PRD",
                        "tasks": [],
                        "complexity_analysis": {},
                        "total_tasks": 0
                    }
                else:
                    logger.info(f"   âœ… Generated {len(task_items)} tasks")
                    
                    # Step 3-2: ë³µì¡ë„ ë¶„ì„ (Task Master ë°©ì‹)
                    logger.info("   Step 3-2: Analyzing task complexity...")
                    complexity_analysis = await analyze_task_complexity(task_items)
                    
                    # Step 3-3: ë³µì¡ë„ ê¸°ë°˜ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±
                    logger.info("   Step 3-3: Generating complexity-based subtasks...")
                    task_items_with_subtasks = await generate_subtasks_for_all_tasks(
                        task_items, 
                        complexity_analysis=complexity_analysis
                    )
                    
                    # Stage 3 ê²°ê³¼ êµ¬ì„±
                    stage3_result = {
                        "success": True,
                        "summary": f"Generated {len(task_items_with_subtasks)} tasks with {sum(len(task.subtasks) for task in task_items_with_subtasks)} subtasks",
                        "action_items": [task.dict() for task in task_items_with_subtasks],  # tasks â†’ action_itemsë¡œ ë³€ê²½
                        "tasks": [task.dict() for task in task_items_with_subtasks],  # í˜¸í™˜ì„±ì„ ìœ„í•´ tasksë„ ìœ ì§€
                        "complexity_analysis": complexity_analysis,
                        "total_tasks": len(task_items_with_subtasks),
                        "total_subtasks": sum(len(task.subtasks) for task in task_items_with_subtasks),
                        "workflow_type": "task_master_3_step"
                    }
                    
                    logger.info(f"   âœ… Task Master workflow completed: {len(task_items_with_subtasks)} tasks, "
                              f"{stage3_result['total_subtasks']} subtasks")
                    
                    # ìƒì„±ëœ ëª¨ë“  íƒœìŠ¤í¬ì™€ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„¸ ë¡œê·¸ ì¶œë ¥
                    try:
                        logger.info("\n" + "="*80)
                        logger.info("ğŸ“‹ ìƒì„±ëœ íƒœìŠ¤í¬ ë° ì„œë¸ŒíƒœìŠ¤í¬ ì „ì²´ ëª©ë¡")
                        logger.info("="*80)
                        
                        for idx, task in enumerate(task_items_with_subtasks, 1):
                            try:
                                logger.info(f"\nğŸ“Œ [{idx}] {task.title}")
                                logger.info(f"   ğŸ“ ì„¤ëª…: {task.description[:100] if task.description else ''}{'...' if task.description and len(task.description) > 100 else ''}")
                                logger.info(f"   âš¡ ë³µì¡ë„: {getattr(task, 'complexity', 'medium')}")
                                logger.info(f"   ğŸ¯ ìš°ì„ ìˆœìœ„: {task.priority}")
                                logger.info(f"   â±ï¸ ì˜ˆìƒì‹œê°„: {task.estimated_hours or 0}ì‹œê°„")
                                logger.info(f"   ğŸ“… ì‹œì‘ì¼: {task.start_date or 'ë¯¸ì •'}")
                                logger.info(f"   ğŸ“… ë§ˆê°ì¼: {task.due_date or 'ë¯¸ì •'}")
                                
                                if hasattr(task, 'dependencies') and task.dependencies:
                                    logger.info(f"   ğŸ”— ì˜ì¡´ì„±: {', '.join(map(str, task.dependencies))}")
                                
                                if hasattr(task, 'acceptance_criteria') and task.acceptance_criteria:
                                    logger.info(f"   âœ… ìˆ˜ë½ ê¸°ì¤€:")
                                    for criteria in task.acceptance_criteria[:3]:
                                        logger.info(f"      - {criteria}")
                                
                                if hasattr(task, 'tags') and task.tags:
                                    logger.info(f"   ğŸ·ï¸ íƒœê·¸: {', '.join(map(str, task.tags))}")
                                
                                if hasattr(task, 'subtasks') and task.subtasks:
                                    logger.info(f"   ğŸ“‚ ì„œë¸ŒíƒœìŠ¤í¬ ({len(task.subtasks)}ê°œ):")
                                    for sub_idx, subtask in enumerate(task.subtasks, 1):
                                        try:
                                            logger.info(f"      [{idx}.{sub_idx}] {subtask.title}")
                                            if hasattr(subtask, 'description') and subtask.description:
                                                logger.info(f"         - ì„¤ëª…: {subtask.description[:60]}{'...' if len(subtask.description) > 60 else ''}")
                                            logger.info(f"         - ì˜ˆìƒì‹œê°„: {getattr(subtask, 'estimated_hours', 0) or 0}ì‹œê°„")
                                            if hasattr(subtask, 'start_date') and subtask.start_date:
                                                logger.info(f"         - ì‹œì‘ì¼: {subtask.start_date}")
                                            if hasattr(subtask, 'due_date') and subtask.due_date:
                                                logger.info(f"         - ë§ˆê°ì¼: {subtask.due_date}")
                                        except Exception as e:
                                            logger.error(f"      ì„œë¸ŒíƒœìŠ¤í¬ ë¡œê·¸ ì¶œë ¥ ì˜¤ë¥˜: {e}")
                                else:
                                    logger.info("   ğŸ“‚ ì„œë¸ŒíƒœìŠ¤í¬: ì—†ìŒ")
                            except Exception as e:
                                logger.error(f"íƒœìŠ¤í¬ {idx} ë¡œê·¸ ì¶œë ¥ ì˜¤ë¥˜: {e}")
                        
                        logger.info("\n" + "="*80)
                        logger.info(f"ğŸ“Š ì´ ìš”ì•½: ë©”ì¸ íƒœìŠ¤í¬ {len(task_items_with_subtasks)}ê°œ, ì„œë¸ŒíƒœìŠ¤í¬ {stage3_result['total_subtasks']}ê°œ")
                        logger.info("="*80 + "\n")
                    except Exception as e:
                        logger.error(f"ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        logger.info(f"âœ… íƒœìŠ¤í¬ ìƒì„± ì™„ë£Œ: {len(task_items_with_subtasks)}ê°œ íƒœìŠ¤í¬, {stage3_result['total_subtasks']}ê°œ ì„œë¸ŒíƒœìŠ¤í¬")
                    
            except Exception as e:
                logger.error(f"âŒ Task Master workflow failed: {str(e)}")
                stage3_result = {
                    "success": False,
                    "summary": f"Task generation failed: {str(e)}",
                    "error": f"Task Master workflow error: {str(e)}",
                    "tasks": [],
                    "complexity_analysis": {},
                    "total_tasks": 0
                }
        
        total_time = time.time() - start_time
        
        return TwoStageAnalysisResponse(
            success=True,
            stage1_notion=stage1_result,
            stage2_prd=stage2_result,
            stage3_tasks=stage3_result,
            formatted_notion=format_notion_project(stage1_result) if stage1_result else None,
            formatted_prd=format_task_master_prd(stage2_result) if stage2_result else None,
            processing_time=total_time
        )
        
    except Exception as e:
        logger.error(f"âŒ 2-stage analysis error: {e}")
        return TwoStageAnalysisResponse(
            success=False,
            error=str(e)
        )

@app.post("/two-stage-pipeline-text", response_model=EnhancedTwoStageResult)
async def enhanced_two_stage_pipeline_text(request: dict):
    """í…ìŠ¤íŠ¸ ì…ë ¥ ì „ìš© 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸: í…ìŠ¤íŠ¸ â†’ Triplet í•„í„°ë§ â†’ 2ë‹¨ê³„ ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting text-based 2-stage pipeline...")
        
        transcript = request.get("transcript", "")
        if not transcript:
            raise ValueError("transcriptê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
        enable_bert_filtering = request.get("enable_bert_filtering", True)
        
        # VLLM ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        use_vllm = os.getenv("USE_VLLM", "true").lower() == "true"
        
        # í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì²˜ë¦¬í•˜ì—¬ Triplet ìƒì„± ë° í•„í„°ë§
        if TRIPLET_AVAILABLE and enable_bert_filtering:
            try:
                triplet_processor = get_triplet_processor()
                # í…ìŠ¤íŠ¸ë¥¼ WhisperX í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                mock_whisperx_result = {
                    "segments": [{"text": transcript, "start": 0, "end": 60}],
                    "full_text": transcript,
                    "language": "ko"
                }
                
                enhanced_result = triplet_processor.process_whisperx_result(
                    whisperx_result=mock_whisperx_result,
                    enable_bert_filtering=enable_bert_filtering,
                    save_noise_log=False
                )
                
                if enhanced_result["success"]:
                    filtered_transcript = enhanced_result["filtered_transcript"]
                    triplet_stats = enhanced_result.get("triplet_stats", {})
                    classification_stats = enhanced_result.get("classification_stats", {})
                else:
                    filtered_transcript = transcript
                    triplet_stats = {}
                    classification_stats = {}
                    
            except Exception as e:
                logger.warning(f"Triplet ì²˜ë¦¬ ì‹¤íŒ¨, ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©: {e}")
                filtered_transcript = transcript
                triplet_stats = {}
                classification_stats = {}
        else:
            filtered_transcript = transcript
            triplet_stats = {}
            classification_stats = {}
        
        # Stage 1: Notion í”„ë¡œì íŠ¸ ìƒì„±
        stage1_notion = None
        if request.get("generate_notion", True):
            try:
                # generate_meeting_analysis_prompts í•¨ìˆ˜ ì‚¬ìš©
                from meeting_analysis_prompts import (
                    generate_meeting_analysis_system_prompt,
                    generate_meeting_analysis_user_prompt
                )
                
                # íšŒì˜ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
                system_prompt = generate_meeting_analysis_system_prompt()
                user_prompt = generate_meeting_analysis_user_prompt(filtered_transcript)
                
                if use_vllm and qwen_model and qwen_tokenizer:
                    from vllm import SamplingParams
                    sampling_params = SamplingParams(
                        temperature=0.3,
                        max_tokens=2048,
                        stop=["<|im_end|>", "<|endoftext|>"]
                    )
                    
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                    
                    formatted_prompt = qwen_tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    
                    outputs = qwen_model.generate([formatted_prompt], sampling_params)
                    result_text = outputs[0].outputs[0].text.strip()
                    
                    try:
                        import json
                        stage1_notion = json.loads(result_text)
                    except:
                        stage1_notion = {"title": "AI í”„ë¡œì íŠ¸", "overview": result_text}
                        
            except Exception as e:
                logger.error(f"Notion ìƒì„± ì‹¤íŒ¨: {e}")
                stage1_notion = None
        
        # Stage 2: PRD ìƒì„±
        stage2_prd = None
        if stage1_notion and request.get("generate_prd", True):
            try:
                from prd_generation_prompts import generate_task_master_prd_prompt
                
                # generate_task_master_prd_promptëŠ” ë…¸ì…˜ í”„ë¡œì íŠ¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ìŒ
                full_prompt = generate_task_master_prd_prompt(stage1_notion)
                
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬
                system_prompt = "ë‹¹ì‹ ì€ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ìƒì„¸í•œ PRD(Product Requirements Document)ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                user_prompt = full_prompt
                
                if use_vllm and qwen_model and qwen_tokenizer:
                    from vllm import SamplingParams
                    sampling_params = SamplingParams(
                        temperature=0.3,
                        max_tokens=2048,
                        stop=["<|im_end|>", "<|endoftext|>"]
                    )
                    
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                    
                    formatted_prompt = qwen_tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    
                    outputs = qwen_model.generate([formatted_prompt], sampling_params)
                    result_text = outputs[0].outputs[0].text.strip()
                    
                    try:
                        stage2_prd = json.loads(result_text)
                    except:
                        stage2_prd = {"title": "PRD", "overview": result_text}
                        
            except Exception as e:
                logger.error(f"PRD ìƒì„± ì‹¤íŒ¨: {e}")
                stage2_prd = None
        
        # Stage 3: ì—…ë¬´ ìƒì„±
        stage3_tasks = None
        if stage2_prd and request.get("generate_tasks", True):
            try:
                # PRDë¥¼ íƒœìŠ¤í¬ë¡œ ë³€í™˜
                num_tasks = request.get("num_tasks", 5)
                system_prompt = generate_prd_to_tasks_system_prompt(num_tasks)
                user_prompt = generate_prd_to_tasks_user_prompt(stage2_prd, num_tasks)
                
                if use_vllm and qwen_model and qwen_tokenizer:
                    from vllm import SamplingParams
                    sampling_params = SamplingParams(
                        temperature=0.3,
                        max_tokens=4096,
                        stop=["<|im_end|>", "<|endoftext|>"]
                    )
                    
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                    
                    formatted_prompt = qwen_tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    
                    outputs = qwen_model.generate([formatted_prompt], sampling_params)
                    result_text = outputs[0].outputs[0].text.strip()
                    
                    try:
                        stage3_tasks = json.loads(result_text)
                    except:
                        stage3_tasks = {"tasks": []}
                        
            except Exception as e:
                logger.error(f"ì—…ë¬´ ìƒì„± ì‹¤íŒ¨: {e}")
                stage3_tasks = None
        
        # Stage 4: ë³µì¡ë„ ë¶„ì„ ë° ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±
        stage4_subtasks = None
        if stage3_tasks and stage3_tasks.get("tasks") and request.get("generate_subtasks", True):
            try:
                logger.info("ğŸ” Stage 4: íƒœìŠ¤í¬ ë³µì¡ë„ ë¶„ì„ ë° ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±...")
                
                # ë³µì¡ë„ ë¶„ì„
                complexity_system_prompt = generate_complexity_analysis_system_prompt()
                complexity_user_prompt = generate_complexity_analysis_prompt(stage3_tasks)
                
                if use_vllm and qwen_model and qwen_tokenizer:
                    from vllm import SamplingParams
                    sampling_params = SamplingParams(
                        temperature=0.2,
                        max_tokens=2048,
                        stop=["<|im_end|>", "<|endoftext|>"]
                    )
                    
                    messages = [
                        {"role": "system", "content": complexity_system_prompt},
                        {"role": "user", "content": complexity_user_prompt}
                    ]
                    
                    formatted_prompt = qwen_tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                    
                    outputs = qwen_model.generate([formatted_prompt], sampling_params)
                    complexity_result = outputs[0].outputs[0].text.strip()
                    
                    try:
                        complexity_analysis = json.loads(complexity_result)
                        logger.info(f"âœ… ë³µì¡ë„ ë¶„ì„ ì™„ë£Œ: {len(complexity_analysis)}ê°œ íƒœìŠ¤í¬ ë¶„ì„ë¨")
                        
                        # ê° íƒœìŠ¤í¬ì— ëŒ€í•œ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±
                        stage4_subtasks = []
                        for task_idx, task in enumerate(stage3_tasks.get("tasks", [])):
                            # í•´ë‹¹ íƒœìŠ¤í¬ì˜ ë³µì¡ë„ ë¶„ì„ ì°¾ê¸°
                            task_analysis = next(
                                (a for a in complexity_analysis if a.get("taskId") == task.get("id")),
                                {"complexityScore": 5, "recommendedSubtasks": 3}
                            )
                            
                            # ë³µì¡ë„ê°€ ë†’ì€ íƒœìŠ¤í¬ë§Œ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„± (ë³µì¡ë„ 5 ì´ìƒ)
                            if task_analysis.get("complexityScore", 5) >= 5:
                                logger.info(f"ğŸ“ íƒœìŠ¤í¬ '{task.get('title', '')}' ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„± ì¤‘...")
                                
                                # ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±
                                subtask_prompt = generate_complexity_based_subtask_prompt(
                                    task, 
                                    task_analysis, 
                                    next_subtask_id=task.get("id", 1) * 100 + 1
                                )
                                
                                subtask_system_prompt = generate_complexity_based_subtask_system_prompt(
                                    task_analysis.get("recommendedSubtasks", 3),
                                    task.get("id", 1) * 100 + 1
                                )
                                
                                messages = [
                                    {"role": "system", "content": subtask_system_prompt},
                                    {"role": "user", "content": subtask_prompt}
                                ]
                                
                                formatted_prompt = qwen_tokenizer.apply_chat_template(
                                    messages, tokenize=False, add_generation_prompt=True
                                )
                                
                                outputs = qwen_model.generate([formatted_prompt], sampling_params)
                                subtask_result = outputs[0].outputs[0].text.strip()
                                
                                try:
                                    subtasks = json.loads(subtask_result)
                                    task["subtasks"] = subtasks.get("subtasks", [])
                                    task["complexityScore"] = task_analysis.get("complexityScore")
                                    task["complexityReasoning"] = task_analysis.get("reasoning", "")
                                    stage4_subtasks.append({
                                        "taskId": task.get("id"),
                                        "taskTitle": task.get("title"),
                                        "subtasks": subtasks.get("subtasks", []),
                                        "complexityScore": task_analysis.get("complexityScore"),
                                        "reasoning": task_analysis.get("reasoning", "")
                                    })
                                    logger.info(f"âœ… {len(subtasks.get('subtasks', []))}ê°œ ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±ë¨")
                                except Exception as e:
                                    logger.warning(f"ì„œë¸ŒíƒœìŠ¤í¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                                    task["subtasks"] = []
                            else:
                                logger.info(f"â­ï¸ íƒœìŠ¤í¬ '{task.get('title', '')}' ë³µì¡ë„ ë‚®ìŒ ({task_analysis.get('complexityScore', 0)}) - ì„œë¸ŒíƒœìŠ¤í¬ ìƒëµ")
                    
                    except Exception as e:
                        logger.error(f"ë³µì¡ë„ ë¶„ì„ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        complexity_analysis = []
                        
            except Exception as e:
                logger.error(f"Stage 4 (ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±) ì‹¤íŒ¨: {e}")
                stage4_subtasks = None
        
        return EnhancedTwoStageResult(
            success=True,
            triplet_stats=triplet_stats,
            classification_stats=classification_stats,
            stage1_notion=stage1_notion,
            stage2_prd=stage2_prd,
            stage3_tasks=stage3_tasks,
            stage4_subtasks=stage4_subtasks if 'stage4_subtasks' in locals() else None,
            formatted_notion=format_notion_project(stage1_notion) if stage1_notion else None,
            formatted_prd=format_task_master_prd(stage2_prd) if stage2_prd else None,
            original_transcript_length=len(transcript),
            filtered_transcript_length=len(filtered_transcript),
            noise_reduction_ratio=1.0 - (len(filtered_transcript) / len(transcript)) if transcript else 0,
            processing_time=time.time() - time.time()
        )
        
    except Exception as e:
        logger.error(f"âŒ Text-based 2-stage pipeline error: {e}")
        return EnhancedTwoStageResult(
            success=False,
            error=str(e)
        )

@app.post("/two-stage-pipeline", response_model=EnhancedTwoStageResult)
async def enhanced_two_stage_pipeline(
    audio: UploadFile = File(...),
    enable_bert_filtering: bool = True,
    save_noise_log: bool = True,
    generate_notion: bool = True,
    generate_tasks: bool = True,
    num_tasks: int = 5
):
    """ìµœì¢… ì „ì²´ íŒŒì´í”„ë¼ì¸: ìŒì„± â†’ Triplet í•„í„°ë§ â†’ 2ë‹¨ê³„ ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting enhanced 2-stage pipeline with Triplets...")
        
        start_time = time.time()
        
        # 1ë‹¨ê³„: í–¥ìƒëœ ì „ì‚¬ (Triplet + BERT)
        logger.info("ğŸ“ Step 1: Enhanced transcription...")
        enhanced_transcribe_result = await transcribe_audio_enhanced(
            audio=audio,
            enable_bert_filtering=enable_bert_filtering,
            save_noise_log=save_noise_log
        )
        
        if not enhanced_transcribe_result.success:
            return EnhancedTwoStageResult(
                success=False,
                error=enhanced_transcribe_result.error
            )
        
        # 2ë‹¨ê³„: í•„í„°ë§ëœ í…ìŠ¤íŠ¸ë¡œ 2ë‹¨ê³„ ë¶„ì„
        logger.info("ğŸ§  Step 2: Running 2-stage analysis on filtered content...")
        filtered_text = enhanced_transcribe_result.filtered_transcript or \
                       enhanced_transcribe_result.transcription["full_text"]
        
        # ğŸ”¥ ì²­í‚¹ í•„ìš”ì„± ì²´í¬ ë° ì²˜ë¦¬
        try:
            from chunking_processor import get_chunking_processor
            chunking_processor = get_chunking_processor(max_context_tokens=32768)
            
            # í•„í„°ë§ëœ í…ìŠ¤íŠ¸ í† í° ìˆ˜ í™•ì¸
            estimated_tokens = chunking_processor.estimate_tokens(filtered_text)
            logger.info(f"ğŸ“Š í•„í„°ë§ëœ í…ìŠ¤íŠ¸ í† í° ìˆ˜: {estimated_tokens}")
            
            # ë…¸ì´ì¦ˆ ì œê±° íš¨ê³¼ ë¡œê·¸
            original_tokens = chunking_processor.estimate_tokens(
                enhanced_transcribe_result.transcription["full_text"]
            )
            token_reduction = ((original_tokens - estimated_tokens) / original_tokens * 100) if original_tokens > 0 else 0
            logger.info(f"ğŸ¯ í† í° ê°ì†Œ íš¨ê³¼: {original_tokens} â†’ {estimated_tokens} ({token_reduction:.1f}% ê°ì†Œ)")
            
        except ImportError:
            logger.warning("âš ï¸ ì²­í‚¹ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            estimated_tokens = len(filtered_text) * 1.5  # ëŒ€ëµì  ì¶”ì •
        
        analysis_request = TwoStageAnalysisRequest(
            transcript=filtered_text,
            generate_notion=generate_notion,
            generate_tasks=generate_tasks,
            num_tasks=num_tasks
        )
        analysis_result = await two_stage_analysis(analysis_request)
        
        total_time = time.time() - start_time
        
        # ê²°ê³¼ í†µê³„ ê³„ì‚°
        original_length = len(enhanced_transcribe_result.transcription["full_text"])
        filtered_length = len(filtered_text)
        noise_reduction = 1.0 - (filtered_length / original_length) if original_length > 0 else 0.0
        
        return EnhancedTwoStageResult(
            success=True,
            triplet_stats=enhanced_transcribe_result.triplet_data,
            classification_stats=enhanced_transcribe_result.processing_stats,
            stage1_notion=analysis_result.stage1_notion if analysis_result.success else None,
            stage2_prd=analysis_result.stage2_prd if analysis_result.success else None,
            stage3_tasks=analysis_result.stage3_tasks if analysis_result.success else None,
            formatted_notion=analysis_result.formatted_notion if analysis_result.success else None,
            formatted_prd=analysis_result.formatted_prd if analysis_result.success else None,
            original_transcript_length=original_length,
            filtered_transcript_length=filtered_length,
            noise_reduction_ratio=noise_reduction,
            processing_time=total_time
        )
        
    except Exception as e:
        logger.error(f"âŒ Enhanced 2-stage pipeline error: {e}")
        return EnhancedTwoStageResult(
            success=False,
            error=str(e)
        )


@app.post("/pipeline-final", response_model=Dict[str, Any])
async def final_pipeline(
    request: Request,
    audio: UploadFile = File(None),  
    transcript: str = Form(None),
    generate_notion: bool = Form(True),
    generate_tasks: bool = Form(True),
    num_tasks: int = Form(5),
    apply_bert_filtering: bool = Form(False)  # í…ìŠ¤íŠ¸ ì…ë ¥ì‹œ BERT í•„í„°ë§ ì—¬ë¶€
):
    """ğŸš€ ìµœì¢… ì „ì²´ íŒŒì´í”„ë¼ì¸: ìŒì„±/í…ìŠ¤íŠ¸ ìë™ ê°ì§€ â†’ VLLM ì´ˆê³ ì† ë¶„ì„"""
    try:
        logger.info("ğŸš€ Starting final pipeline with 2-stage process...")
        
        start_time = time.time()
        
        # JSON ìš”ì²­ ì²˜ë¦¬
        if request.headers.get("content-type") == "application/json":
            body = await request.json()
            transcript = body.get('transcript')
            generate_notion = body.get('generate_notion', True)
            generate_tasks = body.get('generate_tasks', True)
            num_tasks = body.get('num_tasks', 5)
            apply_bert_filtering = body.get('apply_bert_filtering', False)
            logger.info("ğŸ“ JSON request detected")
        else:
            logger.info("ğŸ“ Form request detected")
        
        if transcript:
            # í…ìŠ¤íŠ¸ ì…ë ¥ - ì´ë¯¸ ì •ë¦¬ëœ íšŒì˜ë¡ì´ë¯€ë¡œ BERT í•„í„°ë§ ìƒëµ
            logger.info("ğŸ“ Text input detected (clean transcript, skipping BERT filtering)")
            full_text = transcript
        elif audio and audio.filename:
            # ìŒì„± íŒŒì¼ ì…ë ¥
            logger.info("ğŸ“ Step 1: Transcribing audio...")
            transcribe_result = await transcribe_audio(audio)
            if not transcribe_result.success:
                return {
                    "success": False,
                    "step": "transcription",
                    "error": transcribe_result.error
                }
            
            # ìŒì„± ì „ì‚¬ ê²°ê³¼ì— BERT í•„í„°ë§ ì ìš©
            raw_text = transcribe_result.transcription["full_text"]
            if TRIPLET_AVAILABLE:
                try:
                    triplet_processor = get_triplet_processor()
                    enhanced_result = triplet_processor.process_whisperx_result(
                        whisperx_result=transcribe_result.transcription,
                        enable_bert_filtering=True,
                        save_noise_log=False
                    )
                    
                    if enhanced_result["success"]:
                        full_text = enhanced_result["filtered_transcript"]
                        logger.info(f"âœ… BERT filtering applied to audio: {len(raw_text)} â†’ {len(full_text)} chars")
                    else:
                        full_text = raw_text
                        logger.warning("BERT filtering failed on audio, using original transcription")
                except Exception as e:
                    logger.warning(f"BERT filtering error on audio: {e}, using original transcription")
                    full_text = raw_text
            else:
                full_text = raw_text
        else:
            return {
                "success": False,
                "step": "input",
                "error": "Either transcript or audio file is required"
            }
        
        # 2ë‹¨ê³„: 2ë‹¨ê³„ ë¶„ì„
        logger.info("ğŸ§  Step 2: Running 2-stage analysis...")
        analysis_request = TwoStageAnalysisRequest(
            transcript=full_text,
            generate_notion=generate_notion,
            generate_tasks=generate_tasks,
            generate_subtasks=True,  # ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„± í™œì„±í™”
            num_tasks=num_tasks
        )
        analysis_result = await two_stage_analysis(analysis_request)
        
        total_time = time.time() - start_time
        
        # í…ìŠ¤íŠ¸ ì…ë ¥ê³¼ ìŒì„± ì…ë ¥ì— ë”°ë¼ transcription ì •ë³´ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
        if transcript:
            # í…ìŠ¤íŠ¸ ì…ë ¥ì˜ ê²½ìš° ê°€ì§œ transcription ì •ë³´ ìƒì„±
            transcription_info = {
                "full_text": full_text,
                "segments": [{"text": full_text, "start": 0, "end": 60}],
                "language": "ko"
            }
        else:
            # ìŒì„± ì…ë ¥ì˜ ê²½ìš° ì‹¤ì œ transcription ì •ë³´ ì‚¬ìš©
            transcription_info = transcribe_result.transcription

        # ğŸ”¥ ìƒì„± ì™„ë£Œ ë¡œê·¸ - íšŒì˜ë¡ê³¼ íƒœìŠ¤í¬ ìƒì„¸ ì¶œë ¥
        logger.info("=" * 80)
        logger.info("ğŸ‰ AI íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ!")
        logger.info("=" * 80)
        
        # 1. íšŒì˜ë¡ ìš”ì•½ ì¶œë ¥ (stage1_notionì—ì„œ ì •ë³´ ì¶”ì¶œ)
        if analysis_result.stage1_notion:
            logger.info("\nğŸ“‹ [Stage 1: Notion í”„ë¡œì íŠ¸ ë¶„ì„]")
            logger.info(f"ì œëª©: {analysis_result.stage1_notion.get('title', 'N/A')}")
            logger.info(f"ê°œìš”: {analysis_result.stage1_notion.get('overview', 'N/A')[:200]}...")
            if 'objectives' in analysis_result.stage1_notion:
                logger.info(f"ëª©í‘œ: {len(analysis_result.stage1_notion.get('objectives', []))}ê°œ")
            if 'key_features' in analysis_result.stage1_notion:
                logger.info(f"ì£¼ìš” ê¸°ëŠ¥: {len(analysis_result.stage1_notion.get('key_features', []))}ê°œ")
        
        # 2. PRD ì •ë³´ ì¶œë ¥
        if analysis_result.stage2_prd:
            logger.info("\nğŸ“„ [Stage 2: Task Master PRD]")
            logger.info(f"ì œëª©: {analysis_result.stage2_prd.get('title', 'N/A')}")
            logger.info(f"í”„ë¡œì íŠ¸ ë²”ìœ„: {analysis_result.stage2_prd.get('scope', 'N/A')[:200]}...")
            if 'requirements' in analysis_result.stage2_prd:
                logger.info(f"ìš”êµ¬ì‚¬í•­: {len(analysis_result.stage2_prd.get('requirements', []))}ê°œ")
        
        # 3. ìƒì„±ëœ íƒœìŠ¤í¬ ëª©ë¡ ì¶œë ¥
        if analysis_result.stage3_tasks:
            logger.info("\nğŸ“Œ [Stage 3: ìƒì„±ëœ íƒœìŠ¤í¬ ëª©ë¡]")
            
            # stage3_tasksê°€ dictì´ê³  action_items í‚¤ê°€ ìˆëŠ” ê²½ìš°
            if isinstance(analysis_result.stage3_tasks, dict) and 'action_items' in analysis_result.stage3_tasks:
                tasks = analysis_result.stage3_tasks['action_items']
                logger.info(f"ì´ {len(tasks)}ê°œ íƒœìŠ¤í¬ ìƒì„±")
                
                # ìš”ì•½ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶œë ¥
                if 'summary' in analysis_result.stage3_tasks:
                    logger.info(f"í”„ë¡œì íŠ¸ ìš”ì•½: {analysis_result.stage3_tasks['summary'][:100]}...")
            # stage3_tasksê°€ ë°”ë¡œ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            elif isinstance(analysis_result.stage3_tasks, list):
                tasks = analysis_result.stage3_tasks
                logger.info(f"ì´ {len(tasks)}ê°œ íƒœìŠ¤í¬ ìƒì„±")
            else:
                tasks = []
                logger.info("íƒœìŠ¤í¬ êµ¬ì¡° í™•ì¸ í•„ìš”")
            
            logger.info("-" * 60)
            
            for idx, task in enumerate(tasks, 1):
                logger.info(f"\níƒœìŠ¤í¬ {idx}: {task.get('title', 'N/A')}")
                logger.info(f"  ğŸ“ ì„¤ëª…: {task.get('description', 'N/A')[:100]}...")
                logger.info(f"  ğŸ¯ ìš°ì„ ìˆœìœ„: {task.get('priority', 'N/A')}")
                logger.info(f"  ğŸ“… ì‹œì‘ì¼: {task.get('start_date', 'N/A')}")
                logger.info(f"  ğŸ“… ë§ˆê°ì¼: {task.get('deadline', 'N/A')}")
                logger.info(f"  â±ï¸ ì˜ˆìƒì‹œê°„: {task.get('estimated_hours', 0)}ì‹œê°„")
                logger.info(f"  ğŸ’¼ ë‹´ë‹¹ì: {task.get('assignee', 'ë¯¸ë°°ì •')}")
                
                # ì„œë¸ŒíƒœìŠ¤í¬ê°€ ìˆìœ¼ë©´ í‘œì‹œ
                subtasks = task.get('subtasks', [])
                if subtasks:
                    logger.info(f"  ğŸ“‚ ì„œë¸ŒíƒœìŠ¤í¬: {len(subtasks)}ê°œ")
                    for sub_idx, subtask in enumerate(subtasks[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                        logger.info(f"    - {subtask.get('title', 'N/A')}")
                    if len(subtasks) > 3:
                        logger.info(f"    ... ì™¸ {len(subtasks)-3}ê°œ")
        
        # 4. ì²˜ë¦¬ ì‹œê°„ ë° í†µê³„
        logger.info("\nâ° [ì²˜ë¦¬ ì‹œê°„ ë° í†µê³„]")
        logger.info(f"ì´ ì²˜ë¦¬ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        # tasks ë³€ìˆ˜ê°€ ì •ì˜ë˜ì–´ ìˆìœ¼ë©´ í†µê³„ ì¶œë ¥
        if 'tasks' in locals() and tasks:
            total_subtasks = sum(len(task.get('subtasks', [])) for task in tasks)
            logger.info(f"ìƒì„± í•­ëª©: íƒœìŠ¤í¬ {len(tasks)}ê°œ, ì„œë¸ŒíƒœìŠ¤í¬ {total_subtasks}ê°œ")
            
            # ìš°ì„ ìˆœìœ„ë³„ í†µê³„
            high_priority = sum(1 for task in tasks if task.get('priority', '').lower() == 'high')
            medium_priority = sum(1 for task in tasks if task.get('priority', '').lower() == 'medium')
            low_priority = sum(1 for task in tasks if task.get('priority', '').lower() == 'low')
            logger.info(f"ìš°ì„ ìˆœìœ„: High({high_priority}), Medium({medium_priority}), Low({low_priority})")
            
            # ì´ ì˜ˆìƒ ì‹œê°„
            total_hours = sum(task.get('estimated_hours', 0) for task in tasks)
            logger.info(f"ì´ ì˜ˆìƒ ì‘ì—…ì‹œê°„: {total_hours}ì‹œê°„")
        
        logger.info("=" * 80)
        
        # ê²°ê³¼ ë°˜í™˜ - ë°±ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ
        return {
            "success": True,
            "step": "completed",
            "transcription": transcription_info,
            # ë°±ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” ìµœìƒìœ„ í•„ë“œë“¤
            "stage1_notion": analysis_result.stage1_notion,
            "stage2_prd": analysis_result.stage2_prd,
            "stage3_tasks": analysis_result.stage3_tasks,
            "formatted_notion": analysis_result.formatted_notion,
            "formatted_prd": analysis_result.formatted_prd,
            # ì¶”ê°€ ì •ë³´
            "analysis": {
                "notion_project": analysis_result.stage1_notion,
                "task_master_prd": analysis_result.stage2_prd,
                "generated_tasks": analysis_result.stage3_tasks,
                "formatted_notion": analysis_result.formatted_notion,
                "formatted_prd": analysis_result.formatted_prd
            },
            "processing_time": total_time,
            "model_info": {
                "whisperx": "large-v3",
                "qwen3": "Qwen3-4B LoRA",
                "process": "2-stage-task-master",
                "triplet_available": TRIPLET_AVAILABLE
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Final pipeline error: {e}")
        return {
            "success": False,
            "step": "pipeline",
            "error": str(e)
        }

if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    workers = int(os.getenv("WORKERS", "1"))
    
    logger.info(f"ğŸš€ Starting final server with Triplets on {host}:{port}")
    
    uvicorn.run(
        "ai_server_final_with_triplets:app",
        host=host,
        port=port,
        workers=workers,
        reload=False,
        log_level="info"
    )