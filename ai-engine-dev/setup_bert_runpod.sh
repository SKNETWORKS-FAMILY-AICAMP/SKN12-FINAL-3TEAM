#!/bin/bash

echo "========================================="
echo "ğŸš€ RunPod BERT ì„¤ì • ìŠ¤í¬ë¦½íŠ¸"
echo "========================================="

# BERT ëª¨ë¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /workspace/SKN12-FINAL-3TEAM/Bertëª¨ë¸/Ttalkkak_model_v2/

# config.json ìƒì„±
echo "ğŸ“„ Creating config.json..."
cat > config.json << 'EOF'
{
  "architectures": [
    "BertForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.3,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.3,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.53.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 32002,
  "id2label": {
    "0": "valid",
    "1": "noise"
  },
  "label2id": {
    "valid": 0,
    "noise": 1
  }
}
EOF

# tokenizer_config.json ìƒì„±
echo "ğŸ“ Creating tokenizer_config.json..."
cat > tokenizer_config.json << 'EOF'
{
  "added_tokens_decoder": {
    "0": {
      "content": "[PAD]",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "1": {
      "content": "[UNK]",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "2": {
      "content": "[CLS]",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "3": {
      "content": "[SEP]",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "4": {
      "content": "[MASK]",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "32000": {
      "content": "[TGT]",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "32001": {
      "content": "[/TGT]",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    }
  },
  "additional_special_tokens": [
    "[TGT]",
    "[/TGT]"
  ],
  "clean_up_tokenization_spaces": false,
  "cls_token": "[CLS]",
  "do_basic_tokenize": true,
  "do_lower_case": false,
  "mask_token": "[MASK]",
  "model_max_length": 512,
  "pad_token": "[PAD]",
  "sep_token": "[SEP]",
  "tokenize_chinese_chars": true,
  "tokenizer_class": "BertTokenizer",
  "unk_token": "[UNK]"
}
EOF

# ai-engine-dev ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /workspace/SKN12-FINAL-3TEAM/ai-engine-dev/

# simple_bert_filter.pyê°€ ì—†ìœ¼ë©´ ìƒì„±
if [ ! -f "simple_bert_filter.py" ]; then
    echo "ğŸ”§ Creating simple_bert_filter.py..."
    cat > simple_bert_filter.py << 'EOF'
"""ê°„ë‹¨í•œ BERT í•„í„°ë§ ëŒ€ì²´ ëª¨ë“ˆ"""
import torch
import logging
from typing import Dict, List, Any

logger = logging.getLogger(__name__)

class SimpleBertFilter:
    """BERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ì‹œ ì‚¬ìš©í•  ê°„ë‹¨í•œ í•„í„°"""
    
    def __init__(self):
        logger.info("ğŸ“¦ Simple BERT Filter ì´ˆê¸°í™”")
        self.noise_keywords = [
            "ìŒ", "ì–´", "ê·¸", "ì €ê¸°", "ì•„", "ë„¤", "ì˜ˆ", 
            "í•˜í•˜", "ã…‹ã…‹", "ã…ã…", "ìŒìŒ", "ì–´ì–´",
            "ê·¸ë˜ì„œ", "ê·¸ë‹ˆê¹Œ", "ê·¸ëŸ¬ë‹ˆê¹Œ", "ë­ì§€"
        ]
        
    def filter_text(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ í•„í„°ë§"""
        
        sentences = text.split('.')
        filtered_sentences = []
        noise_count = 0
        
        for sent in sentences:
            sent = sent.strip()
            if not sent or len(sent) < 5:
                noise_count += 1
                continue
                
            # ë…¸ì´ì¦ˆ í‚¤ì›Œë“œ ì²´í¬
            is_noise = False
            for keyword in self.noise_keywords:
                if sent.startswith(keyword) or sent.count(keyword) > 2:
                    is_noise = True
                    noise_count += 1
                    break
            
            if not is_noise:
                filtered_sentences.append(sent)
        
        filtered_text = '. '.join(filtered_sentences)
        
        return {
            "success": True,
            "filtered_transcript": filtered_text,
            "triplet_data": {
                "noise_segments": [],
                "valid_segments": filtered_sentences
            },
            "processing_stats": {
                "total_sentences": len(sentences),
                "filtered_sentences": len(filtered_sentences),
                "noise_count": noise_count,
                "filtering_ratio": noise_count / len(sentences) if sentences else 0
            }
        }

class SimpleTripletProcessor:
    """Triplet Processor ëŒ€ì²´"""
    
    def __init__(self):
        logger.info("ğŸ”§ Simple Triplet í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”")
        self.bert_filter = SimpleBertFilter()
        
    def process_whisperx_result(self, whisperx_result, enable_bert_filtering=True, save_noise_log=False):
        """WhisperX ê²°ê³¼ ì²˜ë¦¬"""
        
        try:
            full_text = whisperx_result.get("full_text", "")
            
            if enable_bert_filtering:
                return self.bert_filter.filter_text(full_text)
            else:
                return {
                    "success": True,
                    "filtered_transcript": full_text,
                    "triplet_data": {},
                    "processing_stats": {}
                }
                
        except Exception as e:
            logger.error(f"Simple Triplet ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "filtered_transcript": whisperx_result.get("full_text", ""),
                "error": str(e)
            }

class SimpleBertClassifier:
    """BERT Classifier ëŒ€ì²´"""
    
    def __init__(self):
        logger.info("ğŸ§  Simple BERT ë¶„ë¥˜ê¸° ì´ˆê¸°í™”")
        
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ (ë”ë¯¸)"""
        logger.info("âœ… Simple BERT ë¶„ë¥˜ê¸° ì¤€ë¹„ ì™„ë£Œ")
        return True
EOF
fi

# íŒŒì¼ í™•ì¸
echo ""
echo "ğŸ“‹ íŒŒì¼ í™•ì¸..."
echo "-------------------"

# BERT ëª¨ë¸ ë””ë ‰í† ë¦¬ íŒŒì¼ë“¤
echo "ğŸ“ /workspace/SKN12-FINAL-3TEAM/Bertëª¨ë¸/Ttalkkak_model_v2/:"
ls -la /workspace/SKN12-FINAL-3TEAM/Bertëª¨ë¸/Ttalkkak_model_v2/ | grep -E "(config|tokenizer|\.pt)"

echo ""
echo "ğŸ“ /workspace/SKN12-FINAL-3TEAM/ai-engine-dev/:"
ls -la | grep -E "(bert|simple|process)"

echo ""
echo "========================================="
echo "âœ… ì„¤ì • ì™„ë£Œ!"
echo "========================================="
echo ""
echo "ì‹¤í–‰:"
echo "  python process_file_standalone.py test.MP3"
echo ""