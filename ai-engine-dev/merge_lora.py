#!/usr/bin/env python3
"""
LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•©í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
VLLMì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•„ìš”
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
import os

# ê²½ë¡œ ì„¤ì •
base_model_name = "Qwen/Qwen3-4B"
lora_path = "/workspace/SKN12-FINAL-3TEAM/ai-engine-dev/qwen3_lora_ttalkkac_4b"
merged_path = "/workspace/SKN12-FINAL-3TEAM/ai-engine-dev/qwen3-4b-merged"

print("=" * 50)
print("ğŸ”„ LoRA ì–´ëŒ‘í„° ë³‘í•© ì‹œì‘")
print("=" * 50)

# LoRA ì–´ëŒ‘í„° íŒŒì¼ í™•ì¸
if not os.path.exists(lora_path):
    print(f"âŒ LoRA ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lora_path}")
    exit(1)

if not os.path.exists(f"{lora_path}/adapter_config.json"):
    print(f"âŒ adapter_config.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lora_path}/adapter_config.json")
    exit(1)

print(f"âœ… LoRA ê²½ë¡œ í™•ì¸: {lora_path}")
print(f"ğŸ“¦ ë² ì´ìŠ¤ ëª¨ë¸: {base_model_name}")
print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {merged_path}")
print()

# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
print("1ï¸âƒ£ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì¤‘... (ì•½ 1-2ë¶„ ì†Œìš”)")
try:
    # accelerateê°€ ìˆëŠ”ì§€ í™•ì¸
    try:
        import accelerate
        print("âœ… accelerate ë°œê²¬, device_map ì‚¬ìš©")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            trust_remote_code=True,
            device_map="auto"
        )
    except ImportError:
        print("âš ï¸ accelerate ì—†ìŒ, ê¸°ë³¸ ë¡œë“œ")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        if torch.cuda.is_available():
            base_model = base_model.cuda()
    
    print("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    print(f"âŒ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit(1)

# LoRA ì–´ëŒ‘í„° ì ìš©
print("2ï¸âƒ£ LoRA ì–´ëŒ‘í„° ì ìš© ì¤‘...")
try:
    model_with_lora = PeftModel.from_pretrained(base_model, lora_path)
    print("âœ… LoRA ì–´ëŒ‘í„° ì ìš© ì™„ë£Œ")
except Exception as e:
    print(f"âŒ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    print(f"   ì–´ëŒ‘í„° íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ í˜¸í™˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    exit(1)

# ë³‘í•©
print("3ï¸âƒ£ ëª¨ë¸ ë³‘í•© ì¤‘...")
try:
    merged_model = model_with_lora.merge_and_unload()
    print("âœ… ëª¨ë¸ ë³‘í•© ì™„ë£Œ")
except Exception as e:
    print(f"âŒ ë³‘í•© ì‹¤íŒ¨: {e}")
    exit(1)

# ë³‘í•©ëœ ëª¨ë¸ ì €ì¥
print("4ï¸âƒ£ ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì¤‘... (ì•½ 1-2ë¶„ ì†Œìš”)")
try:
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(merged_path, exist_ok=True)
    
    # ëª¨ë¸ ì €ì¥
    merged_model.save_pretrained(merged_path)
    print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    
    # í† í¬ë‚˜ì´ì € ì €ì¥
    print("5ï¸âƒ£ í† í¬ë‚˜ì´ì € ì €ì¥ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    tokenizer.save_pretrained(merged_path)
    print("âœ… í† í¬ë‚˜ì´ì € ì €ì¥ ì™„ë£Œ")
    
except Exception as e:
    print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
    exit(1)

print()
print("=" * 50)
print("ğŸ‰ ë³‘í•© ì™„ë£Œ!")
print(f"ğŸ“ ë³‘í•©ëœ ëª¨ë¸ ìœ„ì¹˜: {merged_path}")
print("ğŸš€ ì´ì œ ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ë©´ VLLMì´ ë³‘í•©ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
print("   ì‹¤í–‰: python ai_server_final_with_triplets.py")
print("=" * 50)