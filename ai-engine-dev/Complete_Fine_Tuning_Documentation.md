# ğŸ“š Ttalkkac AI ì‹œìŠ¤í…œ ì™„ì „ ë¬¸ì„œí™”
## íŒŒì¸íŠœë‹ ë°ì´í„°, í”„ë¡¬í”„íŠ¸, í”„ë¡œì„¸ìŠ¤ ìƒì„¸ ê°€ì´ë“œ

---

## ğŸ“‘ ëª©ì°¨

1. [ì‹œìŠ¤í…œ ê°œìš”](#1-ì‹œìŠ¤í…œ-ê°œìš”)
2. [ë°ì´í„° êµ¬ì¡° ìƒì„¸](#2-ë°ì´í„°-êµ¬ì¡°-ìƒì„¸)
3. [í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§](#3-í”„ë¡¬í”„íŠ¸-ì—”ì§€ë‹ˆì–´ë§)
4. [íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤](#4-íŒŒì¸íŠœë‹-í”„ë¡œì„¸ìŠ¤)
5. [ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ìƒì„±](#5-ê³¨ë“œ-ìŠ¤íƒ ë‹¤ë“œ-ìƒì„±)
6. [í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ](#6-í’ˆì§ˆ-í‰ê°€-ì‹œìŠ¤í…œ)
7. [ì½”ë“œ êµ¬í˜„ ìƒì„¸](#7-ì½”ë“œ-êµ¬í˜„-ìƒì„¸)
8. [ì„±ëŠ¥ ë©”íŠ¸ë¦­](#8-ì„±ëŠ¥-ë©”íŠ¸ë¦­)

---

## 1. ì‹œìŠ¤í…œ ê°œìš”

### 1.1 í”„ë¡œì íŠ¸ ì •ì˜

**í”„ë¡œì íŠ¸ëª…**: Ttalkkac - ì§€ëŠ¥í˜• íšŒì˜ë¡ ë¶„ì„ AI ì‹œìŠ¤í…œ

**ëª©ì **: 
- ë¹„ì •í˜• íšŒì˜ ë…¹ìŒ/í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì„œë¡œ ìë™ ë³€í™˜
- í•œêµ­ì–´ ë¹„ì¦ˆë‹ˆìŠ¤ í™˜ê²½ì— íŠ¹í™”ëœ íšŒì˜ ë¶„ì„ ì œê³µ
- ì•¡ì…˜ ì•„ì´í…œ, ì˜ì‚¬ê²°ì •, í”„ë¡œì íŠ¸ ê³„íš ìë™ ìƒì„±

**ê¸°ìˆ  ìŠ¤íƒ**:
```yaml
Base Model: Qwen3-8B
Fine-tuning: QLoRA (4-bit Quantization)
Framework: PyTorch 2.0+, Transformers 4.37+
Memory Optimization: Flash Attention 2, BitsAndBytes
Language: Python 3.10+
GPU: NVIDIA RTX 4090 24GB / A100 40GB
```

### 1.2 ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph Input Processing
        A[Audio File] --> B[WhisperX]
        B --> C[Transcription]
        C --> D[BERT Noise Filter]
        D --> E[Clean Text]
    end
    
    subgraph Fine-tuning Pipeline
        E --> F[Text Chunking<br/>5000 chars]
        F --> G[Prompt Generation]
        G --> H[Qwen3 Template]
        H --> I[Training Data]
    end
    
    subgraph Model Training
        I --> J[QLoRA Fine-tuning<br/>4-bit Quantization]
        J --> K[Trained Model]
    end
    
    subgraph Output Generation
        K --> L[Notion Project]
        K --> M[Task Master PRD]
        K --> N[Meeting Analysis]
    end
```

---

## 2. ë°ì´í„° êµ¬ì¡° ìƒì„¸

### 2.1 ì…ë ¥ ë°ì´í„° ê³„ì¸µ êµ¬ì¡°

#### 2.1.1 ì›ì‹œ íšŒì˜ ë°ì´í„° (Raw Meeting Data)

**íŒŒì¼ëª…**: `05_final_result.json`
**ìœ„ì¹˜**: `batch_triplet_results/result_*/`

```json
{
  "meeting_id": "meeting_20250108_001",
  "duration": "01:30:45",
  "participants": ["ê¹€ê³¼ì¥", "ì´ë¶€ì¥", "ë°•ëŒ€ë¦¬", "ìµœì‚¬ì›"],
  "segments": [
    {
      "segment_id": "seg_001",
      "timestamp": "00:00:30",
      "end_time": "00:01:15",
      "speaker": "ê¹€ê³¼ì¥",
      "text": "ì˜¤ëŠ˜ íšŒì˜ ì£¼ì œëŠ” ì‹ ê·œ AI í”„ë¡œì íŠ¸ ê¸°íšì…ë‹ˆë‹¤.",
      "confidence": 0.95,
      "keywords": ["AI", "í”„ë¡œì íŠ¸", "ê¸°íš"],
      "sentiment": "neutral"
    },
    {
      "segment_id": "seg_002",
      "timestamp": "00:01:15",
      "end_time": "00:02:30",
      "speaker": "ì´ë¶€ì¥",
      "text": "ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼, AI ê¸°ë°˜ íšŒì˜ ë¶„ì„ íˆ´ì˜ ìˆ˜ìš”ê°€ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
      "confidence": 0.92,
      "keywords": ["ì‹œì¥ì¡°ì‚¬", "AI", "íšŒì˜ë¶„ì„", "ìˆ˜ìš”"],
      "sentiment": "positive"
    }
  ],
  "metadata": {
    "recording_quality": "high",
    "background_noise_level": 0.2,
    "language": "ko",
    "dialect": "standard",
    "technical_terms_count": 45,
    "business_terms_count": 32
  }
}
```

#### 2.1.2 í…ìŠ¤íŠ¸ ë³€í™˜ í›„ êµ¬ì¡°

```python
class MeetingTranscript:
    """íšŒì˜ë¡ í…ìŠ¤íŠ¸ êµ¬ì¡°"""
    
    def __init__(self):
        self.full_text: str = ""  # ì „ì²´ í…ìŠ¤íŠ¸
        self.segments: List[Segment] = []  # ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        self.metadata: Dict = {}  # ë©”íƒ€ë°ì´í„°
        
    class Segment:
        timestamp: str  # "00:01:30"
        speaker: str   # "ê¹€ê³¼ì¥"
        text: str      # "ë°œí™” ë‚´ìš©"
        duration: float # 45.5 (seconds)
```

**ë³€í™˜ ì˜ˆì‹œ**:
```text
[00:00:30] ê¹€ê³¼ì¥: ì˜¤ëŠ˜ íšŒì˜ ì£¼ì œëŠ” ì‹ ê·œ AI í”„ë¡œì íŠ¸ ê¸°íšì…ë‹ˆë‹¤.
[00:01:15] ì´ë¶€ì¥: ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼, AI ê¸°ë°˜ íšŒì˜ ë¶„ì„ íˆ´ì˜ ìˆ˜ìš”ê°€ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.
[00:02:30] ë°•ëŒ€ë¦¬: ê°œë°œ ì¼ì •ì€ 3ê°œì›”ë¡œ ì˜ˆìƒë˜ë©°, í•„ìš” ì¸ë ¥ì€ 5ëª…ì…ë‹ˆë‹¤.
[00:03:45] ê¹€ê³¼ì¥: ì˜ˆì‚°ì€ ì–´ëŠ ì •ë„ë¡œ ì±…ì •í•´ì•¼ í• ê¹Œìš”?
[00:04:00] ìµœì‚¬ì›: ì´ˆê¸° ê°œë°œ ë¹„ìš©ì€ ì•½ 5ì²œë§Œì›ìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.
```

#### 2.1.3 ì²­í‚¹ ë©”ì»¤ë‹ˆì¦˜

```python
def chunk_text(text: str, chunk_size: int = 5000, overlap: int = 512) -> List[str]:
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹
    
    Parameters:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸° (ë¬¸ì ë‹¨ìœ„)
        overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© í¬ê¸°
    
    Returns:
        List[str]: ì²­í‚¹ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            # ë§ˆì§€ë§‰ ì²­í¬
            chunk = text[start:]
        else:
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ëŠê¸°
            chunk = text[start:end]
            
            # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ ì°¾ê¸°
            last_period = chunk.rfind('.')
            last_newline = chunk.rfind('\n')
            last_question = chunk.rfind('?')
            last_exclamation = chunk.rfind('!')
            
            # ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì¥ ì¢…ê²° ì§€ì  ì„ íƒ
            break_points = [p for p in [last_period, last_newline, last_question, last_exclamation] if p > chunk_size // 2]
            
            if break_points:
                break_point = max(break_points)
                chunk = text[start:start + break_point + 1]
                end = start + break_point + 1
        
        chunks.append(chunk.strip())
        
        if end >= len(text):
            break
            
        # ì˜¤ë²„ë© ì ìš©
        start = end - overlap
    
    return chunks
```

**ì²­í‚¹ ê²°ê³¼ ì˜ˆì‹œ**:
```json
{
  "original_length": 15000,
  "chunks": [
    {
      "chunk_id": 1,
      "start_pos": 0,
      "end_pos": 4988,
      "length": 4988,
      "first_timestamp": "00:00:00",
      "last_timestamp": "00:25:30",
      "text": "[00:00:00] ê¹€ê³¼ì¥: íšŒì˜ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤..."
    },
    {
      "chunk_id": 2,
      "start_pos": 4476,  // 512ì ì˜¤ë²„ë©
      "end_pos": 9964,
      "length": 5488,
      "first_timestamp": "00:24:00",  // ì˜¤ë²„ë© êµ¬ê°„
      "last_timestamp": "00:50:00",
      "text": "[00:24:00] ì´ë¶€ì¥: ì´ì–´ì„œ ë…¼ì˜í•˜ë©´..."
    },
    {
      "chunk_id": 3,
      "start_pos": 9452,
      "end_pos": 15000,
      "length": 5548,
      "first_timestamp": "00:48:30",
      "last_timestamp": "01:30:45",
      "text": "[00:48:30] ë°•ëŒ€ë¦¬: ë§ˆì§€ë§‰ìœ¼ë¡œ ì •ë¦¬í•˜ë©´..."
    }
  ],
  "overlap_size": 512,
  "chunk_count": 3
}
```

### 2.2 ì¶œë ¥ ë°ì´í„° êµ¬ì¡°

#### 2.2.1 ë…¸ì…˜ í”„ë¡œì íŠ¸ ê¸°íšì•ˆ (Notion Project)

```json
{
  "project_name": "AI ê¸°ë°˜ ì‹¤ì‹œê°„ íšŒì˜ë¡ ë¶„ì„ ì‹œìŠ¤í…œ ê°œë°œ",
  "project_purpose": "íšŒì˜ ë‚´ìš©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ë¬¸ì„œë¡œ ìë™ ë³€í™˜, ì—…ë¬´ íš¨ìœ¨ì„± ê·¹ëŒ€í™”",
  "project_period": "2025.02.01 ~ 2025.07.31 (6ê°œì›”)",
  "project_manager": "ê¹€ê³¼ì¥ (AIê°œë°œíŒ€)",
  "project_team": {
    "core_members": [
      {"name": "ì´ë¶€ì¥", "role": "í”„ë¡œì íŠ¸ ì´ê´„", "responsibility": "ì „ëµ ìˆ˜ë¦½ ë° ì˜ì‚¬ê²°ì •"},
      {"name": "ë°•ëŒ€ë¦¬", "role": "ê°œë°œ ë¦¬ë“œ", "responsibility": "ê¸°ìˆ  êµ¬í˜„ ë° í’ˆì§ˆ ê´€ë¦¬"},
      {"name": "ìµœì‚¬ì›", "role": "ë°ì´í„° ì—”ì§€ë‹ˆì–´", "responsibility": "ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"}
    ],
    "support_members": [
      {"team": "ë””ìì¸íŒ€", "count": 2, "role": "UI/UX ì„¤ê³„"},
      {"team": "QAíŒ€", "count": 1, "role": "í’ˆì§ˆ í…ŒìŠ¤íŠ¸"}
    ]
  },
  "core_objectives": [
    {
      "objective_id": "OBJ-001",
      "title": "ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ë° ì „ì‚¬ ì‹œìŠ¤í…œ êµ¬ì¶•",
      "description": "WhisperX ê¸°ë°˜ ê³ ì •í™•ë„ í•œêµ­ì–´ ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ ê°œë°œ",
      "key_results": [
        "ìŒì„± ì¸ì‹ ì •í™•ë„ 95% ì´ìƒ ë‹¬ì„±",
        "ì‹¤ì‹œê°„ ì²˜ë¦¬ ì§€ì—° ì‹œê°„ 3ì´ˆ ì´ë‚´",
        "ë‹¤ì¤‘ í™”ì êµ¬ë¶„ ì •í™•ë„ 90% ì´ìƒ"
      ],
      "deadline": "2025.03.31",
      "priority": "HIGH"
    },
    {
      "objective_id": "OBJ-002",
      "title": "AI ê¸°ë°˜ íšŒì˜ ë‚´ìš© êµ¬ì¡°í™” ì—”ì§„ ê°œë°œ",
      "description": "Qwen3 ëª¨ë¸ íŒŒì¸íŠœë‹ì„ í†µí•œ íšŒì˜ë¡ ìë™ ë¶„ì„ ë° êµ¬ì¡°í™”",
      "key_results": [
        "í•µì‹¬ ì£¼ì œ ì¶”ì¶œ ì •í™•ë„ 90% ì´ìƒ",
        "ì•¡ì…˜ ì•„ì´í…œ ìë™ ìƒì„±ë¥  85% ì´ìƒ",
        "ì˜ì‚¬ê²°ì • ì‚¬í•­ ë¶„ë¥˜ ì •í™•ë„ 88% ì´ìƒ"
      ],
      "deadline": "2025.05.31",
      "priority": "HIGH"
    },
    {
      "objective_id": "OBJ-003",
      "title": "ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ë° ê²½í—˜ ìµœì í™”",
      "description": "ì§ê´€ì ì´ê³  íš¨ìœ¨ì ì¸ íšŒì˜ë¡ ê´€ë¦¬ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•",
      "key_results": [
        "ì‚¬ìš©ì ë§Œì¡±ë„ 4.5/5.0 ì´ìƒ",
        "í‰ê·  ì‘ì—… ì™„ë£Œ ì‹œê°„ 50% ë‹¨ì¶•",
        "ì¼ì¼ í™œì„± ì‚¬ìš©ì 500ëª… ì´ìƒ"
      ],
      "deadline": "2025.06.30",
      "priority": "MEDIUM"
    }
  ],
  "core_idea": "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ê³¼ ìŒì„± ì¸ì‹ ê¸°ìˆ ì„ ê²°í•©í•œ ì°¨ì„¸ëŒ€ íšŒì˜ ìƒì‚°ì„± ë„êµ¬",
  "idea_description": "ìµœì‹  AI ê¸°ìˆ ì¸ WhisperX(ìŒì„±ì¸ì‹)ì™€ Qwen3(ìì—°ì–´ì²˜ë¦¬)ë¥¼ í†µí•©í•˜ì—¬, íšŒì˜ ì¤‘ ë°œìƒí•˜ëŠ” ëª¨ë“  ëŒ€í™”ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„í•˜ê³  êµ¬ì¡°í™”í•©ë‹ˆë‹¤. ë‹¨ìˆœí•œ ì „ì‚¬ë¥¼ ë„˜ì–´ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ, ì•¡ì…˜ ì•„ì´í…œ ìƒì„±, ì˜ì‚¬ê²°ì • ì¶”ì , ì°¸ì—¬ë„ ë¶„ì„ ë“±ì„ ìë™í™”í•˜ì—¬ íšŒì˜ í›„ follow-up ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ë‹¨ì¶•ì‹œí‚µë‹ˆë‹¤.",
  "technical_stack": {
    "backend": ["Python 3.10+", "FastAPI", "PyTorch", "Transformers"],
    "frontend": ["React 18", "TypeScript", "Material-UI"],
    "database": ["PostgreSQL", "Redis", "Elasticsearch"],
    "ai_models": ["WhisperX-large-v3", "Qwen3-8B-finetuned", "BERT-korean"],
    "infrastructure": ["Docker", "Kubernetes", "AWS/GCP"],
    "monitoring": ["Prometheus", "Grafana", "Sentry"]
  },
  "execution_plan": {
    "phase_1": {
      "name": "ê¸°ë°˜ êµ¬ì¶• ë‹¨ê³„",
      "period": "2025.02.01 ~ 2025.03.15",
      "tasks": [
        "ê°œë°œ í™˜ê²½ ì„¸íŒ… ë° ì¸í”„ë¼ êµ¬ì¶•",
        "WhisperX ëª¨ë¸ í•œêµ­ì–´ ìµœì í™”",
        "ê¸°ì´ˆ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•",
        "í”„ë¡œì íŠ¸ í‚¥ì˜¤í”„ ë° íŒ€ ì˜¨ë³´ë”©"
      ],
      "deliverables": ["ê°œë°œ í™˜ê²½", "ë°ì´í„° íŒŒì´í”„ë¼ì¸", "í”„ë¡œì íŠ¸ ê³„íšì„œ"],
      "milestone": "ìŒì„± ì¸ì‹ POC ì™„ì„±"
    },
    "phase_2": {
      "name": "í•µì‹¬ ê¸°ëŠ¥ ê°œë°œ",
      "period": "2025.03.16 ~ 2025.05.15",
      "tasks": [
        "Qwen3 ëª¨ë¸ íŒŒì¸íŠœë‹ (ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ë°ì´í„° í™œìš©)",
        "ì‹¤ì‹œê°„ ì „ì‚¬ ì‹œìŠ¤í…œ êµ¬í˜„",
        "íšŒì˜ë¡ êµ¬ì¡°í™” ì—”ì§„ ê°œë°œ",
        "REST API ì„¤ê³„ ë° êµ¬í˜„"
      ],
      "deliverables": ["íŒŒì¸íŠœë‹ëœ ëª¨ë¸", "API ì„œë²„", "í•µì‹¬ ê¸°ëŠ¥ ëª¨ë“ˆ"],
      "milestone": "MVP ë²„ì „ ì™„ì„±"
    },
    "phase_3": {
      "name": "í†µí•© ë° ìµœì í™”",
      "period": "2025.05.16 ~ 2025.06.30",
      "tasks": [
        "í”„ë¡ íŠ¸ì—”ë“œ ëŒ€ì‹œë³´ë“œ ê°œë°œ",
        "ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸",
        "ì„±ëŠ¥ ìµœì í™” ë° ìŠ¤ì¼€ì¼ë§",
        "ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° ë°˜ì˜"
      ],
      "deliverables": ["í†µí•© ì‹œìŠ¤í…œ", "ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤", "í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ"],
      "milestone": "ë² íƒ€ ë²„ì „ ì¶œì‹œ"
    },
    "phase_4": {
      "name": "ì¶œì‹œ ë° ì•ˆì •í™”",
      "period": "2025.07.01 ~ 2025.07.31",
      "tasks": [
        "í”„ë¡œë•ì…˜ ë°°í¬",
        "ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•",
        "ì‚¬ìš©ì êµìœ¡ ë° ë¬¸ì„œí™”",
        "ì¶œì‹œ í›„ ì´ìŠˆ ëŒ€ì‘"
      ],
      "deliverables": ["ì •ì‹ ì„œë¹„ìŠ¤", "ì‚¬ìš©ì ë§¤ë‰´ì–¼", "ìš´ì˜ ê°€ì´ë“œ"],
      "milestone": "ì •ì‹ ì„œë¹„ìŠ¤ ëŸ°ì¹­"
    }
  },
  "expected_effects": [
    {
      "category": "ì •ëŸ‰ì  íš¨ê³¼",
      "items": [
        "íšŒì˜ë¡ ì‘ì„± ì‹œê°„ 80% ì ˆê° (í‰ê·  2ì‹œê°„ â†’ 24ë¶„)",
        "ì•¡ì…˜ ì•„ì´í…œ ëˆ„ë½ë¥  95% ê°ì†Œ",
        "íšŒì˜ í›„ì† ì¡°ì¹˜ ì™„ë£Œìœ¨ 40% í–¥ìƒ",
        "ì›”ê°„ íšŒì˜ ì‹œê°„ 20% ë‹¨ì¶• (ë¶ˆí•„ìš”í•œ ë°˜ë³µ ë…¼ì˜ ì œê±°)"
      ]
    },
    {
      "category": "ì •ì„±ì  íš¨ê³¼",
      "items": [
        "ì˜ì‚¬ê²°ì • íˆ¬ëª…ì„± ë° ì¶”ì ê°€ëŠ¥ì„± í™•ë³´",
        "íŒ€ ê°„ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í’ˆì§ˆ í–¥ìƒ",
        "íšŒì˜ ì°¸ì—¬ë„ ë° ì§‘ì¤‘ë„ ì¦ê°€",
        "ì¡°ì§ ì§€ì‹ ìì‚°í™” ë° ì¶•ì "
      ]
    },
    {
      "category": "ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸",
      "items": [
        "ì—°ê°„ ì¸ê±´ë¹„ ì ˆê° íš¨ê³¼ ì•½ 2ì–µì›",
        "í”„ë¡œì íŠ¸ ë¦¬ë“œíƒ€ì„ 15% ë‹¨ì¶•",
        "ê³ ê° ì‘ëŒ€ ì†ë„ 30% ê°œì„ ",
        "ì§ì› ë§Œì¡±ë„ 25% ìƒìŠ¹"
      ]
    }
  ],
  "risk_management": [
    {
      "risk": "ê°œì¸ì •ë³´ ë° ê¸°ë°€ì •ë³´ ìœ ì¶œ",
      "probability": "MEDIUM",
      "impact": "HIGH",
      "mitigation": "ì—”ë“œíˆ¬ì—”ë“œ ì•”í˜¸í™”, ì ‘ê·¼ ê¶Œí•œ ê´€ë¦¬, ë°ì´í„° ë§ˆìŠ¤í‚¹"
    },
    {
      "risk": "ìŒì„± ì¸ì‹ ì •í™•ë„ ë¯¸ë‹¬",
      "probability": "LOW",
      "impact": "HIGH",
      "mitigation": "ì§€ì†ì ì¸ ëª¨ë¸ ê°œì„ , ìˆ˜ë™ ë³´ì • ê¸°ëŠ¥ ì œê³µ"
    },
    {
      "risk": "ì‚¬ìš©ì ì±„íƒë¥  ì €ì¡°",
      "probability": "MEDIUM",
      "impact": "MEDIUM",
      "mitigation": "ë‹¨ê³„ì  ë¡¤ì•„ì›ƒ, ì¶©ë¶„í•œ êµìœ¡, ì¸ì„¼í‹°ë¸Œ ì œê³µ"
    }
  ],
  "budget_estimation": {
    "development_cost": "1.5ì–µì›",
    "infrastructure_cost": "3ì²œë§Œì›",
    "operation_cost_monthly": "500ë§Œì›",
    "total_project_cost": "2ì–µì›",
    "roi_expectation": "íˆ¬ì ëŒ€ë¹„ 18ê°œì›” ë‚´ íšŒìˆ˜"
  },
  "success_criteria": [
    "ì¼ì¼ í™œì„± ì‚¬ìš©ì 500ëª… ì´ìƒ",
    "ì‹œìŠ¤í…œ ê°€ìš©ì„± 99.9% ì´ìƒ",
    "ì‚¬ìš©ì ë§Œì¡±ë„ 4.5/5.0 ì´ìƒ",
    "ìŒì„± ì¸ì‹ ì •í™•ë„ 95% ì´ìƒ",
    "íšŒì˜ë¡ ìƒì„± ì‹œê°„ 5ë¶„ ì´ë‚´"
  ]
}
```

#### 2.2.2 Task Master PRD (Product Requirements Document)

```json
{
  "prd_id": "PRD-2025-001",
  "version": "1.0.0",
  "created_date": "2025-01-08",
  "last_updated": "2025-01-08",
  "status": "DRAFT",
  "product_overview": {
    "product_name": "Ttalkkac Meeting Intelligence Platform",
    "product_version": "1.0",
    "product_category": "Enterprise Productivity Tool",
    "target_market": "B2B SaaS",
    "value_proposition": "Transform meetings into actionable insights with AI-powered real-time analysis"
  },
  "problem_statement": {
    "current_situation": "ê¸°ì—…ë“¤ì€ í‰ê· ì ìœ¼ë¡œ ì§ì› ê·¼ë¬´ì‹œê°„ì˜ 23%ë¥¼ íšŒì˜ì— ì†Œë¹„í•˜ì§€ë§Œ, íšŒì˜ ë‚´ìš©ì˜ 70%ëŠ” ì œëŒ€ë¡œ ê¸°ë¡ë˜ì§€ ì•Šê³  ìœ ì‹¤ë©ë‹ˆë‹¤.",
    "pain_points": [
      "ìˆ˜ë™ íšŒì˜ë¡ ì‘ì„±ì— í‰ê·  2ì‹œê°„ ì†Œìš”",
      "ì¤‘ìš”í•œ ì˜ì‚¬ê²°ì • ì‚¬í•­ ëˆ„ë½ ë¹ˆë²ˆ",
      "ì•¡ì…˜ ì•„ì´í…œ ì¶”ì  ê´€ë¦¬ ì–´ë ¤ì›€",
      "íšŒì˜ íš¨ìœ¨ì„± ì¸¡ì • ë¶ˆê°€ëŠ¥"
    ],
    "opportunity": "AI ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ íšŒì˜ ìƒì‚°ì„±ì„ íšê¸°ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆëŠ” ì‹œì¥ ê¸°íšŒ",
    "market_size": "ê¸€ë¡œë²Œ íšŒì˜ ì†”ë£¨ì…˜ ì‹œì¥ ê·œëª¨ 150ì–µ ë‹¬ëŸ¬ (2025ë…„ ê¸°ì¤€)"
  },
  "solution_design": {
    "core_features": [
      {
        "feature_id": "F001",
        "feature_name": "ì‹¤ì‹œê°„ ìŒì„± ì „ì‚¬",
        "description": "íšŒì˜ ì¤‘ ëª¨ë“  ëŒ€í™”ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜",
        "priority": "P0",
        "complexity": "HIGH",
        "technical_requirements": [
          "WhisperX ëª¨ë¸ í†µí•©",
          "ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë””ì˜¤ ì²˜ë¦¬",
          "ë‹¤ì¤‘ í™”ì êµ¬ë¶„",
          "ë…¸ì´ì¦ˆ í•„í„°ë§"
        ],
        "acceptance_criteria": [
          "ì „ì‚¬ ì •í™•ë„ 95% ì´ìƒ",
          "ì‹¤ì‹œê°„ ì§€ì—° 3ì´ˆ ì´ë‚´",
          "10ëª… ì´ìƒ í™”ì êµ¬ë¶„ ê°€ëŠ¥"
        ]
      },
      {
        "feature_id": "F002",
        "feature_name": "AI ê¸°ë°˜ ë‚´ìš© êµ¬ì¡°í™”",
        "description": "íšŒì˜ ë‚´ìš©ì„ ìë™ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ë¬¸ì„œë¡œ ë³€í™˜",
        "priority": "P0",
        "complexity": "HIGH",
        "technical_requirements": [
          "Qwen3 ëª¨ë¸ íŒŒì¸íŠœë‹",
          "ìì—°ì–´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸",
          "JSON êµ¬ì¡°í™” ì—”ì§„",
          "ì»¨í…ìŠ¤íŠ¸ ì´í•´ ì•Œê³ ë¦¬ì¦˜"
        ],
        "acceptance_criteria": [
          "í•µì‹¬ ì£¼ì œ ì¶”ì¶œ ì •í™•ë„ 90%",
          "ì•¡ì…˜ ì•„ì´í…œ ì¸ì‹ë¥  85%",
          "êµ¬ì¡°í™” ì²˜ë¦¬ ì‹œê°„ 30ì´ˆ ì´ë‚´"
        ]
      },
      {
        "feature_id": "F003",
        "feature_name": "ì•¡ì…˜ ì•„ì´í…œ ì¶”ì ",
        "description": "íšŒì˜ì—ì„œ ë„ì¶œëœ ì•¡ì…˜ ì•„ì´í…œì„ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ê³  ì¶”ì ",
        "priority": "P0",
        "complexity": "MEDIUM",
        "technical_requirements": [
          "íƒœìŠ¤í¬ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜",
          "ë‹´ë‹¹ì ìë™ ë§¤ì¹­",
          "ë§ˆê°ì¼ ì¶”ë¡  ë¡œì§",
          "ì§„í–‰ìƒí™© ì¶”ì  ì‹œìŠ¤í…œ"
        ],
        "acceptance_criteria": [
          "ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ ì •í™•ë„ 85%",
          "ë‹´ë‹¹ì ë§¤ì¹­ ì •í™•ë„ 80%",
          "ìë™ ë¦¬ë§ˆì¸ë” ë°œì†¡"
        ]
      },
      {
        "feature_id": "F004",
        "feature_name": "íšŒì˜ ëŒ€ì‹œë³´ë“œ",
        "description": "íšŒì˜ í†µê³„, ì¸ì‚¬ì´íŠ¸, íŠ¸ë Œë“œë¥¼ ì‹œê°í™”í•˜ëŠ” ëŒ€ì‹œë³´ë“œ",
        "priority": "P1",
        "complexity": "MEDIUM",
        "technical_requirements": [
          "React ê¸°ë°˜ í”„ë¡ íŠ¸ì—”ë“œ",
          "ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸",
          "ì°¨íŠ¸ ë° ê·¸ë˜í”„ ë¼ì´ë¸ŒëŸ¬ë¦¬",
          "ë°˜ì‘í˜• ë””ìì¸"
        ],
        "acceptance_criteria": [
          "í˜ì´ì§€ ë¡œë”© ì‹œê°„ 2ì´ˆ ì´ë‚´",
          "ëª¨ë°”ì¼ í˜¸í™˜ì„± 100%",
          "ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì§€ì›"
        ]
      },
      {
        "feature_id": "F005",
        "feature_name": "í†µí•© ë° ì—°ë™",
        "description": "ê¸°ì¡´ ì—…ë¬´ ë„êµ¬ë“¤ê³¼ì˜ ì›í™œí•œ ì—°ë™",
        "priority": "P1",
        "complexity": "MEDIUM",
        "technical_requirements": [
          "REST API ì œê³µ",
          "Webhook ì§€ì›",
          "OAuth 2.0 ì¸ì¦",
          "ì£¼ìš” í”Œë«í¼ í”ŒëŸ¬ê·¸ì¸"
        ],
        "acceptance_criteria": [
          "Slack, Teams ì—°ë™",
          "Google Calendar ë™ê¸°í™”",
          "Notion, Jira ë‚´ë³´ë‚´ê¸°"
        ]
      }
    ],
    "user_stories": [
      {
        "story_id": "US001",
        "persona": "í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €",
        "story": "As a í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €, I want to íšŒì˜ ë‚´ìš©ì„ ìë™ìœ¼ë¡œ ë¬¸ì„œí™” so that íŒ€ì›ë“¤ê³¼ ì‰½ê²Œ ê³µìœ í•  ìˆ˜ ìˆë‹¤",
        "acceptance_criteria": [
          "íšŒì˜ ì¢…ë£Œ í›„ 5ë¶„ ì´ë‚´ ë¬¸ì„œ ìƒì„±",
          "ëª¨ë“  ì°¸ì„ìì—ê²Œ ìë™ ê³µìœ ",
          "í¸ì§‘ ê°€ëŠ¥í•œ í˜•ì‹ ì œê³µ"
        ]
      },
      {
        "story_id": "US002",
        "persona": "íŒ€ ë¦¬ë”",
        "story": "As a íŒ€ ë¦¬ë”, I want to ì•¡ì…˜ ì•„ì´í…œì„ ìë™ìœ¼ë¡œ ì¶”ì  so that íŒ€ì˜ ì‹¤í–‰ë ¥ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤",
        "acceptance_criteria": [
          "ì•¡ì…˜ ì•„ì´í…œ ìë™ ì¶”ì¶œ",
          "ë‹´ë‹¹ì ìë™ í• ë‹¹",
          "ì§„í–‰ìƒí™© ì‹¤ì‹œê°„ ì¶”ì "
        ]
      },
      {
        "story_id": "US003",
        "persona": "ê²½ì˜ì§„",
        "story": "As a ê²½ì˜ì§„, I want to íšŒì˜ íš¨ìœ¨ì„±ì„ ì¸¡ì • so that ì¡°ì§ì˜ ìƒì‚°ì„±ì„ ê°œì„ í•  ìˆ˜ ìˆë‹¤",
        "acceptance_criteria": [
          "íšŒì˜ í†µê³„ ëŒ€ì‹œë³´ë“œ ì œê³µ",
          "ROI ì¸¡ì • ì§€í‘œ ì œì‹œ",
          "ê°œì„  ì œì•ˆ ìë™ ìƒì„±"
        ]
      }
    ],
    "technical_architecture": {
      "system_design": "Microservices Architecture",
      "components": [
        {
          "name": "Audio Processing Service",
          "technology": "Python, WhisperX",
          "responsibility": "ìŒì„± ì¸ì‹ ë° ì „ì‚¬"
        },
        {
          "name": "NLP Engine",
          "technology": "Python, Qwen3, Transformers",
          "responsibility": "ìì—°ì–´ ì²˜ë¦¬ ë° êµ¬ì¡°í™”"
        },
        {
          "name": "API Gateway",
          "technology": "FastAPI, Kong",
          "responsibility": "API ë¼ìš°íŒ… ë° ì¸ì¦"
        },
        {
          "name": "Web Application",
          "technology": "React, TypeScript",
          "responsibility": "ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤"
        },
        {
          "name": "Data Storage",
          "technology": "PostgreSQL, Redis, S3",
          "responsibility": "ë°ì´í„° ì €ì¥ ë° ìºì‹±"
        }
      ],
      "scalability_plan": {
        "initial_capacity": "ë™ì‹œ ì‚¬ìš©ì 1,000ëª…",
        "scaling_strategy": "Horizontal scaling with Kubernetes",
        "target_capacity": "ë™ì‹œ ì‚¬ìš©ì 10,000ëª… (1ë…„ ë‚´)"
      },
      "security_requirements": [
        "End-to-end encryption",
        "GDPR/CCPA compliance",
        "SOC 2 Type II certification",
        "Regular security audits"
      ]
    }
  },
  "implementation_roadmap": [
    {
      "phase": "Alpha",
      "timeline": "Month 1-2",
      "goals": ["Core functionality", "Internal testing"],
      "success_metrics": ["Feature completeness 80%", "Bug severity < P2"]
    },
    {
      "phase": "Beta",
      "timeline": "Month 3-4",
      "goals": ["Limited release", "User feedback collection"],
      "success_metrics": ["100 beta users", "NPS > 7"]
    },
    {
      "phase": "GA",
      "timeline": "Month 5-6",
      "goals": ["Public launch", "Marketing campaign"],
      "success_metrics": ["1000 users", "MRR $50K"]
    }
  ],
  "success_metrics": {
    "business_metrics": [
      {"metric": "Monthly Recurring Revenue", "target": "$100K", "timeline": "6 months"},
      {"metric": "Customer Acquisition Cost", "target": "< $500", "timeline": "3 months"},
      {"metric": "Customer Lifetime Value", "target": "> $5000", "timeline": "12 months"},
      {"metric": "Churn Rate", "target": "< 5%", "timeline": "6 months"}
    ],
    "product_metrics": [
      {"metric": "Daily Active Users", "target": "1000", "timeline": "6 months"},
      {"metric": "Feature Adoption Rate", "target": "> 70%", "timeline": "3 months"},
      {"metric": "User Satisfaction Score", "target": "> 4.5/5", "timeline": "ongoing"},
      {"metric": "System Uptime", "target": "> 99.9%", "timeline": "ongoing"}
    ],
    "technical_metrics": [
      {"metric": "API Response Time", "target": "< 200ms", "timeline": "ongoing"},
      {"metric": "Transcription Accuracy", "target": "> 95%", "timeline": "3 months"},
      {"metric": "Processing Speed", "target": "Real-time", "timeline": "ongoing"},
      {"metric": "Error Rate", "target": "< 0.1%", "timeline": "ongoing"}
    ]
  },
  "resource_requirements": {
    "team_composition": [
      {"role": "Product Manager", "count": 1, "expertise": "B2B SaaS"},
      {"role": "Backend Engineers", "count": 3, "expertise": "Python, AI/ML"},
      {"role": "Frontend Engineers", "count": 2, "expertise": "React, TypeScript"},
      {"role": "ML Engineers", "count": 2, "expertise": "NLP, Model Training"},
      {"role": "DevOps Engineer", "count": 1, "expertise": "K8s, AWS"},
      {"role": "QA Engineer", "count": 1, "expertise": "Automation Testing"},
      {"role": "UI/UX Designer", "count": 1, "expertise": "Enterprise UX"}
    ],
    "budget_breakdown": {
      "development": "$1,500,000",
      "infrastructure": "$300,000",
      "marketing": "$200,000",
      "operations": "$500,000",
      "total": "$2,500,000"
    },
    "timeline": "6 months to MVP, 12 months to full launch"
  },
  "risks_and_mitigations": [
    {
      "risk": "ê¸°ìˆ ì  ë³µì¡ë„ë¡œ ì¸í•œ ê°œë°œ ì§€ì—°",
      "impact": "HIGH",
      "probability": "MEDIUM",
      "mitigation": "ë‹¨ê³„ì  ê°œë°œ, ì¶©ë¶„í•œ ë²„í¼ ì‹œê°„ í™•ë³´, ì™¸ë¶€ ì „ë¬¸ê°€ ì»¨ì„¤íŒ…"
    },
    {
      "risk": "ì‹œì¥ ê²½ìŸ ì‹¬í™”",
      "impact": "MEDIUM",
      "probability": "HIGH",
      "mitigation": "ì°¨ë³„í™”ëœ ê¸°ëŠ¥ ê°œë°œ, ë¹ ë¥¸ ì‹œì¥ ì§„ì…, íŠ¹í—ˆ ì¶œì›"
    },
    {
      "risk": "ë°ì´í„° ë³´ì•ˆ ì´ìŠˆ",
      "impact": "HIGH",
      "probability": "LOW",
      "mitigation": "ë³´ì•ˆ ì „ë¬¸ê°€ ì˜ì…, ì •ê¸° ë³´ì•ˆ ê°ì‚¬, ë³´í—˜ ê°€ì…"
    }
  ],
  "appendix": {
    "competitive_analysis": [
      {
        "competitor": "Otter.ai",
        "strengths": ["Market leader", "Strong brand"],
        "weaknesses": ["Limited Korean support", "Basic analytics"],
        "our_advantage": "Superior Korean NLP, Advanced analytics"
      },
      {
        "competitor": "Fireflies.ai",
        "strengths": ["Good integrations", "Affordable"],
        "weaknesses": ["Accuracy issues", "Limited customization"],
        "our_advantage": "Higher accuracy, Enterprise features"
      }
    ],
    "technical_specifications": {
      "supported_formats": ["MP3", "WAV", "M4A", "WebM"],
      "languages": ["Korean", "English", "Japanese", "Chinese"],
      "max_meeting_duration": "4 hours",
      "max_participants": "50",
      "api_rate_limit": "1000 requests/minute"
    },
    "glossary": {
      "NLP": "Natural Language Processing",
      "ASR": "Automatic Speech Recognition",
      "LLM": "Large Language Model",
      "STT": "Speech-to-Text",
      "API": "Application Programming Interface"
    }
  }
}
```

---

## 3. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

### 3.1 ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì²´ê³„

#### 3.1.1 ë§ˆìŠ¤í„° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸

```python
MASTER_SYSTEM_PROMPT = """
You are Ttalkkac, an advanced AI assistant specialized in meeting intelligence and analysis.

## Core Capabilities
1. **Meeting Transcription Analysis**: Process and understand Korean business meeting transcripts
2. **Structured Output Generation**: Convert unstructured conversations into organized documents
3. **Task Extraction**: Identify and prioritize action items from discussions
4. **Decision Tracking**: Capture and document key decisions and rationales
5. **Insight Generation**: Provide valuable insights and recommendations

## Operational Guidelines

### Language Processing
- Primary language: Korean (í•œêµ­ì–´)
- Support for: English technical terms, mixed language content
- Maintain formal business tone (ì¡´ëŒ“ë§) unless specified otherwise
- Understand implicit meanings common in Korean business culture

### Analysis Framework
1. **Content Understanding**
   - Identify main topics and subtopics
   - Recognize speaker intentions and concerns
   - Detect agreement, disagreement, and uncertainty
   - Extract quantitative data and commitments

2. **Structural Organization**
   - Chronological flow preservation
   - Logical grouping of related topics
   - Hierarchical information architecture
   - Clear separation of facts vs. opinions

3. **Quality Standards**
   - Accuracy: No fabrication or assumption beyond provided content
   - Completeness: Capture all significant points
   - Clarity: Use clear, concise language
   - Consistency: Maintain uniform formatting and style

## Output Requirements
- Always respond in valid JSON format
- Follow the exact schema provided in user prompts
- Include confidence levels for uncertain interpretations
- Provide reasoning for complex decisions when requested

## Ethical Considerations
- Maintain confidentiality of meeting content
- Avoid bias in summarization and analysis
- Preserve the original intent of speakers
- Flag potentially sensitive information

## Performance Metrics
You will be evaluated on:
1. Accuracy of information extraction (95%+ target)
2. Completeness of structured output (90%+ target)
3. Relevance of identified action items (85%+ target)
4. Quality of insights and recommendations (4.5/5 rating)

Remember: Your goal is to transform raw meeting conversations into actionable business intelligence.
"""
```

#### 3.1.2 íšŒì˜ ë¶„ì„ ì „ìš© í”„ë¡¬í”„íŠ¸

```python
def generate_meeting_analysis_system_prompt(
    context_type: str = "general",
    industry: str = "technology",
    formality_level: str = "formal"
) -> str:
    """
    ì»¨í…ìŠ¤íŠ¸ë³„ ë§ì¶¤í˜• ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    
    Args:
        context_type: íšŒì˜ ìœ í˜• (general, technical, strategic, operational)
        industry: ì‚°ì—… ë¶„ì•¼ (technology, finance, healthcare, manufacturing)
        formality_level: ê²©ì‹ ìˆ˜ì¤€ (formal, semi-formal, casual)
    """
    
    base_prompt = """
    You are an AI meeting analyst with expertise in {industry} industry meetings.
    
    ## Specialization: {context_type} Meetings
    
    ### Context-Specific Guidelines
    """
    
    context_guidelines = {
        "general": """
        - Focus on broad themes and general consensus
        - Extract high-level decisions and next steps
        - Identify key stakeholders and their positions
        """,
        "technical": """
        - Capture technical specifications and requirements
        - Document architectural decisions and trade-offs
        - Extract technical action items and dependencies
        - Preserve technical terminology and acronyms
        """,
        "strategic": """
        - Identify strategic objectives and KPIs
        - Extract long-term goals and vision statements
        - Document strategic risks and opportunities
        - Capture competitive analysis and market insights
        """,
        "operational": """
        - Focus on operational metrics and performance
        - Extract process improvements and optimizations
        - Document resource allocation and constraints
        - Identify bottlenecks and efficiency opportunities
        """
    }
    
    industry_knowledge = {
        "technology": """
        ### Industry Knowledge
        - Software development methodologies (Agile, Scrum, Kanban)
        - Technical stack terminology (Frontend, Backend, DevOps, Cloud)
        - Common metrics (Sprint velocity, Code coverage, Uptime)
        - Industry trends (AI/ML, Cloud native, Microservices)
        """,
        "finance": """
        ### Industry Knowledge
        - Financial terminology (ROI, EBITDA, P&L, Cash flow)
        - Regulatory requirements (SOX, Basel III, MiFID II)
        - Risk management concepts (VaR, Stress testing, Hedging)
        - Market dynamics (Volatility, Liquidity, Arbitrage)
        """,
        "healthcare": """
        ### Industry Knowledge
        - Medical terminology and procedures
        - Regulatory compliance (HIPAA, FDA, CE marking)
        - Clinical trial phases and protocols
        - Patient care standards and quality metrics
        """,
        "manufacturing": """
        ### Industry Knowledge
        - Production terminology (Lean, Six Sigma, JIT, Kaizen)
        - Quality standards (ISO 9001, TQM, SPC)
        - Supply chain concepts (Lead time, Inventory turns, OEE)
        - Safety regulations (OSHA, EHS, HAZMAT)
        """
    }
    
    formality_instructions = {
        "formal": """
        ### Communication Style
        - Use formal business Korean (ë†’ì„ë§)
        - Include honorifics and titles
        - Maintain professional distance
        - Structure responses formally
        """,
        "semi-formal": """
        ### Communication Style
        - Use polite but approachable language
        - Balance professionalism with friendliness
        - Include titles for senior positions
        """,
        "casual": """
        ### Communication Style
        - Use conversational tone
        - Focus on clarity over formality
        - Minimize jargon unless necessary
        """
    }
    
    return base_prompt.format(
        industry=industry,
        context_type=context_type.title()
    ) + context_guidelines.get(context_type, "") + \
    industry_knowledge.get(industry, "") + \
    formality_instructions.get(formality_level, "")
```

### 3.2 ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

#### 3.2.1 ê¸°ë³¸ íšŒì˜ ë¶„ì„ í”„ë¡¬í”„íŠ¸

```python
def generate_meeting_analysis_user_prompt(
    transcript: str,
    meeting_metadata: Dict[str, Any] = None,
    analysis_focus: List[str] = None,
    output_format: str = "full"
) -> str:
    """
    íšŒì˜ ë¶„ì„ì„ ìœ„í•œ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±
    
    Args:
        transcript: íšŒì˜ ë…¹ì·¨ë¡
        meeting_metadata: íšŒì˜ ë©”íƒ€ë°ì´í„° (ì°¸ì„ì, ì¼ì‹œ, ëª©ì  ë“±)
        analysis_focus: ì¤‘ì  ë¶„ì„ ì˜ì—­
        output_format: ì¶œë ¥ í˜•ì‹ (full, summary, action_items_only)
    """
    
    # ë©”íƒ€ë°ì´í„° ì„¹ì…˜ êµ¬ì„±
    metadata_section = ""
    if meeting_metadata:
        metadata_section = f"""
**Meeting Context**
- Date: {meeting_metadata.get('date', 'Unknown')}
- Duration: {meeting_metadata.get('duration', 'Unknown')}
- Participants: {', '.join(meeting_metadata.get('participants', ['Unknown']))}
- Purpose: {meeting_metadata.get('purpose', 'General discussion')}
- Meeting Type: {meeting_metadata.get('type', 'Regular')}
"""
    
    # ë¶„ì„ ì¤‘ì  ì„¹ì…˜
    focus_section = ""
    if analysis_focus:
        focus_section = f"""
**Analysis Focus Areas**
Please pay special attention to:
{chr(10).join(f'- {focus}' for focus in analysis_focus)}
"""
    
    # ì¶œë ¥ í˜•ì‹ë³„ ìŠ¤í‚¤ë§ˆ
    output_schemas = {
        "full": """
**Required Output Format (JSON)**
{
    "meeting_summary": {
        "title": "Meeting title based on content",
        "date": "Meeting date",
        "duration": "Meeting duration",
        "participants": ["List of participants"],
        "purpose": "Meeting purpose",
        "key_topics": ["Main topics discussed"]
    },
    "executive_summary": "2-3 paragraph executive summary",
    "detailed_discussions": [
        {
            "topic": "Discussion topic",
            "summary": "Topic summary",
            "key_points": ["Key points"],
            "speakers": ["Involved speakers"],
            "duration_estimate": "Approximate time spent",
            "decisions": ["Decisions made"],
            "action_items": ["Related action items"]
        }
    ],
    "decisions": [
        {
            "decision_id": "DEC-001",
            "description": "Decision description",
            "rationale": "Reasoning behind decision",
            "made_by": "Decision maker",
            "affected_parties": ["Affected teams/individuals"],
            "implementation_timeline": "When to implement",
            "risks": ["Potential risks"],
            "dependencies": ["Dependencies"]
        }
    ],
    "action_items": [
        {
            "task_id": "TASK-001",
            "description": "Task description",
            "assignee": "Responsible person",
            "due_date": "Deadline",
            "priority": "HIGH/MEDIUM/LOW",
            "dependencies": ["Dependent tasks"],
            "success_criteria": "How to measure completion",
            "resources_needed": ["Required resources"],
            "estimated_effort": "Time/effort estimate"
        }
    ],
    "follow_up_required": [
        {
            "item": "Follow-up item",
            "responsible": "Who should follow up",
            "by_when": "Timeline",
            "reason": "Why follow-up is needed"
        }
    ],
    "risks_and_concerns": [
        {
            "risk": "Risk description",
            "raised_by": "Who raised it",
            "severity": "HIGH/MEDIUM/LOW",
            "mitigation": "Proposed mitigation"
        }
    ],
    "next_steps": {
        "immediate": ["Actions for next 24-48 hours"],
        "short_term": ["Actions for next week"],
        "long_term": ["Actions for next month+"]
    },
    "metrics_and_kpis": [
        {
            "metric": "Metric name",
            "current_value": "Current state",
            "target_value": "Target state",
            "timeline": "Achievement timeline",
            "owner": "Responsible person"
        }
    ],
    "participant_analysis": [
        {
            "participant": "Name",
            "speaking_time_percentage": "Approximate %",
            "key_contributions": ["Main contributions"],
            "action_items_assigned": ["Assigned tasks"],
            "concerns_raised": ["Concerns mentioned"]
        }
    ],
    "metadata": {
        "total_speaking_turns": "Number",
        "average_turn_length": "Seconds",
        "topic_changes": "Number of topic shifts",
        "consensus_level": "HIGH/MEDIUM/LOW",
        "meeting_effectiveness": "1-10 rating",
        "follow_up_meeting_needed": "Boolean",
        "recommended_attendees_for_follow_up": ["Names"]
    }
}
""",
        "summary": """
**Required Output Format (Concise JSON)**
{
    "executive_summary": "1 paragraph summary",
    "key_decisions": ["Decision 1", "Decision 2"],
    "action_items": [
        {
            "task": "Task description",
            "assignee": "Person",
            "due_date": "Date"
        }
    ],
    "next_meeting": "Date/time if mentioned"
}
""",
        "action_items_only": """
**Required Output Format (Action Items JSON)**
{
    "action_items": [
        {
            "task_id": "TASK-001",
            "description": "Detailed task description",
            "assignee": "Responsible person",
            "due_date": "Deadline",
            "priority": "HIGH/MEDIUM/LOW",
            "context": "Why this task was created",
            "success_criteria": "Definition of done"
        }
    ],
    "total_items": "Number",
    "high_priority_count": "Number",
    "assignments": {
        "Person1": ["TASK-001", "TASK-002"],
        "Person2": ["TASK-003"]
    }
}
"""
    }
    
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
    prompt = f"""
Analyze the following meeting transcript and extract structured information.

{metadata_section}
{focus_section}

**Meeting Transcript**
{transcript}

**Analysis Instructions**
1. Read through the entire transcript carefully
2. Identify all speakers and their roles (if apparent)
3. Extract main topics, decisions, and action items
4. Note any disagreements or unresolved issues
5. Capture specific commitments, deadlines, and assignments
6. Identify follow-up requirements
7. Assess overall meeting effectiveness

{output_schemas.get(output_format, output_schemas['full'])}

**Important Notes**
- Extract ONLY information explicitly stated or clearly implied in the transcript
- If information is unclear, mark it as "Unspecified" or "To be determined"
- Preserve the original language for quotes but translate/summarize in Korean
- Maintain chronological order for events and decisions
- Use ISO 8601 format for dates (YYYY-MM-DD)
- Generate unique IDs for tasks and decisions for tracking

Please provide your analysis in the specified JSON format.
"""
    
    return prompt
```

#### 3.2.2 í‰ê°€ í”„ë¡¬í”„íŠ¸

```python
def generate_evaluation_prompt(
    original_transcript: str,
    generated_output: Dict[str, Any],
    evaluation_criteria: List[str] = None
) -> str:
    """
    ìƒì„±ëœ ì¶œë ¥ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” í”„ë¡¬í”„íŠ¸
    """
    
    default_criteria = [
        "meeting_fidelity",
        "completeness",
        "accuracy",
        "practicality",
        "clarity",
        "consistency"
    ]
    
    criteria = evaluation_criteria or default_criteria
    
    criteria_descriptions = {
        "meeting_fidelity": """
        **íšŒì˜ ë‚´ìš© ì¶©ì‹¤ë„ (Meeting Fidelity)**
        - ì›ë³¸ íšŒì˜ ë‚´ìš©ì´ ì •í™•í•˜ê²Œ ë°˜ì˜ë˜ì—ˆëŠ”ê°€?
        - ì¤‘ìš”í•œ ì •ë³´ê°€ ëˆ„ë½ë˜ì§€ ì•Šì•˜ëŠ”ê°€?
        - í™”ìì˜ ì˜ë„ê°€ ì˜¬ë°”ë¥´ê²Œ í•´ì„ë˜ì—ˆëŠ”ê°€?
        í‰ê°€ ì ìˆ˜: 1-10
        """,
        "completeness": """
        **ì™„ì„±ë„ (Completeness)**
        - ëª¨ë“  í•„ìˆ˜ í•„ë“œê°€ ì±„ì›Œì¡ŒëŠ”ê°€?
        - ì •ë³´ì˜ ê¹Šì´ê°€ ì¶©ë¶„í•œê°€?
        - ë…¼ë¦¬ì  ì—°ê²°ê³ ë¦¬ê°€ ëª…í™•í•œê°€?
        í‰ê°€ ì ìˆ˜: 1-10
        """,
        "accuracy": """
        **ì •í™•ì„± (Accuracy)**
        - ì‚¬ì‹¤ ê´€ê³„ê°€ ì •í™•í•œê°€?
        - ìˆ«ì, ë‚ ì§œ, ì´ë¦„ì´ ì˜¬ë°”ë¥¸ê°€?
        - ì¶”ë¡ ì´ í•©ë¦¬ì ì¸ê°€?
        í‰ê°€ ì ìˆ˜: 1-10
        """,
        "practicality": """
        **ì‹¤ìš©ì„± (Practicality)**
        - ì•¡ì…˜ ì•„ì´í…œì´ ì‹¤í–‰ ê°€ëŠ¥í•œê°€?
        - ì œì•ˆì‚¬í•­ì´ í˜„ì‹¤ì ì¸ê°€?
        - ì‹¤ë¬´ì— ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œê°€?
        í‰ê°€ ì ìˆ˜: 1-10
        """,
        "clarity": """
        **ëª…í™•ì„± (Clarity)**
        - í‘œí˜„ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
        - ëª¨í˜¸í•œ í‘œí˜„ì´ ì—†ëŠ”ê°€?
        - êµ¬ì¡°ê°€ ë…¼ë¦¬ì ì¸ê°€?
        í‰ê°€ ì ìˆ˜: 1-10
        """,
        "consistency": """
        **ì¼ê´€ì„± (Consistency)**
        - ì •ë³´ ê°„ ëª¨ìˆœì´ ì—†ëŠ”ê°€?
        - í˜•ì‹ì´ ì¼ê´€ë˜ëŠ”ê°€?
        - ìš©ì–´ ì‚¬ìš©ì´ í†µì¼ë˜ì–´ ìˆëŠ”ê°€?
        í‰ê°€ ì ìˆ˜: 1-10
        """
    }
    
    prompt = f"""
You are an expert evaluator for meeting analysis outputs.

**Original Meeting Transcript**
{original_transcript}

**Generated Analysis Output**
{json.dumps(generated_output, ensure_ascii=False, indent=2)}

**Evaluation Task**
Please evaluate the generated output based on the following criteria:

{chr(10).join(criteria_descriptions[c] for c in criteria if c in criteria_descriptions)}

**Output Format**
{{
    "scores": {{
        {', '.join(f'"{c}": score' for c in criteria)}
    }},
    "overall_score": "average of all scores",
    "strengths": [
        "Specific strength 1",
        "Specific strength 2",
        "Specific strength 3"
    ],
    "weaknesses": [
        "Specific weakness 1",
        "Specific weakness 2"
    ],
    "critical_issues": [
        "Any critical problems that must be fixed"
    ],
    "improvement_suggestions": [
        {{
            "aspect": "What to improve",
            "current_state": "Current problem",
            "suggested_improvement": "Specific improvement",
            "priority": "HIGH/MEDIUM/LOW"
        }}
    ],
    "missing_information": [
        "Information present in transcript but missing in output"
    ],
    "hallucinations": [
        "Any information in output not supported by transcript"
    ],
    "quality_assessment": {{
        "is_production_ready": true/false,
        "requires_human_review": true/false,
        "confidence_level": "HIGH/MEDIUM/LOW",
        "recommended_actions": ["Action 1", "Action 2"]
    }}
}}

Provide a thorough and objective evaluation.
"""
    
    return prompt
```

#### 3.2.3 ê°œì„  í”„ë¡¬í”„íŠ¸

```python
def generate_refinement_prompt(
    original_transcript: str,
    current_output: Dict[str, Any],
    evaluation_result: Dict[str, Any],
    refinement_focus: List[str] = None
) -> str:
    """
    í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶œë ¥ì„ ê°œì„ í•˜ëŠ” í”„ë¡¬í”„íŠ¸
    """
    
    prompt = f"""
You are tasked with improving a meeting analysis output based on evaluation feedback.

**Original Meeting Transcript**
{original_transcript}

**Current Analysis Output**
{json.dumps(current_output, ensure_ascii=False, indent=2)}

**Evaluation Results**
- Overall Score: {evaluation_result.get('overall_score', 'N/A')}/10
- Strengths: {evaluation_result.get('strengths', [])}
- Weaknesses: {evaluation_result.get('weaknesses', [])}
- Critical Issues: {evaluation_result.get('critical_issues', [])}
- Missing Information: {evaluation_result.get('missing_information', [])}
- Hallucinations: {evaluation_result.get('hallucinations', [])}

**Improvement Suggestions**
{json.dumps(evaluation_result.get('improvement_suggestions', []), ensure_ascii=False, indent=2)}

**Refinement Instructions**
1. Address ALL critical issues identified
2. Incorporate missing information from the transcript
3. Remove any hallucinated content
4. Improve weak areas while maintaining strengths
5. Ensure consistency throughout the document
6. Enhance clarity and practicality

{f"**Priority Focus Areas**: {refinement_focus}" if refinement_focus else ""}

**Required Actions**
- Fix critical issues: MANDATORY
- Add missing information: MANDATORY
- Remove hallucinations: MANDATORY
- Improve scores below 7: REQUIRED
- Enhance scores 7-8: RECOMMENDED
- Polish scores above 8: OPTIONAL

Generate an improved version that addresses all feedback while maintaining the same JSON structure.

**Output the complete refined JSON analysis:**
"""
    
    return prompt
```

---

## 4. íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤

### 4.1 ë°ì´í„° ì¤€ë¹„ íŒŒì´í”„ë¼ì¸

#### 4.1.1 ë°ì´í„° ìˆ˜ì§‘ ë° ê²€ì¦

```python
class DataPreparationPipeline:
    """íŒŒì¸íŠœë‹ ë°ì´í„° ì¤€ë¹„ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.data_validator = DataValidator()
        self.quality_checker = QualityChecker()
        self.statistics = {
            "total_files": 0,
            "processed": 0,
            "failed": 0,
            "chunks_created": 0,
            "quality_scores": []
        }
    
    def process_meeting_files(self, input_dir: Path) -> List[Dict]:
        """
        íšŒì˜ íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ í•™ìŠµ ë°ì´í„° ìƒì„±
        """
        processed_data = []
        
        # 1. íŒŒì¼ ìˆ˜ì§‘
        meeting_files = self.collect_meeting_files(input_dir)
        self.statistics["total_files"] = len(meeting_files)
        
        for file_path in meeting_files:
            try:
                # 2. ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
                meeting_data = self.load_and_validate(file_path)
                if not meeting_data:
                    self.statistics["failed"] += 1
                    continue
                
                # 3. í…ìŠ¤íŠ¸ ë³€í™˜
                transcript = self.convert_to_transcript(meeting_data)
                
                # 4. ì²­í‚¹ ì²˜ë¦¬
                chunks = self.apply_chunking(transcript)
                self.statistics["chunks_created"] += len(chunks)
                
                # 5. ê° ì²­í¬ì— ëŒ€í•´ ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ìƒì„±
                for chunk_idx, chunk_text in enumerate(chunks):
                    gold_standard = self.generate_gold_standard(
                        chunk_text, 
                        meeting_data["metadata"],
                        chunk_idx
                    )
                    
                    # 6. í’ˆì§ˆ ê²€ì¦
                    quality_score = self.quality_checker.evaluate(gold_standard)
                    self.statistics["quality_scores"].append(quality_score)
                    
                    if quality_score >= self.config["min_quality_score"]:
                        processed_data.append({
                            "input": chunk_text,
                            "output": gold_standard,
                            "metadata": {
                                "source": file_path.name,
                                "chunk_index": chunk_idx,
                                "quality_score": quality_score
                            }
                        })
                
                self.statistics["processed"] += 1
                
            except Exception as e:
                logger.error(f"Failed to process {file_path}: {e}")
                self.statistics["failed"] += 1
        
        return processed_data
    
    def collect_meeting_files(self, input_dir: Path) -> List[Path]:
        """íšŒì˜ íŒŒì¼ ìˆ˜ì§‘"""
        return list(input_dir.glob("**/05_final_result.json"))
    
    def load_and_validate(self, file_path: Path) -> Optional[Dict]:
        """ë°ì´í„° ë¡œë“œ ë° ê²€ì¦"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ["segments", "metadata"]
            if not all(field in data for field in required_fields):
                logger.warning(f"Missing required fields in {file_path}")
                return None
            
            # ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
            if not self.data_validator.validate(data):
                logger.warning(f"Data validation failed for {file_path}")
                return None
            
            return data
            
        except Exception as e:
            logger.error(f"Failed to load {file_path}: {e}")
            return None
    
    def convert_to_transcript(self, meeting_data: Dict) -> str:
        """íšŒì˜ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        transcript_lines = []
        
        for segment in meeting_data.get("segments", []):
            timestamp = segment.get("timestamp", "00:00:00")
            speaker = segment.get("speaker", "Unknown")
            text = segment.get("text", "")
            
            if text.strip():
                transcript_lines.append(f"[{timestamp}] {speaker}: {text}")
        
        return "\n".join(transcript_lines)
    
    def apply_chunking(self, transcript: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ì²­í‚¹ ì ìš©"""
        if len(transcript) <= self.config["chunk_size"]:
            return [transcript]
        
        chunks = []
        sentences = self.split_into_sentences(transcript)
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size > self.config["chunk_size"]:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunks.append("\n".join(current_chunk))
                
                # ì˜¤ë²„ë© ì²˜ë¦¬
                overlap_sentences = self.get_overlap_sentences(
                    current_chunk, 
                    self.config["overlap_size"]
                )
                current_chunk = overlap_sentences + [sentence]
                current_size = sum(len(s) for s in current_chunk)
            else:
                current_chunk.append(sentence)
                current_size += sentence_size
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append("\n".join(current_chunk))
        
        return chunks
    
    def split_into_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œ ë¼ì¸ ë‹¨ìœ„ë¡œ ë¶„í• 
        lines = text.split('\n')
        sentences = []
        
        for line in lines:
            if line.strip():
                # íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í™”ì ì •ë³´ ë³´ì¡´
                sentences.append(line)
        
        return sentences
    
    def get_overlap_sentences(self, sentences: List[str], overlap_size: int) -> List[str]:
        """ì˜¤ë²„ë©ì„ ìœ„í•œ ë¬¸ì¥ ì¶”ì¶œ"""
        overlap_sentences = []
        current_size = 0
        
        # ë’¤ì—ì„œë¶€í„° ì—­ìˆœìœ¼ë¡œ ì¶”ê°€
        for sentence in reversed(sentences):
            if current_size >= overlap_size:
                break
            overlap_sentences.insert(0, sentence)
            current_size += len(sentence)
        
        return overlap_sentences
```

#### 4.1.2 ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ìƒì„± ì—”ì§„

```python
class GoldStandardGenerator:
    """ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ë°ì´í„° ìƒì„±"""
    
    def __init__(self, llm_client: Any, config: Dict[str, Any]):
        self.llm_client = llm_client
        self.config = config
        self.prompt_generator = PromptGenerator()
        self.output_validator = OutputValidator()
        
    async def generate(
        self, 
        transcript: str, 
        metadata: Dict[str, Any],
        output_type: str = "notion_project"
    ) -> Dict[str, Any]:
        """
        ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ìƒì„±
        
        Args:
            transcript: íšŒì˜ ë…¹ì·¨ë¡
            metadata: íšŒì˜ ë©”íƒ€ë°ì´í„°
            output_type: ì¶œë ¥ ìœ í˜• (notion_project, task_master_prd, meeting_analysis)
        """
        
        # 1. ì´ˆê¸° ìƒì„±
        initial_output = await self.generate_initial(transcript, metadata, output_type)
        
        # 2. í’ˆì§ˆ í‰ê°€
        evaluation = await self.evaluate_output(transcript, initial_output)
        
        # 3. ë°˜ë³µ ê°œì„ 
        refined_output = initial_output
        iteration = 0
        
        while evaluation["overall_score"] < self.config["quality_threshold"] and \
              iteration < self.config["max_iterations"]:
            
            refined_output = await self.refine_output(
                transcript, 
                refined_output, 
                evaluation
            )
            
            evaluation = await self.evaluate_output(transcript, refined_output)
            iteration += 1
            
            logger.info(f"Iteration {iteration}: Score {evaluation['overall_score']}")
        
        # 4. ìµœì¢… ê²€ì¦
        is_valid = self.output_validator.validate(refined_output, output_type)
        
        return {
            "output": refined_output,
            "quality_score": evaluation["overall_score"],
            "iterations": iteration,
            "is_valid": is_valid,
            "evaluation_details": evaluation
        }
    
    async def generate_initial(
        self, 
        transcript: str, 
        metadata: Dict[str, Any],
        output_type: str
    ) -> Dict[str, Any]:
        """ì´ˆê¸° ì¶œë ¥ ìƒì„±"""
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = self.prompt_generator.get_system_prompt(output_type)
        user_prompt = self.prompt_generator.get_user_prompt(
            transcript, 
            metadata, 
            output_type
        )
        
        # LLM í˜¸ì¶œ
        response = await self.llm_client.chat.completions.create(
            model=self.config["model"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=self.config["temperature"],
            max_tokens=self.config["max_tokens"],
            response_format={"type": "json_object"}
        )
        
        # ì‘ë‹µ íŒŒì‹±
        try:
            output = json.loads(response.choices[0].message.content)
        except json.JSONDecodeError:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ì •ë¦¬ í›„ ì¬ì‹œë„
            cleaned_text = self.clean_json_response(response.choices[0].message.content)
            output = json.loads(cleaned_text)
        
        return output
    
    async def evaluate_output(
        self, 
        transcript: str, 
        output: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì¶œë ¥ í’ˆì§ˆ í‰ê°€"""
        
        evaluation_prompt = generate_evaluation_prompt(transcript, output)
        
        response = await self.llm_client.chat.completions.create(
            model=self.config["model"],
            messages=[
                {"role": "system", "content": "You are an expert evaluator."},
                {"role": "user", "content": evaluation_prompt}
            ],
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)
    
    async def refine_output(
        self, 
        transcript: str, 
        current_output: Dict[str, Any],
        evaluation: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì¶œë ¥ ê°œì„ """
        
        refinement_prompt = generate_refinement_prompt(
            transcript, 
            current_output, 
            evaluation
        )
        
        response = await self.llm_client.chat.completions.create(
            model=self.config["model"],
            messages=[
                {"role": "system", "content": "You are an expert at improving outputs."},
                {"role": "user", "content": refinement_prompt}
            ],
            temperature=0.3,
            max_tokens=self.config["max_tokens"],
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)
    
    def clean_json_response(self, text: str) -> str:
        """JSON ì‘ë‹µ ì •ë¦¬"""
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if text.startswith("```json"):
            text = text[7:]
        if text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        # ì˜ëª»ëœ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ìˆ˜ì •
        text = text.replace("\\n", "\n")
        text = text.replace("\\t", "\t")
        
        return text
```

### 4.2 íŒŒì¸íŠœë‹ ì‹¤í–‰

#### 4.2.1 QLoRA ì„¤ì • ë° í•™ìŠµ

```python
class QwenFineTuner:
    """Qwen3 ëª¨ë¸ íŒŒì¸íŠœë‹"""
    
    def __init__(self, model_name: str = "Qwen/Qwen3-8B"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.trainer = None
        
        # QLoRA ì„¤ì •
        self.lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,  # LoRA rank
            lora_alpha=32,  # LoRA scaling parameter
            lora_dropout=0.1,  # LoRA dropout
            target_modules=[
                "q_proj", "k_proj", "v_proj", "o_proj",  # Attention layers
                "gate_proj", "up_proj", "down_proj"  # MLP layers (optional)
            ],
            bias="none",
            modules_to_save=None
        )
        
        # 4-bit ì–‘ìí™” ì„¤ì •
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )
        
        # í•™ìŠµ ì„¤ì •
        self.training_args = TrainingArguments(
            output_dir="./qwen3_lora_ttalkkac",
            num_train_epochs=3,
            per_device_train_batch_size=1,
            per_device_eval_batch_size=1,
            gradient_accumulation_steps=16,
            gradient_checkpointing=False,  # 4-bitì™€ ì¶©ëŒ ë°©ì§€
            warmup_steps=50,
            learning_rate=2e-4,
            fp16=True,
            bf16=False,
            logging_steps=10,
            eval_strategy="steps",
            eval_steps=50,
            save_strategy="steps",
            save_steps=100,
            save_total_limit=3,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to="tensorboard",
            push_to_hub=False,
            optim="paged_adamw_8bit",
            max_grad_norm=0.3,
            lr_scheduler_type="cosine",
            dataloader_num_workers=4,
            remove_unused_columns=False,
            label_names=["labels"]
        )
    
    def setup_model(self):
        """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì •"""
        
        logger.info(f"Loading model: {self.model_name}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            padding_side="right",
            model_max_length=12000
        )
        
        # íŠ¹ìˆ˜ í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Qwen3 ì±„íŒ… í…œí”Œë¦¿ í† í°
        special_tokens = {
            "additional_special_tokens": [
                "<|im_start|>", 
                "<|im_end|>",
                "<|im_sep|>"
            ]
        }
        self.tokenizer.add_special_tokens(special_tokens)
        
        # ëª¨ë¸ ë¡œë“œ (4-bit ì–‘ìí™”)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            quantization_config=self.quantization_config,
            device_map="auto",
            trust_remote_code=True,
            use_cache=False,
            attn_implementation="flash_attention_2" if self.check_flash_attn() else "eager"
        )
        
        # ëª¨ë¸ ë¦¬ì‚¬ì´ì§• (íŠ¹ìˆ˜ í† í° ì¶”ê°€ë¡œ ì¸í•œ)
        self.model.resize_token_embeddings(len(self.tokenizer))
        
        # LoRA ì ìš©
        self.model = get_peft_model(self.model, self.lora_config)
        
        # 4-bit ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥ ì„¤ì •
        self.model.enable_input_require_grads()
        
        # í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° ì¶œë ¥
        self.model.print_trainable_parameters()
        
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        logger.info(f"Total parameters: {total_params:,}")
        logger.info(f"Trainable parameters: {trainable_params:,}")
        logger.info(f"Trainable ratio: {trainable_params/total_params*100:.2f}%")
    
    def check_flash_attn(self) -> bool:
        """Flash Attention ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import flash_attn
            return True
        except ImportError:
            logger.warning("Flash Attention not available, using default attention")
            return False
    
    def prepare_dataset(self, training_data: List[Dict]) -> Dataset:
        """ë°ì´í„°ì…‹ ì¤€ë¹„"""
        
        def format_conversation(example: Dict) -> str:
            """Qwen3 ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
            system_prompt = generate_meeting_analysis_system_prompt()
            user_prompt = generate_meeting_analysis_user_prompt(example["input"])
            assistant_response = json.dumps(example["output"], ensure_ascii=False)
            
            conversation = (
                f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
                f"<|im_start|>user\n{user_prompt}<|im_end|>\n"
                f"<|im_start|>assistant\n{assistant_response}<|im_end|>"
            )
            
            return conversation
        
        def tokenize_function(examples: Dict) -> Dict:
            """í† í¬ë‚˜ì´ì§•"""
            # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            conversations = [format_conversation(ex) for ex in examples["data"]]
            
            # í† í¬ë‚˜ì´ì§•
            tokenized = self.tokenizer(
                conversations,
                truncation=True,
                padding=False,
                max_length=12000,
                return_tensors=None
            )
            
            # Labels ì„¤ì • (input_ids ë³µì‚¬)
            tokenized["labels"] = tokenized["input_ids"].copy()
            
            return tokenized
        
        # Dataset ìƒì„±
        dataset_dict = {"data": training_data}
        dataset = Dataset.from_dict(dataset_dict)
        
        # í† í¬ë‚˜ì´ì§• ì ìš©
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            batch_size=1,
            remove_columns=["data"]
        )
        
        return tokenized_dataset
    
    def train(self, train_dataset: Dataset, eval_dataset: Dataset = None):
        """ëª¨ë¸ í•™ìŠµ"""
        
        # Data Collator
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model,
            padding=True,
            pad_to_multiple_of=8,
            return_tensors="pt",
            label_pad_token_id=self.tokenizer.pad_token_id
        )
        
        # Custom Trainer with callbacks
        class CustomTrainer(Trainer):
            def compute_loss(self, model, inputs, return_outputs=False):
                """Custom loss computation"""
                outputs = model(**inputs)
                loss = outputs.loss
                
                # Log additional metrics
                if self.state.global_step % 10 == 0:
                    perplexity = torch.exp(loss).item()
                    self.log({"perplexity": perplexity})
                
                return (loss, outputs) if return_outputs else loss
        
        # Trainer ì´ˆê¸°í™”
        self.trainer = CustomTrainer(
            model=self.model,
            args=self.training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
            callbacks=[
                EarlyStoppingCallback(early_stopping_patience=3),
                TensorBoardCallback(),
            ]
        )
        
        # í•™ìŠµ ì‹œì‘
        logger.info("Starting training...")
        train_result = self.trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        logger.info("Saving model...")
        self.trainer.save_model()
        self.tokenizer.save_pretrained(self.training_args.output_dir)
        
        # í•™ìŠµ ë©”íŠ¸ë¦­ ì €ì¥
        metrics = train_result.metrics
        self.trainer.log_metrics("train", metrics)
        self.trainer.save_metrics("train", metrics)
        
        return train_result
    
    def evaluate(self, eval_dataset: Dataset) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€"""
        
        if self.trainer is None:
            raise ValueError("Trainer not initialized. Run train() first.")
        
        logger.info("Evaluating model...")
        eval_result = self.trainer.evaluate(eval_dataset)
        
        # í‰ê°€ ë©”íŠ¸ë¦­ ì €ì¥
        self.trainer.log_metrics("eval", eval_result)
        self.trainer.save_metrics("eval", eval_result)
        
        return eval_result
```

---

## 5. ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ìƒì„±

### 5.1 ìƒì„± íŒŒì´í”„ë¼ì¸

```python
class GoldStandardPipeline:
    """ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ìƒì„± ì „ì²´ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, api_key: str, config: Dict[str, Any] = None):
        self.api_key = api_key
        self.config = config or self.get_default_config()
        self.llm_client = OpenAI(api_key=api_key)
        self.generator = GoldStandardGenerator(self.llm_client, self.config)
        self.statistics = defaultdict(int)
        
    def get_default_config(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ì„¤ì •"""
        return {
            "model": "gpt-4o",
            "temperature": 0.3,
            "max_tokens": 4000,
            "quality_threshold": 7.0,
            "max_iterations": 3,
            "chunk_size": 5000,
            "overlap_size": 512,
            "min_quality_score": 7.0,
            "batch_size": 10,
            "output_types": ["notion_project", "task_master_prd", "meeting_analysis"]
        }
    
    async def process_batch(
        self, 
        input_dir: Path,
        output_dir: Path,
        dataset_type: str = "train"
    ) -> List[Dict]:
        """ë°°ì¹˜ ì²˜ë¦¬"""
        
        output_dir.mkdir(parents=True, exist_ok=True)
        results = []
        
        # íŒŒì¼ ìˆ˜ì§‘
        meeting_files = list(input_dir.glob("**/05_final_result.json"))
        logger.info(f"Found {len(meeting_files)} meeting files")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        for i in range(0, len(meeting_files), self.config["batch_size"]):
            batch = meeting_files[i:i+self.config["batch_size"]]
            batch_results = await self.process_files(batch, output_dir, dataset_type)
            results.extend(batch_results)
            
            # ì¤‘ê°„ ì €ì¥
            if i % (self.config["batch_size"] * 5) == 0:
                self.save_intermediate_results(results, output_dir, dataset_type)
        
        # ìµœì¢… ì €ì¥
        self.save_final_results(results, output_dir, dataset_type)
        
        return results
    
    async def process_files(
        self, 
        files: List[Path],
        output_dir: Path,
        dataset_type: str
    ) -> List[Dict]:
        """íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬"""
        
        tasks = []
        for file_path in files:
            task = self.process_single_file(file_path, output_dir, dataset_type)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì—ëŸ¬ ì²˜ë¦¬
        valid_results = []
        for result, file_path in zip(results, files):
            if isinstance(result, Exception):
                logger.error(f"Failed to process {file_path}: {result}")
                self.statistics["failed"] += 1
            else:
                valid_results.extend(result)
                self.statistics["success"] += 1
        
        return valid_results
    
    async def process_single_file(
        self, 
        file_path: Path,
        output_dir: Path,
        dataset_type: str
    ) -> List[Dict]:
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬"""
        
        results = []
        
        # ë°ì´í„° ë¡œë“œ
        with open(file_path, 'r', encoding='utf-8') as f:
            meeting_data = json.load(f)
        
        # í…ìŠ¤íŠ¸ ë³€í™˜
        transcript = self.convert_to_transcript(meeting_data)
        
        # ì²­í‚¹
        chunks = self.apply_chunking(transcript)
        
        # ê° ì²­í¬ ì²˜ë¦¬
        for chunk_idx, chunk_text in enumerate(chunks):
            # ê° ì¶œë ¥ íƒ€ì…ë³„ë¡œ ìƒì„±
            for output_type in self.config["output_types"]:
                result = await self.generator.generate(
                    chunk_text,
                    meeting_data.get("metadata", {}),
                    output_type
                )
                
                # í’ˆì§ˆ í™•ì¸
                if result["quality_score"] >= self.config["min_quality_score"]:
                    # ê²°ê³¼ ì €ì¥
                    sample_id = f"{dataset_type}_{file_path.stem}_chunk{chunk_idx}_{output_type}"
                    
                    gold_standard = {
                        "id": sample_id,
                        "input": chunk_text,
                        "output": result["output"],
                        "output_type": output_type,
                        "metadata": {
                            "source_file": str(file_path),
                            "chunk_index": chunk_idx,
                            "total_chunks": len(chunks),
                            "quality_score": result["quality_score"],
                            "iterations": result["iterations"],
                            "dataset_type": dataset_type,
                            "created_at": datetime.now().isoformat()
                        }
                    }
                    
                    results.append(gold_standard)
                    
                    # ê°œë³„ íŒŒì¼ ì €ì¥
                    self.save_individual_result(gold_standard, output_dir, sample_id)
                    
                else:
                    logger.warning(f"Quality score too low: {result['quality_score']}")
                    self.statistics["low_quality"] += 1
        
        return results
    
    def save_individual_result(
        self, 
        result: Dict,
        output_dir: Path,
        sample_id: str
    ):
        """ê°œë³„ ê²°ê³¼ ì €ì¥"""
        
        file_path = output_dir / f"{sample_id}.json"
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
    
    def save_final_results(
        self, 
        results: List[Dict],
        output_dir: Path,
        dataset_type: str
    ):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        
        # ì „ì²´ ë°ì´í„°ì…‹
        dataset_file = output_dir / f"{dataset_type}_gold_standard.json"
        with open(dataset_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # í†µê³„
        stats_file = output_dir / f"{dataset_type}_statistics.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(dict(self.statistics), f, indent=2)
        
        logger.info(f"Saved {len(results)} samples to {dataset_file}")
        logger.info(f"Statistics: {dict(self.statistics)}")
```

---

## 6. í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ

### 6.1 í‰ê°€ ë©”íŠ¸ë¦­

```python
class QualityEvaluator:
    """í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.metrics = {
            "structural_accuracy": StructuralAccuracyMetric(),
            "content_fidelity": ContentFidelityMetric(),
            "completeness": CompletenessMetric(),
            "consistency": ConsistencyMetric(),
            "practicality": PracticalityMetric()
        }
        
    def evaluate(
        self, 
        original: str,
        generated: Dict[str, Any],
        output_type: str
    ) -> Dict[str, Any]:
        """ì¢…í•© í‰ê°€"""
        
        scores = {}
        details = {}
        
        for metric_name, metric in self.metrics.items():
            score, detail = metric.evaluate(original, generated, output_type)
            scores[metric_name] = score
            details[metric_name] = detail
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        overall_score = sum(scores.values()) / len(scores)
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        quality_grade = self.determine_grade(overall_score)
        
        return {
            "overall_score": overall_score,
            "quality_grade": quality_grade,
            "scores": scores,
            "details": details,
            "is_high_quality": overall_score >= 7.0,
            "is_production_ready": overall_score >= 8.0,
            "recommendations": self.generate_recommendations(scores, details)
        }
    
    def determine_grade(self, score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if score >= 9.0:
            return "A+"
        elif score >= 8.5:
            return "A"
        elif score >= 8.0:
            return "B+"
        elif score >= 7.5:
            return "B"
        elif score >= 7.0:
            return "C+"
        elif score >= 6.5:
            return "C"
        else:
            return "D"
    
    def generate_recommendations(
        self, 
        scores: Dict[str, float],
        details: Dict[str, Any]
    ) -> List[str]:
        """ê°œì„  ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ë‚®ì€ ì ìˆ˜ ë©”íŠ¸ë¦­ ì‹ë³„
        low_scores = {k: v for k, v in scores.items() if v < 7.0}
        
        for metric, score in low_scores.items():
            if metric == "structural_accuracy":
                recommendations.append("JSON êµ¬ì¡° ê²€ì¦ ê°•í™” í•„ìš”")
            elif metric == "content_fidelity":
                recommendations.append("ì›ë³¸ ë‚´ìš© ë°˜ì˜ë„ ê°œì„  í•„ìš”")
            elif metric == "completeness":
                recommendations.append("ëˆ„ë½ëœ ì •ë³´ ë³´ì™„ í•„ìš”")
            elif metric == "consistency":
                recommendations.append("ì •ë³´ ì¼ê´€ì„± ê²€í†  í•„ìš”")
            elif metric == "practicality":
                recommendations.append("ì‹¤ìš©ì„± ë° êµ¬ì²´ì„± í–¥ìƒ í•„ìš”")
        
        return recommendations
```

---

## 7. ì½”ë“œ êµ¬í˜„ ìƒì„¸

### 7.1 ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```python
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì„¤ì • ë¡œë“œ
    config = load_config("config.yaml")
    
    # 1. ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ìƒì„±
    logger.info("="*60)
    logger.info("Starting Gold Standard Generation")
    logger.info("="*60)
    
    gold_pipeline = GoldStandardPipeline(
        api_key=config["openai_api_key"],
        config=config["gold_standard"]
    )
    
    # Train ë°ì´í„° ìƒì„±
    train_results = await gold_pipeline.process_batch(
        input_dir=Path(config["data"]["train_dir"]),
        output_dir=Path(config["output"]["gold_standard_dir"]) / "train",
        dataset_type="train"
    )
    
    # Validation ë°ì´í„° ìƒì„±
    val_results = await gold_pipeline.process_batch(
        input_dir=Path(config["data"]["val_dir"]),
        output_dir=Path(config["output"]["gold_standard_dir"]) / "val",
        dataset_type="val"
    )
    
    logger.info(f"Generated {len(train_results)} training samples")
    logger.info(f"Generated {len(val_results)} validation samples")
    
    # 2. íŒŒì¸íŠœë‹ ë°ì´í„° ì¤€ë¹„
    logger.info("="*60)
    logger.info("Preparing Fine-tuning Data")
    logger.info("="*60)
    
    data_prep = DataPreparationPipeline(config["data_preparation"])
    
    train_dataset = data_prep.prepare_for_finetuning(train_results)
    val_dataset = data_prep.prepare_for_finetuning(val_results)
    
    # 3. ëª¨ë¸ íŒŒì¸íŠœë‹
    logger.info("="*60)
    logger.info("Starting Model Fine-tuning")
    logger.info("="*60)
    
    finetuner = QwenFineTuner(config["model"]["name"])
    finetuner.setup_model()
    
    # ë°ì´í„°ì…‹ ì¤€ë¹„
    train_dataset = finetuner.prepare_dataset(train_dataset)
    val_dataset = finetuner.prepare_dataset(val_dataset)
    
    # í•™ìŠµ ì‹¤í–‰
    train_result = finetuner.train(train_dataset, val_dataset)
    
    # 4. í‰ê°€
    logger.info("="*60)
    logger.info("Model Evaluation")
    logger.info("="*60)
    
    eval_result = finetuner.evaluate(val_dataset)
    
    # 5. ê²°ê³¼ ì €ì¥
    save_results({
        "train_result": train_result,
        "eval_result": eval_result,
        "config": config,
        "timestamp": datetime.now().isoformat()
    }, Path(config["output"]["results_dir"]))
    
    logger.info("="*60)
    logger.info("Pipeline Complete!")
    logger.info("="*60)

if __name__ == "__main__":
    asyncio.run(main())
```

---

## 8. ì„±ëŠ¥ ë©”íŠ¸ë¦­

### 8.1 í•™ìŠµ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ëª©í‘œê°’ | ë‹¬ì„±ê°’ | ìƒíƒœ |
|--------|--------|--------|------|
| Training Loss | < 0.5 | 0.42 | âœ… |
| Validation Loss | < 0.6 | 0.58 | âœ… |
| Perplexity | < 2.0 | 1.78 | âœ… |
| Learning Rate | 2e-4 | 2e-4 | âœ… |
| Gradient Norm | < 1.0 | 0.3 | âœ… |

### 8.2 í’ˆì§ˆ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ëª©í‘œê°’ | ë‹¬ì„±ê°’ | ìƒíƒœ |
|--------|--------|--------|------|
| êµ¬ì¡° ì •í™•ë„ | > 90% | 92% | âœ… |
| ë‚´ìš© ì¶©ì‹¤ë„ | > 85% | 87% | âœ… |
| ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ | > 85% | 86% | âœ… |
| ì˜ì‚¬ê²°ì • ë¶„ë¥˜ | > 80% | 83% | âœ… |
| ì „ì²´ í’ˆì§ˆ ì ìˆ˜ | > 7.5 | 7.8 | âœ… |

### 8.3 ì‹œìŠ¤í…œ ì„±ëŠ¥

| ë©”íŠ¸ë¦­ | ëª©í‘œê°’ | ë‹¬ì„±ê°’ | ìƒíƒœ |
|--------|--------|--------|------|
| ì¶”ë¡  ì†ë„ | > 30 tokens/sec | 32 tokens/sec | âœ… |
| GPU ë©”ëª¨ë¦¬ | < 20GB | 18GB | âœ… |
| ë°°ì¹˜ ì²˜ë¦¬ | > 10 docs/min | 12 docs/min | âœ… |
| API ì‘ë‹µ ì‹œê°„ | < 5 sec | 3.2 sec | âœ… |
| ì‹œìŠ¤í…œ ê°€ìš©ì„± | > 99% | 99.5% | âœ… |

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- [Qwen3 Official Documentation](https://github.com/QwenLM/Qwen)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [Flash Attention](https://arxiv.org/abs/2205.14135)
- [Transformers Documentation](https://huggingface.co/docs/transformers)

---

*ë³¸ ë¬¸ì„œëŠ” Ttalkkac AI ì‹œìŠ¤í…œì˜ ì™„ì „í•œ ê¸°ìˆ  ë¬¸ì„œì…ë‹ˆë‹¤.*  
*ìµœì¢… ì—…ë°ì´íŠ¸: 2025ë…„ 1ì›” 8ì¼*  
*ë²„ì „: 1.0.0*