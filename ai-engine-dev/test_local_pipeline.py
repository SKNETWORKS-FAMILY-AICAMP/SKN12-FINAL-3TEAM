"""
ë¡œì»¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ - ì„œë²„ ì—†ì´ ì§ì ‘ ì‹¤í–‰
ê° ë‹¨ê³„ë³„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path
import time
import sys

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
RESULT_DIR = Path("pipeline_results")
RESULT_DIR.mkdir(exist_ok=True)

# ì„¸ì…˜ ë””ë ‰í† ë¦¬
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
SESSION_DIR = RESULT_DIR / f"local_session_{timestamp}"
SESSION_DIR.mkdir(exist_ok=True)

def save_step_result(step_name: str, data: dict, step_number: int):
    """ë‹¨ê³„ë³„ ê²°ê³¼ ì €ì¥"""
    filename = SESSION_DIR / f"step{step_number}_{step_name}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    logger.info(f"âœ… Saved: {filename}")
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œë„ ì €ì¥ (ì½ê¸° ì‰½ê²Œ)
    if "text" in data or "transcript" in data or "filtered_text" in data:
        txt_filename = SESSION_DIR / f"step{step_number}_{step_name}.txt"
        text_content = data.get("text") or data.get("transcript") or data.get("filtered_text", "")
        with open(txt_filename, 'w', encoding='utf-8') as f:
            f.write(text_content)
        logger.info(f"âœ… Saved text: {txt_filename}")

def test_whisperx_only():
    """WhisperXë§Œ í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("\n" + "="*60)
        logger.info("STEP 1: WhisperX Transcription Test")
        logger.info("="*60)
        
        audio_file = "test.MP3"
        if not os.path.exists(audio_file):
            logger.error(f"âŒ Audio file not found: {audio_file}")
            return None
            
        # WhisperX ë¡œë“œ
        try:
            import whisperx
            import torch
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            compute_type = "float16" if device == "cuda" else "int8"
            
            logger.info(f"ğŸ¤ Loading WhisperX model (device: {device})...")
            model = whisperx.load_model(
                "large-v3",
                device,
                compute_type=compute_type,
                language="ko"
            )
            
            logger.info(f"ğŸ“ Transcribing: {audio_file}")
            result = model.transcribe(audio_file, batch_size=16)
            
            # ê²°ê³¼ ì •ë¦¬
            segments = result.get("segments", [])
            full_text = " ".join([seg.get("text", "") for seg in segments])
            
            step1_data = {
                "success": True,
                "audio_file": audio_file,
                "text": full_text,
                "segments": segments,
                "language": result.get("language", "ko"),
                "duration": sum([seg.get("end", 0) - seg.get("start", 0) for seg in segments]),
                "character_count": len(full_text),
                "segment_count": len(segments)
            }
            
            save_step_result("whisperx_transcription", step1_data, 1)
            
            logger.info(f"âœ… Transcription completed:")
            logger.info(f"   - Characters: {len(full_text)}")
            logger.info(f"   - Segments: {len(segments)}")
            logger.info(f"   - Duration: {step1_data['duration']:.2f} seconds")
            
            # ìƒ˜í”Œ ì¶œë ¥
            if full_text:
                sample = full_text[:500] + "..." if len(full_text) > 500 else full_text
                logger.info(f"\nğŸ“„ Sample text:\n{sample}\n")
            
            return full_text
            
        except ImportError as e:
            logger.error(f"âŒ WhisperX not installed: {e}")
            logger.info("ğŸ’¡ Install with: pip install whisperx")
            
            # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ ê³„ì†
            dummy_text = """
            ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ íšŒì˜ ì£¼ì œëŠ” ì‹ ê·œ í”„ë¡œì íŠ¸ ê¸°íšì…ë‹ˆë‹¤.
            ìš°ì„  í”„ë¡œì íŠ¸ ëª©í‘œë¥¼ ì„¤ì •í•˜ê³ , êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤.
            ë§ˆì¼€íŒ… íŒ€ì€ ì‹œì¥ ì¡°ì‚¬ë¥¼ ì§„í–‰í•˜ê³ , ê°œë°œíŒ€ì€ ê¸°ìˆ  ê²€í† ë¥¼ ì™„ë£Œí•´ì£¼ì„¸ìš”.
            ë‹¤ìŒ íšŒì˜ëŠ” ë‹¤ìŒ ì£¼ í™”ìš”ì¼ì— ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
            """
            
            step1_data = {
                "success": False,
                "text": dummy_text,
                "error": "WhisperX not available - using dummy data",
                "character_count": len(dummy_text)
            }
            save_step_result("whisperx_transcription", step1_data, 1)
            return dummy_text
            
    except Exception as e:
        logger.error(f"âŒ WhisperX test failed: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_bert_filtering(transcript: str):
    """BERT í•„í„°ë§ í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("\n" + "="*60)
        logger.info("STEP 2-3: BERT Filtering Test")
        logger.info("="*60)
        
        # Triplet + BERT ëª¨ë“ˆ ë¡œë“œ ì‹œë„
        try:
            from triplet_processor import get_triplet_processor
            from bert_classifier import get_bert_classifier
            
            logger.info("ğŸ”¬ Loading Triplet + BERT processors...")
            
            # Step 2: ì „ì²˜ë¦¬
            triplet_processor = get_triplet_processor()
            
            # í…ìŠ¤íŠ¸ë¥¼ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„í•  (ë¬¸ì¥ ë‹¨ìœ„)
            sentences = transcript.split('.')
            segments = [{"text": s.strip()} for s in sentences if s.strip()]
            
            step2_data = {
                "original_text": transcript,
                "segments": segments,
                "segment_count": len(segments),
                "preprocessing": "sentence_split"
            }
            save_step_result("bert_preprocessing", step2_data, 2)
            
            # Step 3: BERT ë¶„ë¥˜
            logger.info("ğŸ¤– Classifying with BERT...")
            
            # BERT ë¶„ë¥˜ê¸°ë¡œ ë…¸ì´ì¦ˆ í•„í„°ë§
            bert_classifier = get_bert_classifier()
            
            valid_segments = []
            noise_segments = []
            
            for seg in segments:
                # ì—¬ê¸°ì„œ ì‹¤ì œ BERT ë¶„ë¥˜ ìˆ˜í–‰
                # (ì‹¤ì œ êµ¬í˜„ì€ bert_classifier ëª¨ë“ˆì— ë”°ë¼ ë‹¤ë¦„)
                # ì„ì‹œë¡œ ê¸¸ì´ ê¸°ë°˜ í•„í„°ë§
                if len(seg["text"]) > 10:
                    valid_segments.append(seg["text"])
                else:
                    noise_segments.append(seg["text"])
            
            filtered_text = " ".join(valid_segments)
            
            step3_data = {
                "filtered_text": filtered_text,
                "valid_segments": valid_segments,
                "noise_segments": noise_segments,
                "valid_count": len(valid_segments),
                "noise_count": len(noise_segments),
                "filtering_ratio": len(noise_segments) / len(segments) if segments else 0
            }
            save_step_result("bert_classification", step3_data, 3)
            
            logger.info(f"âœ… BERT filtering completed:")
            logger.info(f"   - Valid segments: {len(valid_segments)}")
            logger.info(f"   - Noise segments: {len(noise_segments)}")
            logger.info(f"   - Text reduction: {len(transcript)} â†’ {len(filtered_text)} chars")
            
            return filtered_text
            
        except ImportError:
            logger.warning("âš ï¸ Triplet/BERT modules not available")
            
            # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ í•„í„°ë§
            lines = transcript.split('\n')
            filtered_lines = [line for line in lines if len(line.strip()) > 20]
            filtered_text = '\n'.join(filtered_lines)
            
            step2_data = {
                "original_text": transcript,
                "preprocessing": "rule_based",
                "min_length": 20
            }
            save_step_result("bert_preprocessing", step2_data, 2)
            
            step3_data = {
                "filtered_text": filtered_text,
                "method": "rule_based",
                "original_length": len(transcript),
                "filtered_length": len(filtered_text)
            }
            save_step_result("bert_classification", step3_data, 3)
            
            return filtered_text if filtered_text else transcript
            
    except Exception as e:
        logger.error(f"âŒ BERT filtering failed: {e}")
        return transcript

def test_llm_postprocessing(filtered_text: str):
    """LLM ì…ë ¥ ë°ì´í„° í›„ì²˜ë¦¬"""
    try:
        logger.info("\n" + "="*60)
        logger.info("STEP 4: LLM Input Post-processing")
        logger.info("="*60)
        
        # í…ìŠ¤íŠ¸ ì •ì œ
        logger.info("ğŸ§½ Cleaning and structuring text...")
        
        # 1. ì¤‘ë³µ ê³µë°± ì œê±°
        cleaned_text = ' '.join(filtered_text.split())
        
        # 2. ë¬¸ì¥ ì •ë¦¬
        sentences = cleaned_text.split('.')
        cleaned_sentences = []
        for sent in sentences:
            sent = sent.strip()
            if sent and len(sent) > 5:
                if not sent.endswith('.'):
                    sent += '.'
                cleaned_sentences.append(sent)
        
        # 3. ë‹¨ë½ êµ¬ì„±
        paragraphs = []
        current_para = []
        
        for i, sent in enumerate(cleaned_sentences):
            current_para.append(sent)
            # 5ë¬¸ì¥ë§ˆë‹¤ ë˜ëŠ” íŠ¹ì • í‚¤ì›Œë“œì—ì„œ ë‹¨ë½ ë¶„í• 
            if len(current_para) >= 5 or any(kw in sent for kw in ['ë‹¤ìŒìœ¼ë¡œ', 'ê·¸ë¦¬ê³ ', 'ë§ˆì§€ë§‰ìœ¼ë¡œ', 'ê²°ë¡ ']):
                paragraphs.append(' '.join(current_para))
                current_para = []
        
        if current_para:
            paragraphs.append(' '.join(current_para))
        
        refined_text = '\n\n'.join(paragraphs)
        
        # 4. í† í° ìµœì í™” ë° ì²­í‚¹
        estimated_tokens = int(len(refined_text) * 1.5)
        max_context = 28000
        
        chunks = []
        if estimated_tokens > max_context:
            logger.info(f"ğŸ”„ Chunking required: {estimated_tokens} tokens > {max_context}")
            
            chunk_size = int(max_context / 1.5)
            text_chunk_size = int(chunk_size / 1.5)
            
            current_chunk = ""
            for para in paragraphs:
                if len(current_chunk) + len(para) < text_chunk_size:
                    current_chunk += para + "\n\n"
                else:
                    chunks.append(current_chunk.strip())
                    current_chunk = para + "\n\n"
            
            if current_chunk:
                chunks.append(current_chunk.strip())
        else:
            chunks = [refined_text]
        
        # 5. êµ¬ì¡°í™”ëœ ì…ë ¥ ìƒì„±
        structured_input = {
            "context": "íšŒì˜ ë…¹ì·¨ë¡ ë¶„ì„",
            "timestamp": datetime.now().isoformat(),
            "content": refined_text,
            "paragraphs": paragraphs,
            "chunks": chunks,
            "metadata": {
                "sentence_count": len(cleaned_sentences),
                "paragraph_count": len(paragraphs),
                "chunk_count": len(chunks),
                "avg_sentence_length": sum(len(s) for s in cleaned_sentences) / len(cleaned_sentences) if cleaned_sentences else 0,
                "estimated_tokens": estimated_tokens
            }
        }
        
        step4_data = {
            "original_filtered_text": filtered_text[:1000] + "..." if len(filtered_text) > 1000 else filtered_text,
            "refined_text": refined_text[:1000] + "..." if len(refined_text) > 1000 else refined_text,
            "full_refined_text": refined_text,  # ì „ì²´ í…ìŠ¤íŠ¸
            "structured_input": structured_input,
            "postprocessing_stats": {
                "original_length": len(filtered_text),
                "refined_length": len(refined_text),
                "reduction_rate": 1 - (len(refined_text) / len(filtered_text)) if len(filtered_text) > 0 else 0,
                "sentence_count": len(cleaned_sentences),
                "paragraph_count": len(paragraphs),
                "chunk_count": len(chunks),
                "estimated_tokens": estimated_tokens,
                "chunking_required": len(chunks) > 1
            },
            "processing_steps": [
                "ì¤‘ë³µ ê³µë°± ì œê±°",
                "ë¬¸ì¥ ì •ë¦¬ ë° í•„í„°ë§",
                "ë‹¨ë½ êµ¬ì„±",
                "í† í° ìµœì í™”",
                "ì²­í‚¹ ì²˜ë¦¬"
            ]
        }
        save_step_result("llm_postprocessing", step4_data, 4)
        
        logger.info(f"âœ… Post-processing completed:")
        logger.info(f"   - Original: {len(filtered_text)} chars")
        logger.info(f"   - Refined: {len(refined_text)} chars")
        logger.info(f"   - Sentences: {len(cleaned_sentences)}")
        logger.info(f"   - Paragraphs: {len(paragraphs)}")
        logger.info(f"   - Chunks: {len(chunks)}")
        logger.info(f"   - Est. tokens: {estimated_tokens}")
        
        return refined_text
        
    except Exception as e:
        logger.error(f"âŒ Post-processing failed: {e}")
        return filtered_text

def test_meeting_generation(refined_text: str):
    """LLMì„ í†µí•œ íšŒì˜ë¡ ë° íƒœìŠ¤í¬ ìƒì„±"""
    try:
        logger.info("\n" + "="*60)
        logger.info("STEP 5-6: Meeting Minutes & Task Generation")
        logger.info("="*60)
        
        # Step 5: íšŒì˜ë¡ ìƒì„± (ë”ë¯¸)
        meeting_minutes = {
            "projectName": "í…ŒìŠ¤íŠ¸ í”„ë¡œì íŠ¸",
            "date": datetime.now().isoformat(),
            "participants": ["ì°¸ê°€ì1", "ì°¸ê°€ì2"],
            "summary": "íšŒì˜ ìš”ì•½ ë‚´ìš©",
            "keyPoints": [
                "í•µì‹¬ í¬ì¸íŠ¸ 1",
                "í•µì‹¬ í¬ì¸íŠ¸ 2",
                "í•µì‹¬ í¬ì¸íŠ¸ 3"
            ],
            "decisions": [
                "ê²°ì • ì‚¬í•­ 1",
                "ê²°ì • ì‚¬í•­ 2"
            ],
            "actionItems": [
                "ì‹¤í–‰ í•­ëª© 1",
                "ì‹¤í–‰ í•­ëª© 2"
            ]
        }
        
        step5_data = {
            "success": True,
            "meeting_minutes": meeting_minutes,
            "formatted_text": json.dumps(meeting_minutes, ensure_ascii=False, indent=2)
        }
        save_step_result("meeting_minutes", step5_data, 5)
        
        # Step 6: Task ìƒì„± (ë”ë¯¸)
        tasks = [
            {
                "id": 1,
                "title": "í”„ë¡œì íŠ¸ ê¸°íšì„œ ì‘ì„±",
                "description": "ìƒì„¸ ê¸°íšì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤",
                "priority": "high",
                "subtasks": [
                    {"id": 1, "title": "ìš”êµ¬ì‚¬í•­ ë¶„ì„", "estimated_hours": 4},
                    {"id": 2, "title": "ê¸°íšì„œ ì´ˆì•ˆ ì‘ì„±", "estimated_hours": 8}
                ]
            },
            {
                "id": 2,
                "title": "ê¸°ìˆ  ê²€í† ",
                "description": "ê¸°ìˆ ì  íƒ€ë‹¹ì„±ì„ ê²€í† í•©ë‹ˆë‹¤",
                "priority": "medium",
                "subtasks": [
                    {"id": 1, "title": "ê¸°ìˆ  ìŠ¤íƒ ì„ ì •", "estimated_hours": 3},
                    {"id": 2, "title": "í”„ë¡œí† íƒ€ì… ê°œë°œ", "estimated_hours": 12}
                ]
            }
        ]
        
        step6_data = {
            "success": True,
            "tasks": tasks,
            "task_count": len(tasks),
            "subtask_count": sum(len(t.get("subtasks", [])) for t in tasks),
            "total_estimated_hours": sum(
                sum(st.get("estimated_hours", 0) for st in t.get("subtasks", []))
                for t in tasks
            )
        }
        save_step_result("tasks_and_subtasks", step6_data, 6)
        
        logger.info(f"âœ… LLM processing completed:")
        logger.info(f"   - Tasks: {step6_data['task_count']}")
        logger.info(f"   - Subtasks: {step6_data['subtask_count']}")
        logger.info(f"   - Total hours: {step6_data['total_estimated_hours']}")
        
    except Exception as e:
        logger.error(f"âŒ LLM processing failed: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    logger.info("\n" + "="*70)
    logger.info("ğŸš€ TtalKkak AI Pipeline Local Test")
    logger.info("="*70)
    logger.info(f"ğŸ“ Results directory: {SESSION_DIR}")
    
    # Step 1: WhisperX
    transcript = test_whisperx_only()
    if not transcript:
        logger.error("âŒ Failed to get transcript. Exiting.")
        return
    
    # Step 2-3: BERT Filtering
    filtered_text = test_bert_filtering(transcript)
    
    # Step 4: LLM Post-processing
    refined_text = test_llm_postprocessing(filtered_text)
    
    # Step 5-6: Meeting & Task Generation
    test_meeting_generation(refined_text)
    
    # ìµœì¢… ìš”ì•½
    logger.info("\n" + "="*70)
    logger.info("ğŸ“Š TEST SUMMARY")
    logger.info("="*70)
    logger.info(f"âœ… All results saved to: {SESSION_DIR}")
    
    # ìš”ì•½ íŒŒì¼ ìƒì„±
    summary = {
        "session_id": timestamp,
        "test_type": "local",
        "steps_completed": [
            "whisperx_transcription",
            "bert_preprocessing",
            "bert_classification",
            "llm_preprocessing",
            "meeting_minutes",
            "tasks_and_subtasks"
        ],
        "results_directory": str(SESSION_DIR),
        "timestamp": datetime.now().isoformat()
    }
    
    summary_file = SESSION_DIR / "test_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“„ Summary: {summary_file}")
    
    # ê²°ê³¼ í´ë” ì—´ê¸°
    if os.name == 'nt':  # Windows
        os.system(f'explorer "{SESSION_DIR}"')

if __name__ == "__main__":
    main()